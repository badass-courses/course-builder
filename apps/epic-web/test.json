[
	{
		"_updatedAt": "2024-03-27T14:14:51Z",
		"body": "Ever found yourself in the midst of a Git nightmare, like accidentally pushing your work-in-progress branch straight to production? It happens to the best of us.\n\nBut mastering Git isn't just about memorizing commands and executing them robotically. It's about understanding the underlying principles, developing best practices, and learning from both successes and failures. It's about transforming Git from a tool you use to a skill you wield with confidence.\n\nWhether you're a coding newbie just starting to explore version control or a seasoned developer looking to refine your Git skills, this tutorial is the perfect git roadmap for you.\n\n## What to expect:\n\n**Foundation Building:** Lay the groundwork for Git control with essential concepts and techniques, from repositories and branches to commits and merges.\n\n**Command Chronicles:** Explore vital Git commands such as `git help` and `git pull` to equip yourself with the skills and confidence needed to navigate Git effectively.\n",
		"instructor": {
			"_id": "8060ae96-00b3-40df-8da0-5b94118a48fb",
			"_type": "contributor",
			"_updatedAt": "2024-03-27T06:55:37Z",
			"name": "Chantastic",
			"picture": {
				"url": "https://cdn.sanity.io/images/i1a93n76/production/056c11360cf84e9d2f4091d0ef65495091097723-400x400.png",
				"alt": "Michael Chan"
			},
			"_createdAt": "2024-03-27T06:55:18Z",
			"bio": "Software engineer and community leader with a passion for education. As the host of React Podcast, he has shared insights from open source luminaries and engineer-creators. His curious and engaging style has helped aspiring engineers learn React., and he also advocates for building a deep understanding over surface-level knowledge. ",
			"links": null,
			"slug": "chantastic"
		},
		"sections": [
			{
				"slug": "commands",
				"lessons": [
					{
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:38Z",
						"title": "Intro to Git Fundamentals",
						"description": "In this tutorial, you'll elevate your Git skills, learning from mistakes and unearthing lesser-known tricks that will help you in your everyday life as a developer.",
						"slug": "intro-to-git-fundamentals",
						"_id": "PoPWMVqVlifcLTjN9EOd17",
						"body": "The first time that I used Git on a team I made a huge mistake.\n\nI took the work-in-progress branch that I was developing and I force pushed it to production.\n\nThe lead on the team spent the whole day kind of untangling the mess I'd made and reconstructing the history from all the various distributed branches that everyone else had.\n\nThis embarrassing experience led me to commit to getting really good with Git.\n\nOver the last 14 years since that incident, I've become the person who offers assistance and helps people when they've made a mess of their Git history.\n\nThis tutorial will lay the foundation for success in Git. But don't be fooled; it's not just for beginners. Even after my 15+ years of experience with Git, I learned some things while doing the research for these lessons.\n\nNo matter where you're at in your Git journey, I believe there's something here for you.",
						"videoResource": {
							"_id": "cCHMkFUt5UU1S5eeML4HvE",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/00-intro.mp4",
							"title": "00-intro",
							"duration": 59.477333,
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:03,520\nThe first time that I used Git on a team I made a huge mistake.\n\n2\n00:00:03,520 --> 00:00:08,800\nI took the work-in-progress branch that I was developing and I force pushed it to production.\n\n3\n00:00:08,800 --> 00:00:13,200\nThe lead on the team spent the whole day kind of untangling the mess I'd made and\n\n4\n00:00:13,200 --> 00:00:17,840\nreconstructing the history from all the various distributed branches that everyone else had.\n\n5\n00:00:17,840 --> 00:00:22,240\nThis was super embarrassing and as a front-end developer that was trying really hard to\n\n6\n00:00:22,240 --> 00:00:25,040\nmake a good impression I really embarrassed myself.\n\n7\n00:00:25,040 --> 00:00:28,639\nSo I committed in that moment to get really good with Git and over the last\n\n8\n00:00:28,639 --> 00:00:33,759\n14 years since that incident I've become the person that offers assistance and\n\n9\n00:00:33,759 --> 00:00:36,880\nhelps people when they've made a mess of their Git history.\n\n10\n00:00:36,880 --> 00:00:40,480\nThis tutorial will lay the foundation for success in Git.\n\n11\n00:00:40,480 --> 00:00:42,720\nBut don't be fooled it's not just for beginners.\n\n12\n00:00:42,720 --> 00:00:48,720\nEven after my 15 plus years of experience with Git I learned some things as I was\n\n13\n00:00:48,720 --> 00:00:50,560\ndoing the research for these lessons.\n\n14\n00:00:50,560 --> 00:00:54,959\nNo matter where you're at in your Git history I believe there's something here for you.\n\n15\n00:00:54,959 --> 00:00:59,279\nI'm Chinatastic and this is my epic guide to everyday Git.\n\n\n",
								"text": "[00:00] The first time that I used Git on a team I made a huge mistake. I took the work-in-progress branch that I was developing and I force pushed it to production. The lead on the team spent the whole day kind of untangling the mess I'd made and reconstructing the history from all the various distributed branches that everyone else had.\n\n[00:17] This was super embarrassing and as a front-end developer that was trying really hard to make a good impression I really embarrassed myself. So I committed in that moment to get really good with Git and over the last 14 years since that incident I've become the person that offers assistance and helps people when they've made a mess of their Git history.\n\n[00:36] This tutorial will lay the foundation for success in Git. But don't be fooled it's not just for beginners. Even after my 15 plus years of experience with Git I learned some things as I was doing the research for these lessons. No matter where you're at in your Git history I believe there's something here for you.\n\n[00:54] I'm Chinatastic and this is my epic guide to everyday Git."
							},
							"_createdAt": "2024-03-12T20:04:41Z",
							"_rev": "9CeTKuUcQZRsVUft8VlQPC",
							"_updatedAt": "2024-03-19T20:21:42Z",
							"muxAsset": {
								"muxAssetId": "FZOrIA2mIewUINx2u02duAjVkkjknzlZpjaCjzJ25gIs",
								"_type": "muxAsset",
								"muxPlaybackId": "L7SSjQ02TWrZwC4Xmrtb9b5EgaWZm6dWSsnxdPqgoyn8"
							},
							"state": "ready",
							"_type": "videoResource"
						},
						"solution": null
					},
					{
						"slug": "git-help-options",
						"solution": null,
						"_id": "uMTXJA9nbLKuwVJ2I4qfYd",
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:39Z",
						"title": "Git Help Options",
						"description": "Improve your daily Git usage by learning essential help commands, including tutorials and subcommand instructions for a smoother workflow.",
						"body": "The most important Git command that you can learn is `git help`.\n\nRunning `git help` with no options will give you a great summary of the `git` command along with several sections for common commands:\n\n```bash\nusage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\n...\n```\n\nAt the bottom of the help summary, you'll see two additional commands that are extremely helpful: `git help -a` and `git help -g`:\n\n```tsx\n'git help -a' and 'git help -g' list available subcommands and some concept guides.\n```\n\nWhen running `git help -g` you'll get a list of tutorials. For example, `everyday` is a set of commands for everyday Git.\n\nIf you run `git help -a`, you'll get a list of all the subcommands.\n\nTo get instructions for using for a specific subcommand, you can run `git help subcommand`, for example: `git help init`.\n\nTo search for a subcommand, type `/subcommand` and hit enter. Use `shift + N` to go back and `N` to go forward. Press `Q` to quit.",
						"videoResource": {
							"title": "01-git-help",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:03,160\nThe most important git command that you can learn is help running.\n\n2\n00:00:03,160 --> 00:00:04,680\nGet help with no options.\n\n3\n00:00:04,680 --> 00:00:06,880\nWe'll give you a great summary of the command.\n\n4\n00:00:07,040 --> 00:00:12,640\nThat's even nicely divided into sections for every day, get versus collaboration.\n\n5\n00:00:12,760 --> 00:00:15,640\nAt the bottom of the help summary, you'll see two additional\n\n6\n00:00:15,640 --> 00:00:18,480\ncommands that are extremely helpful.\n\n7\n00:00:18,520 --> 00:00:23,719\nWhen we run git help G we had a list of really cool tutorials.\n\n8\n00:00:23,840 --> 00:00:25,520\nOne of my favorites is every day.\n\n9\n00:00:25,520 --> 00:00:28,480\nGet a useful, minimal set of commands for every day.\n\n10\n00:00:28,920 --> 00:00:32,520\nThere was another command, get help a, this command lists all of the\n\n11\n00:00:32,520 --> 00:00:36,680\nsub commands that you can run help on to see the list, just keep hitting\n\n12\n00:00:36,680 --> 00:00:42,560\nspace or search it by typing the forward slash key, typing your query and hitting.\n\n13\n00:00:42,599 --> 00:00:47,320\nEnter use shift N to go back and N to go forward.\n\n14\n00:00:47,480 --> 00:00:48,000\nQ to quit.\n\n15\n00:00:48,080 --> 00:00:52,720\nFinally, we can use get help with our sub command name to get\n\n16\n00:00:52,720 --> 00:00:54,799\ninstructions for that sub command.\n\n17\n00:00:54,919 --> 00:00:56,720\nSpace will scroll you down.\n\n18\n00:00:56,840 --> 00:00:59,240\nArrow keys will move you up and down.\n\n19\n00:00:59,439 --> 00:01:00,240\nQ to quit.\n\n\n",
								"text": "[00:00] The most important git command that you can learn is help running. Get help with no options. We'll give you a great summary of the command. That's even nicely divided into sections for every day, get versus collaboration. At the bottom of the help summary, you'll see two additional commands that are extremely helpful.\n\n[00:18] When we run git help G we had a list of really cool tutorials. One of my favorites is every day. Get a useful, minimal set of commands for every day. There was another command, get help a, this command lists all of the sub commands that you can run help on to see the list, just keep hitting\n\n[00:36] space or search it by typing the forward slash key, typing your query and hitting. Enter use shift N to go back and N to go forward. Q to quit. Finally, we can use get help with our sub command name to get instructions for that sub command. Space will scroll you down.\n\n[00:56] Arrow keys will move you up and down. Q to quit."
							},
							"_createdAt": "2024-03-12T20:04:43Z",
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "y9tvurUnGlmrge4DHMiF5xLxl9t012XV6ovQJfBnTXLs",
								"muxAssetId": "LeKoDw2eVn8O01bGajcS02CEtvVN78SMHhJdgPUfAOyzU",
								"_type": "muxAsset"
							},
							"_id": "cCHMkFUt5UU1S5eeML4I4o",
							"state": "ready",
							"_rev": "6LaeqP6n94P8FD3sVhYCCb",
							"duration": 60.444,
							"_updatedAt": "2024-03-19T20:21:39Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/01-git-help.mp4"
						}
					},
					{
						"description": "Learn how to use the git init command to create and initialize new git repositories, and explore the hidden .git directory.",
						"body": "`git init` is the command used to create a new git repository. To initialize a repository in a new directory, provide an optional path:\n\n```bash\ngit init my-git-notes\n```\n\nMove into the directory with `cd my-git-notes`, then run `ls` and see that the directory appears empty.\n\nHowever, running `ls -a` reveals that there is a hidden `.git` directory:\n\n```bash\n$~/my-git-notes> ls -a\n\n.git\n```\n\nYou can use Use `ls .git` to list the contents of the `.git` directory:\n\n![image terminal](https://res.cloudinary.com/epic-web/image/upload/v1709589012/tutorials/git-foundations/Screenshot_2024-03-04_at_3.44.43_PM.png)\n\nThis `.git` directory with those nested folders and files is what makes this otherwise normal system directory into a git repository.\n\nFor more `git init` options and information, type `git help init`.",
						"slug": "creating-a-git-repository-with-git-init",
						"videoResource": {
							"title": "02-git-init",
							"_updatedAt": "2024-03-19T20:21:41Z",
							"_rev": "WsAmd18YA1frbQaWBZpIcU",
							"state": "ready",
							"_type": "videoResource",
							"_id": "cCHMkFUt5UU1S5eeML4IEO",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:07,200\nGit init is how you create git repositories. Provide an optional path, my git notes,\n\n2\n00:00:07,200 --> 00:00:10,400\nto initialize that repository in a new directory.\n\n3\n00:00:10,400 --> 00:00:16,080\ncd into the new git repository and run ls to see that it is in fact empty.\n\n4\n00:00:16,080 --> 00:00:21,120\nNow run ls with the a option to see that there's one hidden directory.\n\n5\n00:00:21,120 --> 00:00:25,760\nLet's take a look inside of that ls again for that .git directory.\n\n6\n00:00:25,760 --> 00:00:32,639\nThis .git hidden directory with these nested folders and files is what makes this boring\n\n7\n00:00:32,639 --> 00:00:40,320\nsystem directory into a git repository. For additional git init options type git help init.\n\n\n",
								"text": "[00:00] Git init is how you create git repositories. Provide an optional path, my git notes, to initialize that repository in a new directory. cd into the new git repository and run ls to see that it is in fact empty.\n\n[00:16] Now run ls with the a option to see that there's one hidden directory. Let's take a look inside of that ls again for that .git directory. This .git hidden directory with these nested folders and files is what makes this boring\n\n[00:32] system directory into a git repository. For additional git init options type git help init."
							},
							"_createdAt": "2024-03-12T20:04:43Z",
							"muxAsset": {
								"muxAssetId": "aT1Imwn8X2i29pb1zYtDLtWuH8VH7rjuImVUS84401xc",
								"_type": "muxAsset",
								"muxPlaybackId": "OLytgaWPOPVdBupKKeHXrWeINBmMP7cDyPb7WD5ifCk"
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/02-git-init.mp4",
							"duration": 40.410667
						},
						"_id": "PoPWMVqVlifcLTjN9EOddh",
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:41Z",
						"title": "Creating a Git Repository with `git init`",
						"solution": null
					},
					{
						"title": "Git Status and the Hidden .git Directory",
						"description": "Learn how to use git status command to view repository information, track and commit changes, and understand the importance of the .git directory.",
						"body": "`git status` is a command you'll use all the time when working with Git repositories.\n\nThis command will show the current branch you're on, and your commit history.\n\nSince our repository is new, there isn't much to see:\n\n```\n$ git status\n\nOn branch main\nNo commits yet\nnothing to commit (create/copy files and use \"git add\" to track)\n```\n\nThis functionality is powered by the hidden `.git` directory in our repository. But what happens if we remove that directory?\n\n```\n$ rm -vrf .git\n```\n\nNow, when we run `git status`, it no longer works because this is no longer a repository, it's just a folder.\n\n```tsx\n$ git status\n\nfatal: not a git repository (or any of the parent directories): .git\n```\n\nTo get back to where we were, run `git init` again inside the directory:\n\n```bash\ngit init\n```",
						"videoResource": {
							"duration": 34.877333,
							"_id": "bm1xlpSMDXY4FOEDFgqZHO",
							"state": "ready",
							"_createdAt": "2024-03-12T20:04:44Z",
							"_rev": "WsAmd18YA1frbQaWBZpIJW",
							"_updatedAt": "2024-03-19T20:21:37Z",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:04,800\ngit status is a command that you will use all the time right now there isn't much to see but there\n\n2\n00:00:04,800 --> 00:00:10,800\nis already a bunch of information here we can see that we're on the main branch there are no commits\n\n3\n00:00:10,800 --> 00:00:18,160\nbut we can create some by creating files and using git add to track them now this is powered by our\n\n4\n00:00:18,160 --> 00:00:24,559\nhidden dot git directory so what happens if we remove that directory git status no longer works\n\n5\n00:00:24,559 --> 00:00:30,320\nbecause this is no longer a repository just a folder so let's run git init again inside\n\n6\n00:00:30,320 --> 00:00:34,639\nthis directory to get back to where we were\n\n\n",
								"text": "[00:00] git status is a command that you will use all the time right now there isn't much to see but there is already a bunch of information here we can see that we're on the main branch there are no commits but we can create some by creating files and using git add to track them now this is powered by our\n\n[00:18] hidden dot git directory so what happens if we remove that directory git status no longer works because this is no longer a repository just a folder so let's run git init again inside this directory to get back to where we were"
							},
							"title": "03-git-status",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/03-git-status.mp4",
							"_type": "videoResource",
							"muxAsset": {
								"muxAssetId": "qBoFsa00ygKFrNZvTIQbqgSmIoZ3Kj1MOSaYs02vgXm02c",
								"_type": "muxAsset",
								"muxPlaybackId": "rZaTGXHFOxHrHP00oGaeYVMa009RZEHg7nOED021y2Ipi8"
							}
						},
						"_updatedAt": "2024-03-13T15:44:42Z",
						"_type": "lesson",
						"slug": "git-status-and-the-hidden-git-directory",
						"solution": null,
						"_id": "PoPWMVqVlifcLTjN9EOe5t"
					},
					{
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:43Z",
						"body": "When we ran `git status` inside of our repository in our previous lesson , we were told that there is nothing to commit, and that we should make files and add them. So let's do that!\n\n## Create Files with `echo`\n\nThe `echo` command can be used to write text into new files.\n\nThe following commands will create new files for `git-help.txt`, `git-init.txt`, and `git-status.txt`:\n\n```markdown\necho \"#git-help\" > git-help.txt\necho \"#git-init\" > git-init.txt\necho \"#git-status\" > git-status.txt\n```\n\nRunning `ls` will show us the files have been created.\n\n## Check the File Status\n\nRunning `git status` shows that there are untracked files:\n\n```markdown\n$ git status\n\nOn branch main\nNo commits yet\nUntracked files:\n(use \"git add <file>...\" to include in what will be committed)\ngit-help.txt\ngit-init.txt\ngit-status.txt\nnothing added to commit but untracked files present (use \"git add\" to track)\n```\n\nIn order to track the files, we first need to add them.\n\n## Add Files\n\nWe can add the files with the `git add` command along with a filename.\n\nLet's add `git-help.txt` first:\n\n```\ngit add githelp.txt\n```\n\nRunning `git status` again, we have an additional section called \"changes to be committed.\":\n\n```tsx\n$ git status\n\nOn branch main\nNo commits yet\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n        new file:   git-help.txt\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        git-init.txt\n        git-status.txt\n```\n\nNote that the `git-help.txt` changes aren't committed yet but staged for commit.\n\n## Stage the Rest of the Files\n\nThe remaining files can be individually with `git add`.\n\nHowever, using the shortcut `git add .` will add everything that hasn't been added to the index.\n\nAfter adding the rest of the files, running `git status` one last time will show that everything has been moved into our index and staged for commit.",
						"slug": "git-tracking-and-staging-files",
						"solution": null,
						"_id": "PoPWMVqVlifcLTjN9EOeQf",
						"title": "Git Tracking and Staging Files",
						"description": "Learn the basics of working with Git like tracking files, understanding the status, and staging changes for commit.",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:21:35Z",
							"_type": "videoResource",
							"duration": 63.310667,
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:05,280\nWhen we ran git status inside of our repository, we were told that there is nothing to commit,\n\n2\n00:00:05,280 --> 00:00:11,920\nand that we should make files and add them. So let's do that. Let's echo a githelp heading\n\n3\n00:00:11,920 --> 00:00:17,840\ninto a githelp.txt file and repeat that for init and status.\n\n4\n00:00:20,480 --> 00:00:26,160\nTyping ls we should see three files. Let's run git status again. We have a new category now\n\n5\n00:00:26,160 --> 00:00:32,240\nfor untracked files. We can add them with the git add subcommand. Let's do that with a file name\n\n6\n00:00:32,240 --> 00:00:39,279\nlike it's suggesting here. git add githelp.txt. Run git status again. Now we have an additional\n\n7\n00:00:39,279 --> 00:00:45,040\nsection, changes to be committed. Note that these aren't committed yet, but they're staged for\n\n8\n00:00:45,040 --> 00:00:50,160\ncommit. If we want to stage the rest of these files, we can do them individually with git add,\n\n9\n00:00:50,160 --> 00:00:56,080\nbut we can use the dot shortcut to add everything that hasn't been added to the index. Finally,\n\n10\n00:00:56,080 --> 00:01:01,439\nrun git status one last time to see that everything has been moved into our index\n\n11\n00:01:01,439 --> 00:01:03,119\nand staged for commit.\n\n\n",
								"text": "[00:00] When we ran git status inside of our repository, we were told that there is nothing to commit, and that we should make files and add them. So let's do that. Let's echo a githelp heading into a githelp.txt file and repeat that for init and status.\n\n[00:20] Typing ls we should see three files. Let's run git status again. We have a new category now for untracked files. We can add them with the git add subcommand. Let's do that with a file name like it's suggesting here. git add githelp.txt. Run git status again. Now we have an additional\n\n[00:39] section, changes to be committed. Note that these aren't committed yet, but they're staged for commit. If we want to stage the rest of these files, we can do them individually with git add, but we can use the dot shortcut to add everything that hasn't been added to the index. Finally,\n\n[00:56] run git status one last time to see that everything has been moved into our index and staged for commit."
							},
							"muxAsset": {
								"muxPlaybackId": "4fwk1Pec3B2nDgZ3jm01CFbEA8L1jA2PviENFUOdspvQ",
								"muxAssetId": "kNRJ00UfydZBCgRQPhsRxVJqZZ4tZw6kWGAiRycWoMXs",
								"_type": "muxAsset"
							},
							"_id": "cCHMkFUt5UU1S5eeML4INy",
							"_createdAt": "2024-03-12T20:04:45Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/04-git-add.mp4",
							"state": "ready",
							"title": "04-git-add",
							"_rev": "9CeTKuUcQZRsVUft8VlQ2x"
						}
					},
					{
						"slug": "using-git-reset-to-refine-your-staging-area",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "dt2QumKLxLz5LTjMYDR6pYYCurZytmPgo8aP01hxoSVw",
								"muxAssetId": "r00RblfEaT5afeX9aOBznD02DbXFnsQ9sM6uIXbzcWssk",
								"_type": "muxAsset"
							},
							"state": "ready",
							"_updatedAt": "2024-03-19T20:21:34Z",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:06,040\nBefore we create a commit, we want to make sure that only files and changes that we intend\n\n2\n00:00:06,040 --> 00:00:09,840\nto include are in the index or staged area.\n\n3\n00:00:09,840 --> 00:00:17,480\nTo remove the git status file from this commit, type git reset and the filename, git status.txt.\n\n4\n00:00:17,480 --> 00:00:22,280\nRunning git status again, we see that only these two files are staged for commit, and\n\n5\n00:00:22,280 --> 00:00:23,959\nthis file remains untracked.\n\n6\n00:00:23,959 --> 00:00:30,719\nGit reset is the logical opposite of git add, meaning we can use . to reset all of the files.\n\n\n",
								"text": "[00:00] Before we create a commit, we want to make sure that only files and changes that we intend to include are in the index or staged area. To remove the git status file from this commit, type git reset and the filename, git status.txt.\n\n[00:17] Running git status again, we see that only these two files are staged for commit, and this file remains untracked. Git reset is the logical opposite of git add, meaning we can use . to reset all of the files."
							},
							"_type": "videoResource",
							"title": "05-git-reset",
							"_rev": "9CeTKuUcQZRsVUft8VlPn4",
							"_createdAt": "2024-03-12T20:04:47Z",
							"duration": 33.360667,
							"_id": "cCHMkFUt5UU1S5eeML4IXY",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/05-git-reset.mp4"
						},
						"solution": null,
						"_id": "PoPWMVqVlifcLTjN9EOemv",
						"_updatedAt": "2024-03-13T15:44:45Z",
						"description": "Learn how to manage the staging area before creating a commit by utilizing the git reset command to exclude files and changes.",
						"body": "Before we create a commit, we want to make sure that only files and changes that we intend to include are in the index or staged area.\n\nTo remove a file from the commit, you can use the `git reset` command with the filename:\n\n```markdown\ngit reset git-status.txt\n```\n\nAfter running the command, you can use `git status` again to verify that the file has been removed from the staged area.\n\n```tsx\n$ git status\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached <file>...\" to unstage)\n        new file:   git-help.txt\n        new file:   git-init.txt\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        git-status.txt\n```\n\nLike before, `get reset .` will remove all files from the staged area. This allows you to review and stage the intended files before committing.",
						"_type": "lesson",
						"title": "Using `git reset` to Refine Your Staging Area"
					},
					{
						"_updatedAt": "2024-03-13T15:44:46Z",
						"title": "Setting Up Git Configuration for Commits",
						"videoResource": {
							"title": "06-git-config",
							"_rev": "WsAmd18YA1frbQaWBZpHsQ",
							"_createdAt": "2024-03-12T20:04:48Z",
							"_updatedAt": "2024-03-19T20:21:32Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/06-git-config.mp4",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:04,080\nBefore we can commit anything, we need to add some git configuration.\n\n2\n00:00:04,080 --> 00:00:07,600\nAssuming you're not on a shared machine, you can add this globally.\n\n3\n00:00:07,600 --> 00:00:11,040\nAdd a username that you'd like to show up in your commits.\n\n4\n00:00:11,040 --> 00:00:12,640\nI'll use Michael Chan.\n\n5\n00:00:12,640 --> 00:00:15,280\nAs well as a user email address.\n\n6\n00:00:15,280 --> 00:00:17,760\nI'll use hi at Chan dot dev.\n\n7\n00:00:17,760 --> 00:00:21,200\nWe can see our config using the list flag.\n\n8\n00:00:21,200 --> 00:00:25,680\nThese are the two options that I provided alongside some system defaults.\n\n9\n00:00:25,680 --> 00:00:28,559\nWith a user and email, we're now ready to sign our commits.\n\n\n",
								"text": "[00:00] Before we can commit anything, we need to add some git configuration. Assuming you're not on a shared machine, you can add this globally. Add a username that you'd like to show up in your commits. I'll use Michael Chan. As well as a user email address. I'll use hi at Chan dot dev.\n\n[00:17] We can see our config using the list flag. These are the two options that I provided alongside some system defaults. With a user and email, we're now ready to sign our commits."
							},
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "QIbIrZvGH7OqybbudyqaSUGUb02ELffEEaF3yZXyC4pY",
								"muxAssetId": "lGGH01P6AiGjxFOf5JD81q01U0171K5014n8o2x800lJH1400"
							},
							"_id": "cCHMkFUt5UU1S5eeML4Ih8",
							"state": "ready",
							"duration": 29.044,
							"_type": "videoResource"
						},
						"solution": null,
						"_id": "3YUWuQVY9BQaBV1o5nO6FO",
						"description": "Learn how to configure Git with your user information so it shows up in your commits.",
						"body": "Before you can commit anything, you need to add some git configuration. Assuming you're not on a shared machine, you can add this globally.\n\nYou'll need to add a username and a user email address that you'd like to show up in your commits.\n\nUse the `config` command with the `--global` flag to set your username and email with these commands:\n\n```markdown\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@address.com\"\n```\n\nAt any time, you can see your git config using the list flag:\n\n```markdown\ngit config --list\n```\n\nThis command will display the two options you provided, alongside some system defaults. With a user and email, you're now ready to sign your commits.",
						"slug": "setting-up-git-configuration-for-commits",
						"_type": "lesson"
					},
					{
						"_updatedAt": "2024-03-13T15:44:47Z",
						"videoResource": {
							"_rev": "WsAmd18YA1frbQaWBZpHcA",
							"_type": "videoResource",
							"state": "ready",
							"title": "07-git-commit",
							"duration": 41.360667,
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:07,360\nuse git commit to make a commit commits require a message which we can use the dash m flag for\n\n2\n00:00:07,360 --> 00:00:14,720\ni like to use present tense commit messages like add help init and status notes now instead of a\n\n3\n00:00:14,720 --> 00:00:21,200\ncommit we actually got the status output here and that's because no files are staged to be committed\n\n4\n00:00:21,200 --> 00:00:27,040\nwe can fix that with git add dot which adds all those files and running our commit command again\n\n5\n00:00:27,040 --> 00:00:32,240\nwe see the branch it's abbreviated commit shaw and the message that we wrote three files were\n\n6\n00:00:32,240 --> 00:00:38,000\nchanged with three insertions creating these three files and now when we run git status again\n\n7\n00:00:38,000 --> 00:00:41,119\nwe see that there's nothing to commit and we have a clean working tree\n\n\n",
								"text": "[00:00] use git commit to make a commit commits require a message which we can use the dash m flag for i like to use present tense commit messages like add help init and status notes now instead of a\n\n[00:14] commit we actually got the status output here and that's because no files are staged to be committed we can fix that with git add dot which adds all those files and running our commit command again we see the branch it's abbreviated commit shaw and the message that we wrote three files were\n\n[00:32] changed with three insertions creating these three files and now when we run git status again we see that there's nothing to commit and we have a clean working tree"
							},
							"muxAsset": {
								"muxAssetId": "nKFMPv1Rq1BOaGzyV2OtpWGgGhAb9mTODZchWpUeYu8",
								"_type": "muxAsset",
								"muxPlaybackId": "Vb3a35893RR3fvpV7FjzOSDvb7UI4CNNUyd1Lt7KkRs"
							},
							"_id": "cCHMkFUt5UU1S5eeML4Iqi",
							"_updatedAt": "2024-03-19T20:21:29Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/07-git-commit.mp4",
							"_createdAt": "2024-03-12T20:04:49Z"
						},
						"solution": null,
						"description": "Learn how to manage your code with git by committing changes, staging files, and checking the status of your working tree.",
						"body": "The `git commit` command is used to make a commit. Commits require a message, which is added with the `-m` flag.\n\nIt's a good practice to use present tense commit messages:\n\n```\ngit commit -m \"Add help, init, and status notes\"\n```\n\nAfter the `commit` command finishes, it will show the branch, its abbreviated commit SHA, and the message that we wrote:\n\n```\n$ git commit -m \"Add help, init, and status notes\"\n\n[main (root-commit) 1e3b9b0] Add help, init, and status notes\n 3 files changed, 3 insertions(+)\n create mode 100644 git-help.txt\n create mode 100644 git-init.txt\n create mode 100644 git-status.txt\n```\n\nNote that if no files are staged to be committed, the `commit` command will return the status output instead of actually making a commit.\n\nAfter a successful commit, running `git status` again will report that there's nothing to commit and we have a clean working tree:\n\n```\n$ git status\n\nOn branch main\nnothing to commit, working tree clean\n```",
						"slug": "understanding-git-commit-add-and-status",
						"_id": "uMTXJA9nbLKuwVJ2I4qhgX",
						"_type": "lesson",
						"title": "Understanding Git Commit, Add, and Status"
					},
					{
						"_id": "3YUWuQVY9BQaBV1o5nO7va",
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:49Z",
						"slug": "inspecting-commit-history-with-git-show",
						"title": "Inspecting Commit History with `git show`",
						"description": "Discover how to inspect and navigate through git commit history using `git show` and various arguments, such as commit SHA and aliases.",
						"body": "`git show` is a command that allows you to inspect your commits. When used without any arguments, it will display the latest commit, including the full diff that the commit composes:\n\n```markdown\n$ git show\n\ncommit 1e3b9b0061712908132409ssdf234 (HEAD -> main)\nAuthor: Your Name <youremail@address.com>\nDate: Mon Jan 1 00:00:00 2021 -0400\n\n    Add help, init, and status notes\n\ndiff --git a/git-help.txt b/git-help.txt\nnew file mode 100644\nindex 0000000..e69de29\n--- /dev/null\n+++ b/git-help.txt\n@@ -0,0 +1 @@\n+#git-help\ndiff --git a/git-init.txt b/git-init.txt\nnew file mode 100644\nindex 0000000..23b0b9e\n```\n\nPressing `q` on your keyboard will exit this view.\n\nYou can also be explicit with `git show` by using the commit SHA. You can either use the full shaw or the abbreviated six-character version:\n\n```markdown\ngit show 1e3b9b\n```\n\nIf the commit you want to inspect is at the head of the main branch, you can use the `HEAD` alias instead of the commit SHA:\n\n```markdown\ngit show HEAD\n```\n\nThese commands will display the same information as in the example above.",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "LJm02VLjDxfEeaPHlR7OM01NiwbQw41EKU9hSxsY3ZULQ",
								"muxAssetId": "BYhwOab30201aKsMer9zvc33qM7KlK4CBo4aGEuJqrBAQ",
								"_type": "muxAsset"
							},
							"_rev": "9CeTKuUcQZRsVUft8VlPgi",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/08-git-show.mp4",
							"duration": 38.044,
							"state": "ready",
							"title": "08-git-show",
							"_updatedAt": "2024-03-19T20:21:30Z",
							"transcript": {
								"text": "[00:00] git show is the command that we use to inspect our commits. Without arguments, it will show the latest commit, including the full diff that the commit composes. Use Q to get out of this view. We can be explicit with git show using the commit SHA. Type git show the SHA ID.\n\n[00:19] Now, that's a pretty big ID, so the abbreviated versions work as well. Let's copy just the first six characters and run git show with that. You'll notice that this commit is at head on the main branch, so we can also use the head alias, git show head.",
								"srt": "1\n00:00:00,000 --> 00:00:03,800\ngit show is the command that we use to inspect our commits.\n\n2\n00:00:03,800 --> 00:00:06,480\nWithout arguments, it will show the latest commit,\n\n3\n00:00:06,480 --> 00:00:10,280\nincluding the full diff that the commit composes.\n\n4\n00:00:10,280 --> 00:00:12,160\nUse Q to get out of this view.\n\n5\n00:00:12,160 --> 00:00:16,280\nWe can be explicit with git show using the commit SHA.\n\n6\n00:00:16,280 --> 00:00:19,320\nType git show the SHA ID.\n\n7\n00:00:19,320 --> 00:00:21,000\nNow, that's a pretty big ID,\n\n8\n00:00:21,000 --> 00:00:23,680\nso the abbreviated versions work as well.\n\n9\n00:00:23,680 --> 00:00:28,799\nLet's copy just the first six characters and run git show with that.\n\n10\n00:00:28,799 --> 00:00:32,480\nYou'll notice that this commit is at head on the main branch,\n\n11\n00:00:32,480 --> 00:00:35,119\nso we can also use the head alias,\n\n12\n00:00:35,119 --> 00:00:37,759\ngit show head.\n\n\n"
							},
							"_createdAt": "2024-03-12T20:04:49Z",
							"_type": "videoResource",
							"_id": "cCHMkFUt5UU1S5eeML4J0I"
						},
						"solution": null
					},
					{
						"solution": null,
						"_id": "PoPWMVqVlifcLTjN9EOfxd",
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:50Z",
						"title": "Interactive Git Staging",
						"body": "Before we learn a new command, let's work through a quick exercise to practice the git subcommands we've covered so far.\n\n## Check `git status`\n\nRunning `git status` shows that we have several files that are not yet staged for commit:\n\n```markdown\n$ git status\n\nOn branch main\nUntracked files:\n(use \"git add <file>...\" to include in what will be committed)\ngit-add.txt\ngit-commit.txt\ngit-config.txt\ngit-log.txt\ngit-reset.txt\ngit-show.txt\n```\n\n## Add and Commit\n\nLet's add the `git-add.txt` and `git-reset.txt` files:\n\n```markdown\ngit add git-add.txt git-reset.txt\n```\n\nThen we'll commit them and add a message:\n\n```markdown\ngit commit -m \"Add stagingi subcommand notes\"\n```\n\nNotice we made a typo in the commit message!\n\n## Amend a Commit Message\n\nWe can fix the most recent commit message with `git commit --amend`:\n\n```markdown\ngit commit --amend\n```\n\nThis will open the terminal text editor (usually Vim) to edit the message:\n\nIn Vim, use the arrow keys to navigate to the incorrect character, press `x` to delete it, and then type `:wq` to save and quit.\n\nIf there was more than a single character that needs fixed, you could use `i` to enter Vim's insert mode and fix the message. Hit `escape` to exit insert mode and type `:wq` to save and quit.\n\n## See Commits with `git show`\n\nBack in the terminal, run `git show` to see the last commit and verify that the message is fixed.\n\nTo check previous commits, use `git show head^` will show us the commit before last. The `^` is a relative reference added to `head` which says we want the previous commit:\n\n```markdown\ngit show head^\n```\n\nIn this case, we'll see the commit from earlier when we added the `git-help.txt`, `git-init.txt`, and `git-status.txt` files.\n\nPressing `q` will exit the commit view.\n\n## Stage and Commit Remaining Files\n\nNow, let's stage the remaining files with `git add .`:\n\n```markdown\n$ git add .\n\nOn branch main\nChanges to be committed:\n(use \"git restore --staged <file>...\" to unstage)\nnew file: git-commit.txt\nnew file: git-config.txt\nnew file: git-log.txt\nnew file: git-show.txt\n```\n\nBecause the `git-config.txt` file operates on the git configuration and not on commits, we want to keep this file separate from the other commands.\n\n## Unstage a File\n\nTo unstage the `git-config.txt` file, use `git reset`:\n\n```markdown\ngit reset git-config.txt\n```\n\nAlternatively, you could unstage all files with `git reset .` then add back what you need to commit.\n\nWe've successfully staged, committed, fixed a commit message, and unstaged a file using various git subcommands, but we had to\n\n## Use String Globs as Wildcards\n\nIf you're tired of typing the full file name, you can use a string glob to identify a file.\n\nFor example, we could target `get-config.txt` with the following:\n\n```markdown\ngit add '_con_'\n```\n\nSince the `con` substring is unique to the `git-config.txt` file, it will be the only one added.\n\nOnce the file is added, we can commit the changes:\n\n```markdown\ngit commit -m \"Add username and email config\"\n```\n\n## Add Files in Interactive Mode\n\nTo add the remaining files, let's use `git add` in interactive mode:\n\n```markdown\ngit add -i\n```\n\nThis mode allows us to interact with the staging area without having to prefix all of our commands with `git`:\n\n![git interactive screenshot](https://res.cloudinary.com/epic-web/image/upload/v1709677836/tutorials/git-foundations/Screenshot_2024-03-05_at_4.14.07_PM.png)\n\nIn the \"What now?\" space, type the letter of the corresponding command you want to use.\n\nWe want to use \"add untracked\", so we'll enter `a`.\n\nNow, you'll see the remaining untracked files. Use the numbers to select the file.\n\n![files added in interactive mode](https://res.cloudinary.com/epic-web/image/upload/v1709677837/tutorials/git-foundations/Screenshot_2024-03-05_at_4.16.07_PM.png)\n\nOnce you're done, hit enter again to go back to the \"What now?\" prompt.\n\n## Check the Status in Interactive Mode\n\nTo check the status, type `s` at the prompt. It will show that the three files are now staged:\n\n```markdown\nWhat now> s\nstaged unstaged path\n1: +1/-0 nothing git-commit.txt\n2: +1/-0 nothing git-log.txt\n3: +1/-0 nothing git-show.txt\n```\n\nYou can revert any changes by inputting `r` at the prompt if needed.\n\nIf everything looks good, hit enter to get out of that and use `q` to quit the interactive mode.\n\n## Check `git status` and Commit\n\nAfter exiting interactive mode, `git status` will show everything that we committed with `git add` interactive mode. Go ahead and commit the changes with `git commit -m \"Add commit subcommands\"`.\n\nNow you've learned how to edit commit messages, use string globs to select files, and use git in interactive mode. You're ready to move on to the next command.",
						"slug": "interactive-git-staging",
						"description": "Learn how to efficiently add, reset, and amend git commit messages with a step-by-step exercise on staging and committing changes.",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:21:27Z",
							"state": "ready",
							"title": "09-workflow-exercise",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/09-workflow-exercise.mp4",
							"duration": 196.044,
							"_createdAt": "2024-03-12T20:04:51Z",
							"_id": "cCHMkFUt5UU1S5eeML4J9s",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:03,600\nTo show off the next command, I need more than one commit.\n\n2\n00:00:03,600 --> 00:00:06,880\nSo let's take a break and run a quick workflow exercise.\n\n3\n00:00:06,880 --> 00:00:10,480\nRunning git status, we can see all of the new files that I've added for\n\n4\n00:00:10,480 --> 00:00:12,640\nthe git subcommands that we've covered.\n\n5\n00:00:12,640 --> 00:00:15,280\nAdd, commit, config, log, reset, and show.\n\n6\n00:00:15,280 --> 00:00:17,600\nLet's commit add and reset.\n\n7\n00:00:17,600 --> 00:00:23,040\nGit add the git add text file and git reset text file.\n\n8\n00:00:23,040 --> 00:00:27,520\nRun git status again and see that just these two files are staged for commit.\n\n9\n00:00:27,520 --> 00:00:28,400\nThat's perfect.\n\n10\n00:00:28,400 --> 00:00:35,200\nNow we can git commit those with a message add staging subcommand notes.\n\n11\n00:00:35,200 --> 00:00:37,599\nNow my message actually has an error in it.\n\n12\n00:00:37,599 --> 00:00:39,200\nSo let me show you a quick freebie.\n\n13\n00:00:39,200 --> 00:00:43,279\nRun git commit amend to change our last commit message.\n\n14\n00:00:43,840 --> 00:00:49,200\nThis will open in your terminal text editor, which is typically vim on most systems.\n\n15\n00:00:49,200 --> 00:00:54,400\nIn this case, I can use the arrow keys to navigate to that character and hit x to delete it.\n\n16\n00:00:54,400 --> 00:00:58,400\ncolon wq for write quit to quit vim.\n\n17\n00:00:58,400 --> 00:01:04,480\nRun git show to show our last commit and see that the message is in fact fixed.\n\n18\n00:01:04,480 --> 00:01:08,320\nWe can use git show with a relative reference to head.\n\n19\n00:01:08,320 --> 00:01:10,000\nAnd now that we have two commits,\n\n20\n00:01:10,000 --> 00:01:14,080\nuse the caret to reference previous commits relative to that head.\n\n21\n00:01:14,080 --> 00:01:17,599\nThis was our first commit where we added help init and status notes.\n\n22\n00:01:17,599 --> 00:01:21,440\nHit q to escape and to see what's left to commit.\n\n23\n00:01:21,440 --> 00:01:24,559\nUse git add dot to commit all of these.\n\n24\n00:01:26,800 --> 00:01:31,360\nBut config is distinctly different from these other commands which operate on commits.\n\n25\n00:01:31,360 --> 00:01:37,440\nSo run git reset the opposite command of git add and pass the files that we'd like to remove.\n\n26\n00:01:37,440 --> 00:01:39,760\nCommit git log, etc.\n\n27\n00:01:39,760 --> 00:01:44,639\nThat's a lot to type, so it may be easier to use the shortcut dot to reset all of it.\n\n28\n00:01:46,080 --> 00:01:48,720\nAnd add back the git config file.\n\n29\n00:01:48,720 --> 00:01:52,959\nIf we get tired of typing this git prefix with the dot txt suffix,\n\n30\n00:01:52,959 --> 00:01:56,639\ninstead I could use a string glob to use an identifying\n\n31\n00:01:56,639 --> 00:02:00,959\nsubstring surrounded by any number of other characters.\n\n32\n00:02:00,959 --> 00:02:05,120\nIn this case, I know that con is unique to the config file.\n\n33\n00:02:05,120 --> 00:02:07,519\nOf course, I need to use the add subcommand here.\n\n34\n00:02:08,800 --> 00:02:13,839\nAnd git status reveals that we added only this git config text file.\n\n35\n00:02:13,839 --> 00:02:18,880\nLet's commit this with the message add username and email config.\n\n36\n00:02:18,880 --> 00:02:21,119\nAnd make one last commit for the remaining files.\n\n37\n00:02:21,679 --> 00:02:26,720\nThis time, let's use git add interactive, which can be summoned with i for short.\n\n38\n00:02:26,720 --> 00:02:31,279\nGit add interactive allows us to interact with the staging area\n\n39\n00:02:31,279 --> 00:02:33,839\nwithout having to prefix all of our commands with git.\n\n40\n00:02:33,839 --> 00:02:37,520\nIn the what now space, I'll type a letter of the command that I want to use.\n\n41\n00:02:37,520 --> 00:02:39,679\nHere I'll use add untracked.\n\n42\n00:02:39,679 --> 00:02:41,039\nHit a and enter.\n\n43\n00:02:41,039 --> 00:02:46,399\nWe see the remaining untracked files, and we can use our numbers to select them.\n\n44\n00:02:46,399 --> 00:02:48,479\nOne, two, and three.\n\n45\n00:02:48,479 --> 00:02:50,160\nOnce we're done, hit enter again.\n\n46\n00:02:50,160 --> 00:02:51,839\nS for status.\n\n47\n00:02:51,839 --> 00:02:53,839\nSee that these three files are now staged.\n\n48\n00:02:53,839 --> 00:02:57,279\nWe can revert any changes if we want to with r.\n\n49\n00:02:57,279 --> 00:03:00,000\nBut this is looking good, so hit enter to get out of that.\n\n50\n00:03:00,000 --> 00:03:02,559\nAnd q to quit this interactive mode.\n\n51\n00:03:03,119 --> 00:03:07,759\nNow git status will show us everything that we committed with git add interactive,\n\n52\n00:03:07,759 --> 00:03:08,880\nwhich we can commit.\n\n53\n00:03:09,839 --> 00:03:11,600\nAdd commit subcommands.\n\n54\n00:03:12,880 --> 00:03:15,759\nNow with multiple commits, you can learn the next command.\n\n\n",
								"text": "[00:00] To show off the next command, I need more than one commit. So let's take a break and run a quick workflow exercise. Running git status, we can see all of the new files that I've added for the git subcommands that we've covered. Add, commit, config, log, reset, and show. Let's commit add and reset.\n\n[00:17] Git add the git add text file and git reset text file. Run git status again and see that just these two files are staged for commit. That's perfect. Now we can git commit those with a message add staging subcommand notes. Now my message actually has an error in it.\n\n[00:37] So let me show you a quick freebie. Run git commit amend to change our last commit message. This will open in your terminal text editor, which is typically vim on most systems. In this case, I can use the arrow keys to navigate to that character and hit x to delete it.\n\n[00:54] colon wq for write quit to quit vim. Run git show to show our last commit and see that the message is in fact fixed. We can use git show with a relative reference to head. And now that we have two commits, use the caret to reference previous commits relative to that head.\n\n[01:14] This was our first commit where we added help init and status notes. Hit q to escape and to see what's left to commit. Use git add dot to commit all of these. But config is distinctly different from these other commands which operate on commits.\n\n[01:31] So run git reset the opposite command of git add and pass the files that we'd like to remove. Commit git log, etc. That's a lot to type, so it may be easier to use the shortcut dot to reset all of it. And add back the git config file.\n\n[01:48] If we get tired of typing this git prefix with the dot txt suffix, instead I could use a string glob to use an identifying substring surrounded by any number of other characters. In this case, I know that con is unique to the config file. Of course, I need to use the add subcommand here.\n\n[02:08] And git status reveals that we added only this git config text file. Let's commit this with the message add username and email config. And make one last commit for the remaining files. This time, let's use git add interactive, which can be summoned with i for short.\n\n[02:26] Git add interactive allows us to interact with the staging area without having to prefix all of our commands with git. In the what now space, I'll type a letter of the command that I want to use. Here I'll use add untracked. Hit a and enter. We see the remaining untracked files, and we can use our numbers to select them.\n\n[02:46] One, two, and three. Once we're done, hit enter again. S for status. See that these three files are now staged. We can revert any changes if we want to with r. But this is looking good, so hit enter to get out of that. And q to quit this interactive mode.\n\n[03:03] Now git status will show us everything that we committed with git add interactive, which we can commit. Add commit subcommands. Now with multiple commits, you can learn the next command."
							},
							"muxAsset": {
								"muxPlaybackId": "c9LnhkkTn3T2hh02dOjg50097X6oQo00294h1sJPr4nXF8",
								"muxAssetId": "DUjkpw018PaaUVxcxurQ6MqDzbk5f4ezPj5HQuLa91900",
								"_type": "muxAsset"
							},
							"_rev": "6LaeqP6n94P8FD3sVhYByo",
							"_type": "videoResource"
						}
					},
					{
						"_id": "3YUWuQVY9BQaBV1o5nO8ro",
						"videoResource": {
							"title": "10-git-log",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/10-git-log.mp4",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:03,800\nGit log is used to see a list of commits.\n\n2\n00:00:03,800 --> 00:00:07,160\nThis is our commit for adding commit sub-command notes.\n\n3\n00:00:07,160 --> 00:00:10,860\nThis is our commit for adding the username and email config note,\n\n4\n00:00:10,860 --> 00:00:14,280\nand we can hit space to continue on through history.\n\n5\n00:00:14,280 --> 00:00:16,260\nNow, this view is a little wordy for me,\n\n6\n00:00:16,260 --> 00:00:20,000\nso I prefer git log with the one line flag.\n\n7\n00:00:20,000 --> 00:00:23,100\nHere we see just the most relevant information,\n\n8\n00:00:23,100 --> 00:00:25,540\neight unique characters from each SHA,\n\n9\n00:00:25,540 --> 00:00:27,940\nthe head commit and which branch we're on,\n\n10\n00:00:27,940 --> 00:00:29,559\nand the commit messages.\n\n11\n00:00:29,559 --> 00:00:33,139\nIf you like this log format and want to save it to config,\n\n12\n00:00:33,139 --> 00:00:36,900\nuse the format.pretty one-line option.\n\n13\n00:00:36,900 --> 00:00:39,820\nNow, we can run git log with just SHA,\n\n14\n00:00:39,820 --> 00:00:42,820\nbranch, and commit message in one line.\n\n15\n00:00:42,820 --> 00:00:45,860\nNow, you'll notice that this commit is significantly longer.\n\n16\n00:00:45,860 --> 00:00:47,099\nThis is the full commit.\n\n17\n00:00:47,099 --> 00:00:53,419\nWe also need to set the git config log.abbrev commit to true.\n\n18\n00:00:53,419 --> 00:00:55,660\nLet's try that one more time for\n\n19\n00:00:55,660 --> 00:00:59,919\nnice concise log messages with just the git log command.\n\n20\n00:00:59,919 --> 00:01:02,439\nAs always, you can inspect your git config with\n\n21\n00:01:02,439 --> 00:01:06,440\ngit config list to see which options are being applied.\n\n\n",
								"text": "[00:00] Git log is used to see a list of commits. This is our commit for adding commit sub-command notes. This is our commit for adding the username and email config note, and we can hit space to continue on through history. Now, this view is a little wordy for me,\n\n[00:16] so I prefer git log with the one line flag. Here we see just the most relevant information, eight unique characters from each SHA, the head commit and which branch we're on, and the commit messages. If you like this log format and want to save it to config,\n\n[00:33] use the format.pretty one-line option. Now, we can run git log with just SHA, branch, and commit message in one line. Now, you'll notice that this commit is significantly longer. This is the full commit.\n\n[00:47] We also need to set the git config log.abbrev commit to true. Let's try that one more time for nice concise log messages with just the git log command. As always, you can inspect your git config with git config list to see which options are being applied."
							},
							"_rev": "WsAmd18YA1frbQaWBZpHU2",
							"_type": "videoResource",
							"duration": 66.110667,
							"_createdAt": "2024-03-12T20:04:52Z",
							"_id": "cCHMkFUt5UU1S5eeML4JJS",
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "2BrbhjuPTNfsP9fQR0102YUa5ckaNzvhzXo5LJ6wIrOAI",
								"muxAssetId": "evz28yvZT00eAU2Tzh4Wxf7o9jFZWQchCa6yp3KUP3ms"
							},
							"state": "ready",
							"_updatedAt": "2024-03-19T20:21:25Z"
						},
						"body": "The `git log` command shows a list of commits in your repository.\n\nBy default, the output is verbose and shows detailed information about each commit:\n\n![Git log default view](https://res.cloudinary.com/epic-web/image/upload/v1709678379/tutorials/git-foundations/Screenshot_2024-03-05_at_4.33.18_PM.png)\n\nUsing the `--oneline` flag will give a more concise view of the commit history. This will display just the most relevant information for each commit, including the first 7 characters of each SHA and its message:\n\n```bash\ngit log --oneline\n```\n\n![Git log one line view](https://res.cloudinary.com/epic-web/image/upload/v1709678384/tutorials/git-foundations/Screenshot_2024-03-05_at_4.34.21_PM.png)\n\nIf you like this concise format and want to save it as the default setting, you can configure Git to always use the `oneline` format by setting the `format.pretty` option in your `git config`:\n\n```bash\ngit config format.pretty oneline\n```\n\nInterestingly, adding the `online` option to the config results in the full SHA being shown in the log output. To ensure the abbreviated SHA is used, set the `log.abbrevCommit` option to `true`:\n\n```bash\ngit config log.abbrevCommit true\n```\n\nWith these configuration updates, `git log` will display nice, concise log messages.\n\nYou can inspect your Git configuration with the `git config --list` command to see which options are being applied.",
						"slug": "optimizing-git-log-output-for-better-workflow",
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:51Z",
						"title": "Optimizing Git Log Output for Better Workflow",
						"description": "Learn how to customize the git log output to display concise and relevant commit information. We'll be using different flags and config settings to achieve the perfect git commit overview."
					},
					{
						"slug": "syncing-local-and-remote-git-repositories-with-github",
						"solution": null,
						"_id": "uMTXJA9nbLKuwVJ2I4qj9V",
						"_updatedAt": "2024-03-13T15:44:52Z",
						"description": "Learn how to synchronize local and remote repositories on GitHub, along with understanding the usage of git remote, git branch, and git push commands for seamless collaboration.",
						"body": "Git repositories are designed to be distributed, so let's share `my-git-notes` online.\n\n## Sharing a Repository on GitHub\n\nWhen you create a new repository on GitHub, it will provide instructions on how to start with your remote repository. Because we already have a repository, we're going to use the \"push an existing repository from the command line\" instructions:\n\n```bash\ngit remote add origin git@github.com:username/my-git-notes.git\ngit branch -M main\ngit push -u origin main\n```\n\nLet's read through those commands:\n\n1. `git remote add origin`: Origin is how we refer to our remote repository locally. You could choose a different name, but origin is pretty conventional. The origin repository lives at the address you use in your command, in this case, it lives at `https://github.com/chantastic/my-git-notes.git`\n\n2. `git branch -M main`: This command changes the primary branch to main.\n\n3. `git push -u origin main`: This command ensures that our local branch and origin branch share the same relative name `main`.\n\nAfter running the commands, we'll see the status of what's being pushed up to the remote repository, and a confirmation that this new remote branch has been added and is being tracked:\n\n![git push output](https://res.cloudinary.com/epic-web/image/upload/v1709679797/tutorials/git-foundations/Screenshot_2024-03-05_at_4.57.31_PM.png)\n\nRefreshing the GitHub repo page in the browser will confirm the files made are now available.",
						"videoResource": {
							"_createdAt": "2024-03-12T20:04:53Z",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:04,080\nGit repositories are designed to be distributed,\n\n2\n00:00:04,080 --> 00:00:06,400\nso let's share this one online.\n\n3\n00:00:06,400 --> 00:00:08,720\nNow, I'm using GitHub as a Git provider,\n\n4\n00:00:08,720 --> 00:00:10,900\nbut you can use any that you're familiar with.\n\n5\n00:00:10,900 --> 00:00:14,280\nI've created a MyGitNotesRepository.\n\n6\n00:00:14,280 --> 00:00:15,960\nI've used the same name here,\n\n7\n00:00:15,960 --> 00:00:19,360\nbut it does not have to be the same name as our local repository.\n\n8\n00:00:19,360 --> 00:00:23,680\nNow, GitHub does something really nice on new empty repositories,\n\n9\n00:00:23,680 --> 00:00:28,440\nproviding instructions on how to start with your remote repository.\n\n10\n00:00:28,440 --> 00:00:30,280\nBecause we already have a repository,\n\n11\n00:00:30,280 --> 00:00:33,200\nwe're going to use these push an existing repository\n\n12\n00:00:33,200 --> 00:00:35,320\nfrom the command line instructions.\n\n13\n00:00:35,320 --> 00:00:38,040\nYou can just copy and run.\n\n14\n00:00:39,320 --> 00:00:42,520\nLet's read through those commands and the output.\n\n15\n00:00:42,520 --> 00:00:46,279\nFirst, we use git remote to add an origin.\n\n16\n00:00:46,279 --> 00:00:50,360\nOrigin is how we refer to our remote repository locally.\n\n17\n00:00:50,360 --> 00:00:51,959\nYou could choose a different name,\n\n18\n00:00:51,959 --> 00:00:53,880\nbut origin is pretty conventional.\n\n19\n00:00:53,880 --> 00:00:57,200\nThe origin repository lives at this address,\n\n20\n00:00:57,200 --> 00:01:02,520\ngit.github.com colon Chantastic slash MyGitNotes.git.\n\n21\n00:01:02,520 --> 00:01:05,320\nWe change the primary branch to main,\n\n22\n00:01:05,320 --> 00:01:08,360\nwhich is extraneous for us because we were already using main,\n\n23\n00:01:08,360 --> 00:01:10,239\nand then use git push,\n\n24\n00:01:10,239 --> 00:01:13,519\nsetting the upstream branch to origin main,\n\n25\n00:01:13,519 --> 00:01:15,320\nso that our local branch,\n\n26\n00:01:15,320 --> 00:01:20,120\nmain, and origin branch name share the same relative name.\n\n27\n00:01:20,120 --> 00:01:23,160\nAll of this output is a result of the push command,\n\n28\n00:01:23,160 --> 00:01:27,040\njust the status of what's being pushed up to the remote repository,\n\n29\n00:01:27,040 --> 00:01:31,000\nand a confirmation that this new remote branch has been added,\n\n30\n00:01:31,000 --> 00:01:33,919\nand our local branch is tracking to that origin branch.\n\n31\n00:01:33,919 --> 00:01:37,279\nLet's clear this and run through some of these subcommands.\n\n32\n00:01:37,279 --> 00:01:38,839\nFirst, we have git remote.\n\n33\n00:01:38,839 --> 00:01:40,320\nOur only remote is origin,\n\n34\n00:01:40,320 --> 00:01:41,320\nwhich we just added,\n\n35\n00:01:41,320 --> 00:01:45,360\nand we can see details with git remote v for verbose.\n\n36\n00:01:45,360 --> 00:01:46,919\nThere are options for removing,\n\n37\n00:01:46,919 --> 00:01:49,279\nediting, or updating these options,\n\n38\n00:01:49,279 --> 00:01:52,239\nwhich you can find using git help remote.\n\n39\n00:01:52,239 --> 00:01:53,680\nBut in most common cases,\n\n40\n00:01:53,680 --> 00:01:54,800\nyou set it up and you're done.\n\n41\n00:01:54,800 --> 00:01:56,879\nThe other subcommand was git branch.\n\n42\n00:01:56,879 --> 00:01:59,480\nNow, locally, we only have our main branch,\n\n43\n00:01:59,480 --> 00:02:01,160\nbut if we run git branch,\n\n44\n00:02:01,160 --> 00:02:03,320\nthe A or all flag,\n\n45\n00:02:03,320 --> 00:02:04,879\nwe'll see that there are technically two.\n\n46\n00:02:04,879 --> 00:02:07,839\nThere's main and remotes origin main.\n\n47\n00:02:07,839 --> 00:02:10,279\nKnowing that these are two separate branches\n\n48\n00:02:10,279 --> 00:02:11,839\nrepresented on your local machine\n\n49\n00:02:11,839 --> 00:02:14,440\nwill come in really handy in future lessons.\n\n50\n00:02:14,440 --> 00:02:17,119\nFor now, let's look at that last command, git push.\n\n51\n00:02:17,119 --> 00:02:22,119\nGit push is how we sync our local repository to our remote repository.\n\n52\n00:02:22,119 --> 00:02:23,119\nWe've already done that,\n\n53\n00:02:23,119 --> 00:02:24,800\nso there's nothing left to sync.\n\n54\n00:02:24,880 --> 00:02:26,119\nLet's go back to the web browser\n\n55\n00:02:26,119 --> 00:02:29,559\nand see if all of our changes landed on our remote\n\n56\n00:02:29,559 --> 00:02:31,000\nby hitting refresh.\n\n57\n00:02:31,000 --> 00:02:31,839\nAnd here they are.\n\n58\n00:02:31,839 --> 00:02:34,199\nEverything that we've written so far and committed\n\n59\n00:02:34,199 --> 00:02:37,320\nis represented on this main remote branch.\n\n\n",
								"text": "[00:00] Git repositories are designed to be distributed, so let's share this one online. Now, I'm using GitHub as a Git provider, but you can use any that you're familiar with. I've created a MyGitNotesRepository. I've used the same name here, but it does not have to be the same name as our local repository.\n\n[00:19] Now, GitHub does something really nice on new empty repositories, providing instructions on how to start with your remote repository. Because we already have a repository, we're going to use these push an existing repository from the command line instructions. You can just copy and run.\n\n[00:39] Let's read through those commands and the output. First, we use git remote to add an origin. Origin is how we refer to our remote repository locally. You could choose a different name, but origin is pretty conventional. The origin repository lives at this address,\n\n[00:57] git.github.com colon Chantastic slash MyGitNotes.git. We change the primary branch to main, which is extraneous for us because we were already using main, and then use git push, setting the upstream branch to origin main, so that our local branch,\n\n[01:15] main, and origin branch name share the same relative name. All of this output is a result of the push command, just the status of what's being pushed up to the remote repository, and a confirmation that this new remote branch has been added, and our local branch is tracking to that origin branch.\n\n[01:33] Let's clear this and run through some of these subcommands. First, we have git remote. Our only remote is origin, which we just added, and we can see details with git remote v for verbose. There are options for removing, editing, or updating these options, which you can find using git help remote. But in most common cases,\n\n[01:53] you set it up and you're done. The other subcommand was git branch. Now, locally, we only have our main branch, but if we run git branch, the A or all flag, we'll see that there are technically two. There's main and remotes origin main. Knowing that these are two separate branches represented on your local machine\n\n[02:11] will come in really handy in future lessons. For now, let's look at that last command, git push. Git push is how we sync our local repository to our remote repository. We've already done that, so there's nothing left to sync. Let's go back to the web browser and see if all of our changes landed on our remote by hitting refresh. And here they are.\n\n[02:31] Everything that we've written so far and committed is represented on this main remote branch."
							},
							"_updatedAt": "2024-03-19T20:21:24Z",
							"_type": "videoResource",
							"_id": "cCHMkFUt5UU1S5eeML4JT2",
							"title": "11-git-remote",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/11-git-remote.mp4",
							"duration": 157.644,
							"state": "ready",
							"muxAsset": {
								"muxPlaybackId": "lYOvl4tf00Z028R00JSArozvu2nW6y9IYRfyVsJpH2zDS8",
								"muxAssetId": "56Xz00o501pjzd2zy018VYeYR6T01b2UeuMbM2BbuPHowdc",
								"_type": "muxAsset"
							},
							"_rev": "WsAmd18YA1frbQaWBZpHOc"
						},
						"_type": "lesson",
						"title": "Syncing Local and Remote Git Repositories with GitHub"
					},
					{
						"_updatedAt": "2024-03-13T15:44:53Z",
						"body": "`git push` is how we sync commits from our local repository to our remote repository.\n\nRight now, all of the commits in our `git log` are already synced, so there's nothing to push. But if we run `git status`, we see that a new file for `git-remote.txt` has been added, which we can commit and sync.\n\nLet's add everything and commit it using the message \"add remote note\":\n\n```markdown\ngit add .\ngit commit -m \"add remote note\"\n```\n\nNow, let's run `git status` again, where we'll see a message we haven't seen before:\n\n![git status output](https://res.cloudinary.com/epic-web/image/upload/v1709680722/tutorials/git-foundations/Screenshot_2024-03-05_at_5.14.41_PM.png)\n\nLet's do what it says, `git push`. Assuming everything was successful, we can go to our remote repository and hit refresh to see the updates. Here we see our Git remote text note that was added one minute ago.\n\n![Screenshot: Remote Repository with Git Remote Text Note](https://res.cloudinary.com/epic-web/image/upload/v1709680724/tutorials/git-foundations/Screenshot_2024-03-05_at_5.17.00_PM.png)",
						"videoResource": {
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:06,200\nGit push is how we sync commits from our local repository to our remote repository.\n\n2\n00:00:06,200 --> 00:00:09,720\nRight now, all of the commits in our Git log are already synced,\n\n3\n00:00:09,720 --> 00:00:11,080\nso there's nothing to push.\n\n4\n00:00:11,080 --> 00:00:12,760\nBut if we run Git status,\n\n5\n00:00:12,760 --> 00:00:18,100\nwe see that I added a new file for Git remote that we can commit and sync.\n\n6\n00:00:18,100 --> 00:00:24,040\nLet's add everything and commit it using the message add remote note.\n\n7\n00:00:24,040 --> 00:00:26,680\nFinally, let's run Git status again,\n\n8\n00:00:26,680 --> 00:00:29,080\nwhere we'll see a message we haven't seen before.\n\n9\n00:00:29,080 --> 00:00:32,919\nYour branch is head of origin main by one commit.\n\n10\n00:00:32,919 --> 00:00:35,840\nUse Git push to publish your local commits.\n\n11\n00:00:35,840 --> 00:00:38,119\nLet's do what it says, Git push.\n\n12\n00:00:38,119 --> 00:00:40,400\nLooks like everything was successful,\n\n13\n00:00:40,400 --> 00:00:44,439\nso we can go to our remote and hit refresh.\n\n14\n00:00:44,439 --> 00:00:50,360\nHere we see our Git remote text note that was added one minute ago.\n\n\n",
								"text": "[00:00] Git push is how we sync commits from our local repository to our remote repository. Right now, all of the commits in our Git log are already synced, so there's nothing to push. But if we run Git status, we see that I added a new file for Git remote that we can commit and sync.\n\n[00:18] Let's add everything and commit it using the message add remote note. Finally, let's run Git status again, where we'll see a message we haven't seen before. Your branch is head of origin main by one commit. Use Git push to publish your local commits.\n\n[00:35] Let's do what it says, Git push. Looks like everything was successful, so we can go to our remote and hit refresh. Here we see our Git remote text note that was added one minute ago."
							},
							"_createdAt": "2024-03-12T20:04:54Z",
							"_type": "videoResource",
							"title": "12-git-push",
							"_updatedAt": "2024-03-19T20:21:22Z",
							"muxAsset": {
								"muxPlaybackId": "AdCR9PI02mbU3tjiJbqsaWz57UY00S1BUOY9HSASLX1ro",
								"muxAssetId": "on02QUkOomzOqYIFIhCooZ3nl8zcBboxWsmQmWKFa301I",
								"_type": "muxAsset"
							},
							"_id": "cCHMkFUt5UU1S5eeML4Jcc",
							"state": "ready",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/12-git-push.mp4",
							"duration": 49.577333,
							"_rev": "6LaeqP6n94P8FD3sVhYBrS"
						},
						"_id": "uMTXJA9nbLKuwVJ2I4qjTy",
						"title": "Syncing Local Commits with Remote Repository",
						"description": "Learn how to use Git push to sync your local commits to a remote repository, ensuring your code stays up-to-date across platforms.",
						"slug": "syncing-local-commits-with-remote-repository",
						"solution": null,
						"_type": "lesson"
					},
					{
						"_id": "3YUWuQVY9BQaBV1o5nOA3M",
						"_updatedAt": "2024-03-13T15:44:54Z",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:21:20Z",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:06,160\nGit pull is how we sync remote commits back down to our local repository.\n\n2\n00:00:06,160 --> 00:00:10,160\nRight now everything is up to date so let's add a file remotely.\n\n3\n00:00:10,160 --> 00:00:16,480\nCreate file, name this git push.txt and give it the heading git push.\n\n4\n00:00:16,480 --> 00:00:22,160\nThat's enough for our purposes so hit commit changes and change this to add push note.\n\n5\n00:00:22,160 --> 00:00:23,680\nFinally commit.\n\n6\n00:00:23,680 --> 00:00:29,200\nNow on our remote repository we have a commit that doesn't exist in our local repository.\n\n7\n00:00:29,200 --> 00:00:32,959\nThe commit that starts with da385e3.\n\n8\n00:00:32,959 --> 00:00:35,840\nLet's run git pull again and see what happens.\n\n9\n00:00:35,840 --> 00:00:40,639\nHere we see the output for that command that resulted in adding this one file with the new\n\n10\n00:00:40,639 --> 00:00:41,360\ncommit.\n\n11\n00:00:41,360 --> 00:00:45,759\nNow when we run git log locally we see that new commit and that head,\n\n12\n00:00:45,759 --> 00:00:48,560\nmain, and origin main are all in sync.\n\n\n",
								"text": "[00:00] Git pull is how we sync remote commits back down to our local repository. Right now everything is up to date so let's add a file remotely. Create file, name this git push.txt and give it the heading git push.\n\n[00:16] That's enough for our purposes so hit commit changes and change this to add push note. Finally commit. Now on our remote repository we have a commit that doesn't exist in our local repository. The commit that starts with da385e3. Let's run git pull again and see what happens.\n\n[00:35] Here we see the output for that command that resulted in adding this one file with the new commit. Now when we run git log locally we see that new commit and that head, main, and origin main are all in sync."
							},
							"state": "ready",
							"title": "13-git-pull",
							"duration": 48.844,
							"_rev": "9CeTKuUcQZRsVUft8VlOM6",
							"_type": "videoResource",
							"_id": "cCHMkFUt5UU1S5eeML4JmC",
							"_createdAt": "2024-03-12T20:04:55Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/13-git-pull.mp4",
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "dtuyn43FwXYQ8i7PZvZhaug3tAKWWiRc1iWZkJSEmzU",
								"muxAssetId": "H4bUcxhaKoNtRARoAP1QggGf2K4fpL9rQDzuTAvln28"
							}
						},
						"solution": null,
						"slug": "syncing-remote-commits-to-local-repository",
						"_type": "lesson",
						"title": "Syncing Remote Commits to Local Repository",
						"description": "Learn the importance of syncing your remote commits to your local repository using Git and how to do it by running the git pull command.",
						"body": "The `git pull` command syncs remote commits back down to the local repository.\n\nIn our case, everything is up to date, so let's add a file remotely to demonstrate the process.\n\n## Adding a File via the GitHub Interface\n\nGitHub provides a web interface to add files to a repository. Let's use it to add a file named `git-push.txt` with the heading \"Git Push\". When finished, commit the changes with the message \"Add push note\".\n\nNow, on our remote repository, we have a commit that doesn't exist in our local repository. In this case, the commit starts with `da385e3`.\n\n![commit screenshot](https://res.cloudinary.com/epic-web/image/upload/v1709681018/tutorials/git-foundations/Screenshot_2024-03-05_at_5.22.12_PM.png)\n\n## Running `git pull` Locally\n\nBack in the terminal, running `git pull` shows that the new file has been added, along with the new commit.\n\n```tsx\nFrom github.com:username/my-git-notes\n   da385e3..f3b3b3e  main     -> origin/main\nUpdating da385e3..f3b3b3e\nFast-forward\n git-push.txt | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 git-push.txt\n```\n\nRunning `git log` will show that the new commit is present and that `HEAD`, `main`, and `origin/main` are all in sync."
					},
					{
						"videoResource": {
							"state": "ready",
							"_createdAt": "2024-03-12T20:04:56Z",
							"title": "14-git-fetch%2Cgit-merge",
							"_updatedAt": "2024-03-19T20:21:19Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/14-git-fetch%2Cgit-merge.mp4",
							"duration": 81.010667,
							"_rev": "WsAmd18YA1frbQaWBZpGmg",
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "56Gww015lTj006dzGycy01cUQvec701200sNjHZ802xiCK9L8",
								"muxAssetId": "EulxvqSMiNJ35VA008GB01XLOiSUdPn3EB02qyA78JQGBE",
								"_type": "muxAsset"
							},
							"_id": "cCHMkFUt5UU1S5eeML4Jvm",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:03,200\nGit pull is actually two commands in one.\n\n2\n00:00:03,200 --> 00:00:05,880\nIt's git fetch and git merge.\n\n3\n00:00:05,880 --> 00:00:08,400\nRemember in our previous lesson when I pointed out that\n\n4\n00:00:08,400 --> 00:00:10,960\ngit branch all has two branches,\n\n5\n00:00:10,960 --> 00:00:13,080\nmain and remote origin main?\n\n6\n00:00:13,080 --> 00:00:14,760\nLet's see what happens when we split\n\n7\n00:00:14,760 --> 00:00:17,520\ngit fetch and git merge from git pull.\n\n8\n00:00:17,520 --> 00:00:19,520\nLet's start by making a new remote file.\n\n9\n00:00:19,520 --> 00:00:21,559\nWe'll make a new node for git pull,\n\n10\n00:00:21,559 --> 00:00:24,360\ntxt, give the heading git pull.\n\n11\n00:00:24,360 --> 00:00:28,120\nCommit as add pull node,\n\n12\n00:00:28,240 --> 00:00:29,959\nwhich gives us commit that only exists\n\n13\n00:00:29,959 --> 00:00:31,559\non our remote repository.\n\n14\n00:00:31,559 --> 00:00:34,400\nNow let's run both commands of git pull,\n\n15\n00:00:34,400 --> 00:00:36,480\nour fetch and merge.\n\n16\n00:00:36,480 --> 00:00:38,360\nFirst, let's run git fetch.\n\n17\n00:00:38,360 --> 00:00:40,639\nGit fetch pulls down all of the changes\n\n18\n00:00:40,639 --> 00:00:43,200\ninto the origin main branch.\n\n19\n00:00:43,200 --> 00:00:45,040\nSo when I run git log,\n\n20\n00:00:45,040 --> 00:00:48,360\nwe don't see our note on git pull just yet.\n\n21\n00:00:48,360 --> 00:00:50,439\nAnd we can run git status to see why.\n\n22\n00:00:50,439 --> 00:00:51,599\nHere we see a new message.\n\n23\n00:00:51,599 --> 00:00:55,040\nOur branch is behind origin main by one commit\n\n24\n00:00:55,040 --> 00:00:56,320\nand can be fast forwarded,\n\n25\n00:00:56,320 --> 00:00:58,919\nrun git pull to update your local branch.\n\n26\n00:00:58,919 --> 00:01:01,160\nUsing git branch all again,\n\n27\n00:01:01,160 --> 00:01:04,279\nit's telling us that this branch is up to date,\n\n28\n00:01:04,279 --> 00:01:06,720\nbut not our main branch, the one that we're on.\n\n29\n00:01:06,720 --> 00:01:10,199\nSo let's run git merge to merge what we've pulled\n\n30\n00:01:10,199 --> 00:01:13,400\nfrom our origin main branch into our main branch.\n\n31\n00:01:13,400 --> 00:01:14,639\nThat succeeded.\n\n32\n00:01:14,639 --> 00:01:17,080\nSo we can run git log.\n\n33\n00:01:17,080 --> 00:01:20,639\nAnd once again, main and origin main are both in sync.\n\n\n",
								"text": "[00:00] Git pull is actually two commands in one. It's git fetch and git merge. Remember in our previous lesson when I pointed out that git branch all has two branches, main and remote origin main? Let's see what happens when we split git fetch and git merge from git pull. Let's start by making a new remote file.\n\n[00:19] We'll make a new note for git pull, txt, give the heading git pull. Commit as add pull note, which gives us commit that only exists on our remote repository. Now let's run both commands of git pull, our fetch and merge. First, let's run git fetch.\n\n[00:38] Git fetch pulls down all of the changes into the origin main branch. So when I run git log, we don't see our note on git pull just yet. And we can run git status to see why. Here we see a new message. Our branch is behind origin main by one commit and can be fast forwarded,\n\n[00:56] run git pull to update your local branch. Using git branch all again, it's telling us that this branch is up to date, but not our main branch, the one that we're on. So let's run git merge to merge what we've pulled from our origin main branch into our main branch. That succeeded.\n\n[01:14] So we can run git log. And once again, main and origin main are both in sync."
							}
						},
						"solution": null,
						"description": "Learn about the two underlying commands of Git Pull, Git Fetch and Git Merge, and how they are used to update your local and remote branches.",
						"slug": "keeping-branches-updated",
						"_updatedAt": "2024-03-13T15:44:55Z",
						"title": "Keeping Branches Updated",
						"body": "When we created our GitHub repo, we ended up with `main` and `remotes/origin/main`.\n\nRunning the command `git branch -a` will show us all of our branches, including the remote ones:\n\n```bash\n$ git branch -a\n\n* main\n  remotes/origin/main\n```\n\nThe `*` indicates the branch we are currently on.\n\nWe used the `git pull` command to pull the remote changes, which is actually a combination of `git fetch` and `git merge`.\n\nLet's explore what happens when we separate `git fetch` and `git merge` from `git pull`.\n\n## Creating a New Remote File\n\nTo start, create a new file `git-pull.txt` using the GitHub interface. Add a heading, then make a commit with the message \"add pull note\".\n\nLike before, we now have a commit that only exists in our remote repository.\n\n## `git fetch`\n\nBack in your terminal, run `git fetch`.\n\nThe `git fetch` pulls down all changes into the origin main branch.\n\nHowever, when we run `git log`, we won't see the commit with the `git-pull.txt` file yet.\n\nRunning `git status` will tell us why:\n\n```tsx\n$ git status\n\nOn branch main\nYour branch is behind 'origin/main' by 1 commit, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n```\n\nThe message tells us our `main` branch is behind by a commit.\n\n## `git merge`\n\nRunning `git merge` will merge the changes we pulled from our origin main branch into our main branch:\n\n```tsx\n$ git merge\n\nUpdating da385e3..f3b3b3e\nFast-forward\n git-pull.txt | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 git-pull.txt\n```\n\nNow the `main` and `remotes/origin/main` branches are in sync again.",
						"_id": "3YUWuQVY9BQaBV1o5nOAb4",
						"_type": "lesson"
					},
					{
						"videoResource": {
							"_type": "videoResource",
							"title": "15-git-switch%2Cgit-branch",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/15-git-switch%2Cgit-branch.mp4",
							"transcript": {
								"text": "[00:00] Use git switch to create and switch branches. Here, I didn't define a branch to switch to, and that's because we have no branches to switch to. I can create and switch to a branch using a single command with the C option and naming my branch. Running git status, we see for the first time that we are on\n\n[00:19] a non-main branch and running git log, we can see where this branch diverges from main. Let's do a couple of things on this branch. Run git push to push it up to the remote repository. We get an error, but it's an extremely helpful error. It actually gives us the command that we can use to set up\n\n[00:38] an origin branch that's connected to this local branch. If we don't want to do that every time, it gives us an option for the configuration we can use to have it set up automatically. There's no time like the present, so let's add that config. Now, if we run git push again, git will automatically set up a tracking branch\n\n[00:58] for us using the same name. My branch is tracked to origin my branch. Over in GitHub, our remote repository, hit refresh and see that in addition to our main branch, we now have a my branch as well. Let's add one final file for git branch.\n\n[01:16] Add it to our index and commit it. Run git status to see that we are ahead of our origin, which we can fix by typing git push. Seeing an automatic refresh on the GitHub side, and run git log to see what's happening locally. We're currently on this commit, where my branch and origin my branch are in sync,\n\n[01:35] but we're ahead of main and origin main. Let's switch back to main and merge everything from my branch back in to the main branch. Run git log again to see that main is now in sync with origin my branch and my branch, but origin main isn't. Easy fix, we run git push to push\n\n[01:54] that last commit to our origin main. Now, all of our branches are in sync again.",
								"srt": "1\n00:00:00,000 --> 00:00:03,560\nUse git switch to create and switch branches.\n\n2\n00:00:03,560 --> 00:00:06,080\nHere, I didn't define a branch to switch to,\n\n3\n00:00:06,080 --> 00:00:08,960\nand that's because we have no branches to switch to.\n\n4\n00:00:08,960 --> 00:00:11,680\nI can create and switch to a branch using\n\n5\n00:00:11,680 --> 00:00:16,080\na single command with the C option and naming my branch.\n\n6\n00:00:16,080 --> 00:00:19,040\nRunning git status, we see for the first time that we are on\n\n7\n00:00:19,040 --> 00:00:22,459\na non-main branch and running git log,\n\n8\n00:00:22,459 --> 00:00:25,680\nwe can see where this branch diverges from main.\n\n9\n00:00:25,680 --> 00:00:27,719\nLet's do a couple of things on this branch.\n\n10\n00:00:27,719 --> 00:00:31,639\nRun git push to push it up to the remote repository.\n\n11\n00:00:31,639 --> 00:00:34,680\nWe get an error, but it's an extremely helpful error.\n\n12\n00:00:34,680 --> 00:00:38,560\nIt actually gives us the command that we can use to set up\n\n13\n00:00:38,560 --> 00:00:42,240\nan origin branch that's connected to this local branch.\n\n14\n00:00:42,240 --> 00:00:43,959\nIf we don't want to do that every time,\n\n15\n00:00:43,959 --> 00:00:45,320\nit gives us an option for\n\n16\n00:00:45,320 --> 00:00:48,520\nthe configuration we can use to have it set up automatically.\n\n17\n00:00:48,520 --> 00:00:49,959\nThere's no time like the present,\n\n18\n00:00:49,959 --> 00:00:52,360\nso let's add that config.\n\n19\n00:00:52,880 --> 00:00:56,160\nNow, if we run git push again,\n\n20\n00:00:56,160 --> 00:00:58,560\ngit will automatically set up a tracking branch\n\n21\n00:00:58,560 --> 00:01:01,200\nfor us using the same name.\n\n22\n00:01:01,200 --> 00:01:04,360\nMy branch is tracked to origin my branch.\n\n23\n00:01:04,360 --> 00:01:06,959\nOver in GitHub, our remote repository,\n\n24\n00:01:06,959 --> 00:01:10,320\nhit refresh and see that in addition to our main branch,\n\n25\n00:01:10,320 --> 00:01:13,199\nwe now have a my branch as well.\n\n26\n00:01:13,199 --> 00:01:16,480\nLet's add one final file for git branch.\n\n27\n00:01:16,480 --> 00:01:18,800\nAdd it to our index and commit it.\n\n28\n00:01:18,800 --> 00:01:22,800\nRun git status to see that we are ahead of our origin,\n\n29\n00:01:22,800 --> 00:01:25,639\nwhich we can fix by typing git push.\n\n30\n00:01:25,639 --> 00:01:28,279\nSeeing an automatic refresh on the GitHub side,\n\n31\n00:01:28,279 --> 00:01:30,599\nand run git log to see what's happening locally.\n\n32\n00:01:30,599 --> 00:01:32,320\nWe're currently on this commit,\n\n33\n00:01:32,320 --> 00:01:35,199\nwhere my branch and origin my branch are in sync,\n\n34\n00:01:35,199 --> 00:01:37,720\nbut we're ahead of main and origin main.\n\n35\n00:01:37,720 --> 00:01:40,120\nLet's switch back to main and merge\n\n36\n00:01:40,120 --> 00:01:44,320\neverything from my branch back in to the main branch.\n\n37\n00:01:44,320 --> 00:01:47,440\nRun git log again to see that main is now\n\n38\n00:01:47,440 --> 00:01:50,760\nin sync with origin my branch and my branch,\n\n39\n00:01:50,760 --> 00:01:52,559\nbut origin main isn't.\n\n40\n00:01:52,559 --> 00:01:54,959\nEasy fix, we run git push to push\n\n41\n00:01:54,959 --> 00:01:58,160\nthat last commit to our origin main.\n\n42\n00:01:58,160 --> 00:02:02,199\nNow, all of our branches are in sync again.\n\n\n"
							},
							"_id": "cCHMkFUt5UU1S5eeML4K5M",
							"duration": 123.094,
							"_rev": "WsAmd18YA1frbQaWBZpGZ8",
							"state": "ready",
							"_createdAt": "2024-03-12T20:04:57Z",
							"muxAsset": {
								"muxPlaybackId": "fiycDPhv3ISsJqWLH02TiHzhZQrh3PKFBhcfApeL00uAo",
								"muxAssetId": "l01eHJRyJDIUKdvLx79fEQnENOw02i5f4aGBHDAEbXeKY",
								"_type": "muxAsset"
							},
							"_updatedAt": "2024-03-19T20:21:17Z"
						},
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:56Z",
						"title": "Branch Management and Syncing with Git and GitHub",
						"description": "Learn how to create, switch, and merge branches in Git, and see how to sync them with your remote repository in GitHub for effective collaboration.",
						"_id": "3YUWuQVY9BQaBV1o5nOBEu",
						"body": "The `git switch` command is used to to create and switch branches.\n\nRunning `git switch` without any arguments will show that we don't have any branches to switch to:\n\n```tsx\n$ git switch\n\nfatal: missing branch or commit argument\n```\n\nWe can create and switch to a branch in a single command with the `-c` option followed by the branch name.\n\n```markdown\ngit switch -c my-branch\n```\n\nAfter switching to the new branch, running `git log` will show us where the branch diverges:\n\n```tsx\n$ git log\n\n617f42f (HEAD -> my-branch, origin/main, main) Add pull note\n...\n```\n\nLet's do a couple of things on this branch.\n\n## Run `git push`\n\nRunning `git push` will push the branch up to the remote repository.\n\nIn this case we get an error that tells us we need to set up an origin branch that's connected to this local branch:\n\n```tsx\n$ git push\n\nfatal: The current branch my-branch has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin my-branch\n\nTo have this happen automatically for branches without a tracking upstream, see 'push.autoSetupRemote' in 'git help config'.\n```\n\nWe can add that configuration with the following command:\n\n```markdown\ngit config push.autoSetupRemote true\n```\n\nNow when we run `git push` again, it will automatically configure the remote tracking branch for us using the same name.\n\nOver in GitHub, our remote repository, hit refresh and see that in addition to our main branch, we now have a `my-branch` as well.\n\n## Add and Commit a New File\n\nLet's add a file on `my-branch`:\n\n```markdown\necho \"#git-branch\" > git-branch.txt\ngit add .\ngit commit -m \"Add branch note\"\n```\n\nRunning `git status` shows we are ahead of our origin, which we can fix by running `git push`.\n\nThe GitHub page will reload to show that changes were pushed to `my-branch`, but let's run `git log` locally to see what's going on:\n\n```markdown\n$ git log\n\n24ed6c9 (HEAD -> my-branch, origin/my-branch) Add branch note\n617f42f (origin/main, main) Add pull note\n```\n\nThe output shows that `my-branch` and `origin/my-branch` are in sync with one another, but both are now ahead of the `main` and `origin/main` branches.\n\nTo switch back to the main branch and merge everything from `my-branch` back into the main branch, use the following commands:\n\n```markdown\ngit switch main\ngit merge my-branch\n```\n\nNow running `git log` will show that the `main` branch is now in sync with `origin/my-branch` and `my-branch`:\n\n```markdown\n$ git log\n\n24ed6c9 (HEAD -> main, origin/my-branch, my-branch) Add branch note\n```",
						"slug": "branch-management-and-syncing-with-git-and-github"
					},
					{
						"slug": "concluding-the-git-fundamentals-tutorial",
						"videoResource": {
							"_type": "videoResource",
							"_id": "cCHMkFUt5UU1S5eeML4KEw",
							"muxAsset": {
								"muxPlaybackId": "CUYqsMmyhuVMyuDIrM3CdmaauWplaLsLTbbsQcHQkIA",
								"muxAssetId": "dw3nOR5P400e01V01y9pN02vfhaNMsf2L84ylmcxzJ01m2jk",
								"_type": "muxAsset"
							},
							"title": "16-outro",
							"transcript": {
								"srt": "1\n00:00:00,000 --> 00:00:02,920\nCongratulations on making it all the way through this Git course.\n\n2\n00:00:03,120 --> 00:00:07,040\nI hope you learned something. And if you did, tag me on X to let me know.\n\n3\n00:00:07,080 --> 00:00:08,520\nI'm chantastic there.\n\n4\n00:00:08,600 --> 00:00:11,240\nGit is just one of the things that you can learn on Epic web.\n\n5\n00:00:11,240 --> 00:00:15,320\nSo be sure to check out some of the other free tutorials and workshops on\n\n6\n00:00:15,560 --> 00:00:20,120\nReact, Tailwind, testing library, you name it. There is a ton.\n\n7\n00:00:20,320 --> 00:00:21,920\nSo go forth and have fun.\n\n\n",
								"text": "[00:00] Congratulations on making it all the way through this Git course. I hope you learned something. And if you did, tag me on X to let me know. I'm chantastic there. Git is just one of the things that you can learn on Epic web. So be sure to check out some of the other free tutorials and workshops on React, Tailwind, testing library, you name it. There is a ton.\n\n[00:20] So go forth and have fun."
							},
							"_updatedAt": "2024-03-19T20:01:42Z",
							"duration": 22.844,
							"_createdAt": "2024-03-12T20:04:58Z",
							"state": "ready",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/git-foundations-tutorial/16-outro.mp4",
							"_rev": "9CeTKuUcQZRsVUft8VlNwg"
						},
						"description": "Take a moment to feel proud of completing this Git course and get ready to explore more tutorials and workshops on Epic web!",
						"body": "Congratulations on making it all the way through this Git tutorial. If you learned something, let me know by tweeting [@chantastic\n](https://twitter.com/chantastic) on X.\n\nGit is just one of the things that you can learn on Epic Web, so be sure to check out some of the other free tutorials and workshops on React, Tailwind, testing library, and more!",
						"solution": null,
						"_id": "3YUWuQVY9BQaBV1o5nOBsk",
						"_type": "lesson",
						"_updatedAt": "2024-03-13T15:44:57Z",
						"title": "Concluding the Git Fundamentals Tutorial"
					}
				],
				"resources": [],
				"_id": "9d9a4161-0df4-40a0-b58f-aa06d3e415e7",
				"_type": "section",
				"_updatedAt": "2024-03-13T16:31:46Z",
				"title": "Commands",
				"description": null
			}
		],
		"_type": "module",
		"image": "https://cdn.sanity.io/images/i1a93n76/production/a785bf6c051932151ecee5b6b8cbe86c36dd94b2-1200x1200.png",
		"slug": {
			"current": "git-fundamentals",
			"_type": "slug"
		},
		"_createdAt": "2024-03-13T15:59:40Z",
		"description": null,
		"moduleType": "tutorial",
		"state": "published",
		"_id": "a9ddd560-0d56-469b-9728-486d4d9b72d2",
		"title": "Git Fundamentals"
	},
	{
		"_createdAt": "2024-02-05T14:36:32Z",
		"moduleType": "tutorial",
		"state": "published",
		"instructor": {
			"_id": "61052001-bcc3-4f25-bec8-4e767889924c",
			"_type": "contributor",
			"_updatedAt": "2024-03-27T05:48:41Z",
			"bio": "A world renowned speaker, teacher, open source contributor, created epicweb.dev, epicreact.dev, testingjavascript.com. instructs on egghead.io, frontend masters, google developer expert.",
			"links": null,
			"slug": "kent-c-dodds",
			"_createdAt": "2024-03-27T05:48:41Z",
			"name": "Kent C. Dodds",
			"picture": {
				"url": "https://cdn.sanity.io/images/i1a93n76/production/ef97de2cb638463af3562c055ff442a917eedeba-800x800.png",
				"alt": "Kent C. Dodds"
			}
		},
		"sections": [
			{
				"title": "Real World Examples",
				"description": null,
				"slug": "real-world-examples",
				"lessons": [
					{
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:23:56Z",
						"title": "Introduction to Developing with AI Assistants",
						"description": "The goal of this tutorial is to develop a mindset that is universally applicable to any AI tool you use.",
						"body": "Artificial intelligence has taken the web development world by storm.\nWhile I don't believe AI will replace our jobs completely, it will help someone to take the job of anyone who doesn't know how to use an AI assistant effectively.\n\nThis is why I've put together this video tutorial series showcasing how I incorporate AI assistants in my day-to-day work as a web developer.\n\n## Understanding the AI Mindset\n\nIt's important to have a good mindset around AI assistants.\n\nYou shouldn't perceive AI assistants as the main coders (yet!). Instead, think of them as useful aids in your programming tasks.\n\nThe realm of AI evolves at a rapid pace; by the time you view these videos, there could be more advanced AI tools.\n\nBut the focus of these videos is to help you develop a mindset that is universally applicable to any AI tool you use.\n\nWhile AI tools continue to improve, it's essential to learn how to make the most of these tools efficiently today.\n\n## The AI Tools I Use\n\nThroughout this series, I will be predominantly using [ChatGPT](https://chat.openai.com) along with [GitHub Copilot](https://github.com/features/copilot). Both are paid platforms and are definitely worth the investment, though they do both have free tiers.\n\nAlternative tools include: [Cody by Sourcegraph](https://sourcegraph.com/cody) and [Tabnine](https://www.tabnine.com/).\n\nThere are plenty more tools out there, and the intention of this series is not to get into specific tools, but rather to shed light on a fundamental approach to utilizing any AI assistant.\n\n## Maximizing Your AI Assistant\n\nThink of AI assistants as force multipliers– they amplify your efforts but don't work without input.\n\nIf you think about it, you're the one who wields the hammer and drives the nail; the hammer doesn't operate on its own.\n\nUnderstanding how to efficiently use these AI tools can greatly enhance your web development skills, and I'm eager to show you how I do it.",
						"solution": null,
						"_id": "a18Kw6QuExWfoACr8K1EqC",
						"slug": "introduction-to-developing-with-ai-assistants",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/00.intro.mp4",
							"castingwords": {
								"transcript": "no"
							},
							"muxAsset": {
								"muxPlaybackId": "rpGUPqaT00O74NsDzif7lgMTD2x2OiXYNj5KodMDBL5k",
								"muxAssetId": "e4AugGMvt8Rp5Pea4Yx9tNszYHNvYg5QyFtiEQRQ5mk",
								"_type": "muxAsset"
							},
							"_type": "videoResource",
							"_rev": "9CeTKuUcQZRsVUft8VlXa2",
							"_updatedAt": "2024-03-19T20:23:57Z",
							"title": "00.intro",
							"duration": 165.8,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_id": "zZ4ErPnSL4ZWs2Du0ONL6z",
							"_createdAt": "2024-02-05T17:56:57Z"
						}
					},
					{
						"title": "Custom Instructions in ChatGPT",
						"slug": "custom-instructions-in-chatgpt",
						"solution": null,
						"_id": "MZsarppRb0Cv7zOKd9Z1fv",
						"_type": "lesson",
						"_updatedAt": "2024-02-05T22:44:19Z",
						"description": "Adding custom instructions to ChatGPT is a great way to personalize its responses.",
						"body": "When working with AI assistants, the more they know about you, the better.\n\nYou have preferences for how you format code, the tools you use, and more. For example, if you're a JavaScript developer, you don't want to ask your AI assistant how to parse JSON and have it give you some Python code. The more contextual information you can provide your AI assistant, the less specific you have to be in your prompt.\n\nThe more contextual information your AI assistant has about you, the more succinct your prompts can be.\n\n## Custom Instructions in ChatGPT\n\nYou can personalize ChatGPT by using the Custom Instructions feature.\n\nThis section allows you to share specifics about your interests, preferred tools, and even details like your prettier config.\n\nThere is also the option for providing guidance on how you want the assistant to respond. I prefer a terse, more casual dialogue, and if the assistant needs to address me, it should use my name:\n\n![Custom Instructions in ChatGPT](https://res.cloudinary.com/epic-web/image/upload/v1707173021/tutorials/ai/10_00-59920-about-yourself-and-the-things-that-youre-interested-in-i-have-like-the-tools-that-im-usi.png)\n\nI continually refine my custom instructions to ChatGPT. Each interaction provides an opportunity to adjust the instructions, helping the assistant avoid undesired actions in the future without me having to explicitly instruct it every time.\n\n## The Value of Customization\n\nSpending time customizing your AI assistant can significantly enhance its usefulness.\n\nProviding context and instructions fine-tuned to your preferences can lead to more improved and personalized responses, so don't hesitate to make it yours!",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:23:56Z",
							"_createdAt": "2024-02-05T17:56:58Z",
							"_rev": "WsAmd18YA1frbQaWBZpeLC",
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "X4d2tadq027paz9oGOOksWCFH37O238Q5IWQdLs58qRA",
								"muxAssetId": "rJ5RqylO4TUmSLzJ014OvCalakl9G8ZnS3EsDHrWunoo"
							},
							"_id": "MZsarppRb0Cv7zOKd9Hykt",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_type": "videoResource",
							"castingwords": {
								"transcript": "no"
							},
							"title": "01.custom-instructions",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/01.custom-instructions.mp4",
							"duration": 122.233333
						}
					},
					{
						"_type": "lesson",
						"body": "As a web developer, the challenge is often not lack of options, but rather an abundance of good ones.\n\nWhether you're facing a dilemma in your technical stack or trying to decide the best approach for a new feature, having a sounding board can be incredibly helpful—even if it's an artificial one.\n\n## Guided by AI\n\nEven if you think you've got a good grasp on the options at hand, discussing them with an AI assistant can provide fresh perspectives and help validate your choices.\n\nYou could simply go with your gut feeling, but engaging in a conversation with an AI assistant like ChatGPT can shed light on perspectives you might have missed.\n\nAsk the AI to argue both sides of the debate at hand. This can help reveal anything you may have missed your consideration, and guide you in making an informed decision.\n\nThis tactic works not only with AI but also in community forums like Twitter, where the collective wisdom of many people can be harnessed for valuable insights.\n\nHowever, having an AI assistant that has been trained on diverse and extensive information can give you an almost immediate insight into your decision making.\n\n## Making the Tough Decisions\n\nSometimes, technical decisions are not as straightforward. For instance, choosing to use a Time-Based One-Time Password (TOTP) algorithm can involve more complex considerations, especially concerning security.\n\nIn scenarios like this, you can take a similar approach by discussing your desired outcomes and potential solutions with your AI assistant. You can also present specific contexts where each alternative could be applicable and ask for the AI's views.\n\nNot only can the AI assistant point out possible issues with your approach but it can also suggest solutions and provide further subjects to consider, deepening the conversation and refining your decision-making process.\n\n## Utilizing AI as a Sparring Partner\n\nThe conversation with an AI assistant may become long, but the insights and decision guidance it provides are worth the investment of time. Having a \"rubber duck that talks back\" to bounce ideas off can be invaluable when wrestling with technical decisions.\n\nThrough a series of back-and-forth conversations, you can refine your understanding and approach to a topic, even before presenting it to a human expert. This way, you can tap into the collective wisdom of the AI's training and use it as a springboard to dive deeper into implementation and problem-solving.\n\nSo the next time you're stuck with a tech decision, remember, your AI assistant is there, ready to help you navigate through your choices and guide you to an informed decision.",
						"solution": null,
						"_id": "a18Kw6QuExWfoACr8K1F1N",
						"_updatedAt": "2024-02-05T21:23:59Z",
						"title": "Making Technical Decisions with AI",
						"description": "Conversing with an AI assistant can help you make informed decisions about your technical stack.",
						"slug": "making-technical-decisions-with-ai",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:23:54Z",
							"_rev": "9CeTKuUcQZRsVUft8VlXTg",
							"muxAsset": {
								"muxPlaybackId": "MjQOxIwzP7E01rr02SF01e8LZTpQUmdOGTeG02REzNYNwEA",
								"muxAssetId": "01015e1PT01Vitt5kSoj2UwlsoWoYtn2DtbIUd6U67oWbs",
								"_type": "muxAsset"
							},
							"duration": 303.9,
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_createdAt": "2024-02-05T17:56:59Z",
							"_type": "videoResource",
							"title": "02.technical-decisions",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/02.technical-decisions.mp4",
							"_id": "zZ4ErPnSL4ZWs2Du0ONLO9"
						}
					},
					{
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:24:00Z",
						"title": "Exploring Unfamiliar Topics with ChatGPT",
						"description": "An AI assistant can help you explore unfamiliar topics and guide you in the right direction.",
						"body": "Having a conversation with ChatGPT is a great way to explore unfamiliar topics. It can help you get started on a new project or provide a starting point for further research.\n\n## A Real-World Example\n\nIn the Epic Workshops, a custom web application is used that embeds videos from Mux.\n\nOne day, a workshop participant asked if it was possible to display the duration of the videos on the user interface, so that they would not have to start the video to see its length. I wasn't quite sure how to go about this, as I was not well-versed with the Mux API.\n\nSo, I turned to ChatGPT with my question, \"I'm using Mux for video hosting. How do I get the length of the Mux video with the Mux playback ID?\"\n\nIn response, ChatGPT advised me that I would need to get an access token and fetch the video details using a specific endpoint. From there, I could extract the video duration from the response. It even provided an example for me.\n\nThat immediate response from ChatGPT provided a path to follow and gave me a sense that what I needed to do was, in fact, possible. It provided details on the API endpoint I needed to hit, which allowed me to start prototyping in my editor.\n\n## Adapting the Solution\n\nThe response contained a minor issue- it included the use of `node-fetch`. I prefer not to use that library since `fetch` is now built into Node, but that was a minor detail and easy to work around. The essential part was understanding how to set up the authorization, which was clearly explained.\n\nHowever, there was another wrinkle for the workshop app.\n\nI couldn't use my token secret since the app runs locally on users' machines, so I would need a proxy in order to use a secret.\n\nI asked ChatGPT again if there was a workaround for using a Mux secret. ChatGPT responded, explaining that avoiding the use of a Mux secret wouldn't be possible due to security reasons.\n\nAlthough it wasn't the answer I hoped for, it was still extremely helpful. It gave me a clearer overview of the constraints I was working under.\n\n## Start with AI\n\nOften, when we face a technical question, our first instinct might be to Google the answer. However, in my experience an AI assistant like ChatGPT could be a quicker and more efficient solution.\n\nThere are times when the AI assistant might not have the answer. But more often than not, it provides useful information to base further exploration on.\n\nChatGPT helps in understanding the problem space and in finding a starting point to begin implementation. However, always remember to verify such information against official documentation as software and APIs can change.",
						"slug": "exploring-unfamiliar-topics-with-chatgpt",
						"_id": "zZ4ErPnSL4ZWs2Du0OkvDD",
						"solution": null,
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "U32DzamP83UKHmh5ykF9FDqxw2pstAkekuS8kxmNfMk",
								"muxAssetId": "4eIvFrOusVH445AL4ieMMporsjUcJ3stNHcbmYiJa00k",
								"_type": "muxAsset"
							},
							"title": "03.exploring-unfamiliar-things",
							"duration": 202.2,
							"_createdAt": "2024-02-05T17:57:00Z",
							"_updatedAt": "2024-03-19T20:23:53Z",
							"_id": "zZ4ErPnSL4ZWs2Du0ONLfJ",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_rev": "9CeTKuUcQZRsVUft8VlXNK",
							"_type": "videoResource",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/03.exploring-unfamiliar-things.mp4"
						}
					},
					{
						"title": "Install GitHub Copilot",
						"body": "The value of having an AI assistant incorporated into your coding editor cannot be overstated.\n\nI've been using GitHub Copilot, but other options like Tabnine and Sourcegraph's Cody are also available.\n\nRegardless of the AI assistant you choose, the core principles remain the same.\n\n## Using GitHub Copilot\n\nIn order to use GitHub Copilot, you'll need to choose a license that suits you and your organization.\n\nAfter you've purchased a license, navigate to your the [Copilot settings in your GitHub account](https://github.com/settings/copilot). Inside the Copilot settings, you’ll be able to customize a few things.\n\n![The GitHub Copilot settings page](https://res.cloudinary.com/epic-web/image/upload/v1707173536/tutorials/ai/16_01-23760-if-youre-using-visual-studio-code--thatll-pull-up-the-prerequisites-and-everything-for.png)\n\n## Installing the Extension\n\nIf your editor is Visual Studio Code (VS Code), you'll see a link to download the Copilot extension from the VS Code Marketplace.\n\n![GitHub Copilot on the VS Code Marketplace](https://res.cloudinary.com/epic-web/image/upload/v1707173652/tutorials/ai/23_01-47360-and-with-that-installed--then-you-should-be-able-to-even-just-open-up-a-new-thing.png)\n\nOnce the extension is installed, you'll need to sign in and configure a few other things.\n\nWhen you're done with this step, pull up a new window, and Copilot should be ready to assist you.\n\nPress `CMD+I` to open a prompt to instruct GitHub Copilot, and type in the type of program you want to create or problem you want to solve.",
						"solution": null,
						"_id": "zZ4ErPnSL4ZWs2Du0OkvQ5",
						"_type": "lesson",
						"_updatedAt": "2024-02-05T22:58:30Z",
						"description": "GitHub Copilot",
						"slug": "install-github-copilot",
						"videoResource": {
							"_createdAt": "2024-02-05T17:57:01Z",
							"_id": "zZ4ErPnSL4ZWs2Du0ONLsB",
							"title": "04.install-copilot",
							"_updatedAt": "2024-03-19T20:23:51Z",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_rev": "6LaeqP6n94P8FD3sVhYEHJ",
							"castingwords": {
								"transcript": "no"
							},
							"muxAsset": {
								"muxPlaybackId": "7qVhfcj801DG4AA5NLoYIOPkhMQloUjPEPwUYX9LBrbo",
								"muxAssetId": "5bKnMa4ZkgefEqXLRAnJwxkGBUNkrvAFMs6J00kf78Do",
								"_type": "muxAsset"
							},
							"duration": 146.666667,
							"_type": "videoResource",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/04.install-copilot.mp4"
						}
					},
					{
						"_id": "a18Kw6QuExWfoACr8K1FCY",
						"title": "Writing Tests with GitHub Copilot",
						"description": "Using AI assistants like GitHub Copilot can be a game-changer when it comes to writing tests for your code.",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/05.writing-tests.mp4",
							"duration": 484.933333,
							"_id": "zZ4ErPnSL4ZWs2Du0ONM9L",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_rev": "6LaeqP6n94P8FD3sVhYECi",
							"_createdAt": "2024-02-05T17:57:02Z",
							"title": "05.writing-tests",
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "bbumMFigGGtFM2VRRn0202gaUqRE01A7ZmJUY02G31WiNHU",
								"muxAssetId": "MGAm0000MH00M2sIBapgLDQFEholNhXNHewmLUUVSUalqk",
								"_type": "muxAsset"
							},
							"_updatedAt": "2024-03-19T20:23:50Z"
						},
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-02-05T23:01:29Z",
						"body": "Using AI assistants like GitHub Copilot can be a game-changer when it comes to writing tests for your code.\n\nFor this example, we will use GitHub Copilot to help us write tests for the `epicweb/invariant` utility while utilizing Node's built-in test runner.\n\nHere's a step-by-step guide on how to use your AI assistant effectively:\n\n### 1. Provide Context\n\nWhen you go to write tests, keep the relevant files open.\nAI assistants like GitHub Copilot can use your project's files as part of its context for code generation. Different AI assistants may have their own ways of including files or even a whole project in context, so it's important to determine the mechanism of your specific AI assistant.\n\n### 2. Write Conversational Prompts\n\nWhen asking your AI assistant to generate code or tests, start by using more vague, conversational prompts.\n\nFor example:\n\n```markdown\nPlease write a test for the invariant utility.\n```\n\nOnce you see the response, you can edit the prompt to be more specific depending on the output.\n\nIn the case of the `invariant` utility, the generated output used keywords like `describe` and `expect` which are typical for testing utilities but not for Node's built-in test runner.\n\n### 3. Make Your Prompt More Specific\n\nIf the generated output isn't what you're looking for, make your prompts more specific. This helps the AI assistant understand what you need, leading to improved output. For example:\n\n```markdown\nPlease write a test for invariant using the built-in testing utilities from Node (like node:assert and node:test).\n```\n\nReview the generated code, and edit your prompt accordingly until you get the desired output.\n\nOnce your AI assistant has generated a test that passes, break the implementation and check whether the test still passes or fails. This step is essential to ensure that the generated tests are functional and validates that the AI-generated code is working as intended.\n\n### 4. Prompt GitHub Copilot with a Comment\n\nAnother option for interacting with GitHub Copilot is to write a comment with an instruction.\n\nFor example:\n\n```markdown\n// verify the message argument can be a function\n```\n\nCopilot will begin to suggest lines of code for autocompletion based on the comment. You can hit `tab` to accept a full line, or `CMD+right arrow` to accept a single word.\n\n### 5. Ask Copilot to Edit Code\n\nAfter highlighting some code, you can ask Copilot to edit it.\n\nFor example, asking Copilot to verify the error message that is thrown.\n\nCopilot will display a diff to show you how edits would change the code. You can then accept or reject the changes.\n\n![Copilot diff for selected code](https://res.cloudinary.com/epic-web/image/upload/v1707173265/tutorials/ai/71_06-03100-were-going-to-use-the-callback-form-here--and-we-can-say--return-error-message-equals.png)\n\n## Treat Your AI Assistant as a Pairing Partner\n\nNo matter which technique you use for interacting with an AI assistant, treat it as if you were pairing with a coworker. Make the interactions conversational and collaborative. This opens up the possibility of creating code and tests at a faster pace with improved accuracy.\n",
						"slug": "writing-tests-with-github-copilot"
					},
					{
						"description": "Generate realistic test data effortlessly for efficient database seeding with the help of your AI assistant.",
						"body": "Imagine you need to seed a database with test data that is realistic.\n\nInstead of coming up with each unique item manually, or using 'lorem ipsum' everywhere, your AI assistant can help you generate realistic data.\n\nBy writing a comment inside of your database seeding script, GitHub Copilot will be aware of what you want it to do:\n\n```\n// create a bunch of notes with  hardcoded ids that do not use lorem ipsum\n```\n\nThe AI assistant will follow the pattern already present in your code to create more notes, generating unique and realistic content each time for the various fields:\n\n```javascript\ncreate: [\n\t{\n\t\tid: \"abc123\",\n\t\ttitle: \"Fun facts about koalas\",\n\t\tcontent:\n\t\t\t\"Koalas are marsupials native to Australia. They are known for their adorable appearance and their diet of eucalyptus leaves...\",\n\t},\n];\n```\n\nSometimes, the AI doesn't automatically generate the data as expected.\n\nWhen that happens, you can use the AI's chat feature to request it to generate more of these hard-coded notes:\n\n```\ncreate another three of these fake hard-coded notes\n```\n\nThe AI might not get the formatting perfect every time, but the clean-up required is worth the time saved from coming up with your own test data.\n\nIf you need to generate a lot of data, you could directly open up the AI's chat and ask it to create examples that meet your requirements:\n\n```javascript\ncreate a bunch of examples of notes that have a hard-coded id, title, and content, based on various subjects and serialized as JSON\n```\n\nOnce the data has been generated, you can then paste it directly into your seed script.\n\nThe combination of inline comments and off-to-the-side chat with your AI assistant keeps you going smoothly and conversationally, saving you a ton of time. This is a fantastic way of generating realistic data with the help of your AI assistant.",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/06.creating-data.mp4",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_createdAt": "2024-02-05T17:57:03Z",
							"_type": "videoResource",
							"castingwords": {
								"transcript": "no"
							},
							"_updatedAt": "2024-03-19T20:23:48Z",
							"title": "06.creating-data",
							"duration": 364.633333,
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "BQTCi1W7a13AOVxyZWphVu3qEPuJBgyW00DjwQ9UKl1k",
								"muxAssetId": "fqv5D7hxeOWjz01VRBgi9B9weE8jM901nneHDvJLOGcHo"
							},
							"_rev": "6LaeqP6n94P8FD3sVhYEAs",
							"_id": "zZ4ErPnSL4ZWs2Du0ONMUn"
						},
						"_id": "MZsarppRb0Cv7zOKd9Z1pb",
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:24:05Z",
						"title": "Realistic Database Seeding with AI",
						"slug": "realistic-database-seeding-with-ai",
						"solution": null
					},
					{
						"videoResource": {
							"_rev": "9CeTKuUcQZRsVUft8VlXGy",
							"title": "07.from-slack",
							"_updatedAt": "2024-03-19T20:23:46Z",
							"duration": 304.466667,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/07.from-slack.mp4",
							"castingwords": {
								"transcript": "no"
							},
							"muxAsset": {
								"_type": "muxAsset",
								"muxPlaybackId": "k102JViV22R6eTUuqZ1mvFI02j9lwCBGB027HIKxQn2jC00",
								"muxAssetId": "kXjjL7xKcYnFJDJgqya8FnlpLu8ngLjApJKVqe9vvjw"
							},
							"_id": "zZ4ErPnSL4ZWs2Du0ONMhf",
							"_type": "videoResource",
							"_createdAt": "2024-02-05T17:57:04Z"
						},
						"solution": null,
						"_type": "lesson",
						"body": "AI assistants are also great for letting you chat with an API based on a description of it.\n\nIn chat, provide the following prompt:\n\n```\nWrite some code to make a fetch request to the API described here:\n<paste description>\n```\n\nWhen you submit the prompt, the AI assistant will generate an example that includes a fetch API function with the appropriate request information.\n\n## Addressing Generated Code Issues\n\nOf course, the fetch API function may not be entirely accurate, particularly if you're running server-side code.\n\nYou may have to manually replace the domain or device-specific tokens which you'll have to source and input yourself.\n\nCopilot also has a \"Vulnerability\" feature that will alert you to issues like hardcoded credentials. You can choose to ignore these warnings if they're not relevant to your use case.\n\nTo save on back-and-forths, include all your changes in a single message. You can even change the task at hand a bit to include parsing responses with a Zod schema.\n\nThere are all kinds of development tasks that AI assistants can help with.\n\nIf you'd like to change from a require to an import statement. Just ask your AI assistant to switch from CommonJS to ESM import. Or perhaps you'd like to handle errors more gracefully by returning an error message. Again, just ask.\n\nIf you don't get the results you were hoping for, just make some adjustments and allow it to make those until you get what you want.\n\nThe code updating process is iterative. Keep tweaking with your AI assistant until you're satisfied with the outcome.",
						"slug": "simplify-api-interaction-with-ai-assistants",
						"description": "Prompt AI for fetch request code, address issues, and optimize development tasks seamlessly.",
						"_id": "MZsarppRb0Cv7zOKd9Z25j",
						"_updatedAt": "2024-02-05T21:24:06Z",
						"title": "Simplify API Interaction with AI Assistants"
					},
					{
						"_id": "MZsarppRb0Cv7zOKd9Z2FP",
						"_type": "lesson",
						"videoResource": {
							"_id": "MZsarppRb0Cv7zOKd9Hz7T",
							"duration": 69.8,
							"castingwords": {
								"transcript": "no"
							},
							"_createdAt": "2024-02-05T17:57:06Z",
							"_updatedAt": "2024-03-19T20:23:43Z",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_type": "videoResource",
							"_rev": "6LaeqP6n94P8FD3sVhYE1g",
							"muxAsset": {
								"muxPlaybackId": "d025B87YAaKe400WaKBl6Xpxk5c8oNUODYcCxw67C01lNY",
								"muxAssetId": "jkE7mXznDD5x5P026P4YLELbywiv6myUj4KFfHHf7kf8",
								"_type": "muxAsset"
							},
							"title": "08.explaining-code",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/08.explaining-code.mp4"
						},
						"_updatedAt": "2024-02-05T21:24:07Z",
						"title": "Understanding Code with AI",
						"description": "Gain insights, explanations, and bug identification with AI assistants, streamlining your understanding in your codebase.",
						"body": "AI assistants are great for helping you understand code that you may not understand in your codebase. They can provide valuable insights and explanations, helping to clear up confusion or misunderstandings.\n\nFor example, a learner in a recent Epic Web Workshop had some questions about a particular piece of code.\n\nBy opening the AI chat and pasting the learner's question into it along with the code, the AI assistant generated a detailed explanation.\n\nIn addition to helping you become familiar with new code, chat can be helpful in identifying bugs.\n\nThese days I pretty much always just go straight to AI assistants when I'm trying to understand something before I go to regular humans these days. Sorry, Stack Overflow!",
						"slug": "understanding-code-with-ai",
						"solution": null
					},
					{
						"solution": null,
						"_id": "zZ4ErPnSL4ZWs2Du0Okvpp",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "O102BFovWrXFXjHbt6xQGT9oOVZ0000VHq1902gh3IDxzQY",
								"muxAssetId": "WYX7H3519UntusngRpek5Qdfsp86iKqZkZ7SNa007SlE",
								"_type": "muxAsset"
							},
							"title": "09.unfamiliar-apis",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/09.unfamiliar-apis.mp4",
							"duration": 176.433333,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"castingwords": {
								"transcript": "no"
							},
							"_type": "videoResource",
							"_rev": "6LaeqP6n94P8FD3sVhYE92",
							"_id": "zZ4ErPnSL4ZWs2Du0ONN37",
							"_updatedAt": "2024-03-19T20:23:45Z",
							"_createdAt": "2024-02-05T17:57:06Z"
						},
						"title": "Navigating APIs Documentation with AI",
						"description": "Quickly grasp and navigate APIs with your AI assistant. Learn to filter errors, such as Chrome extensions in Sentry, with ease. Verify solutions for accuracy.",
						"body": "AI assistants are pretty good at is helping you understand APIs that you're not familiar with.\n\nFor example, when using Sentry I was seeing errors from Chrome extensions that I'm not in control over. I wanted to filter them out, but I didn't know how to do it with the Sentry API.\n\nInstead of digging through the Sentry docs, I could just use my AI assistant.\n\nIn this case, all I had to do was open a chat and ask how to filter out the errors from Chrome and Firefox extensions in Sentry. The AI used its understanding of the Sentry API to suggest a solution.\n\nAs general advice, when the AI assistant provides you with code, you should do some quick research to make sure that the solution is accurate. This is especially important when working with unfamiliar APIs that may have changed since the AI model was trained.",
						"slug": "navigating-apis-documentation-with-ai",
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:24:08Z"
					},
					{
						"solution": null,
						"_type": "lesson",
						"title": "AI-Driven Debugging - Deciphering Errors",
						"description": "Paste error messages into AI chat for quick potential causes and troubleshooting steps, expediting the debugging process",
						"body": "When it comes to debugging, AI assistants are invaluable for deciphering error messages in the development process and even in production logs.\n\nWhen you see an error message in the log, copy and paste it into the AI chat.\n\nThe assistant will reply with several possible causes for the error.\n\nFor example, when asked about an SQLite error about a missing database, the AI assistant provided a list of possible reasons along with troubleshooting steps to track down the root cause.\n\nWhile AI may not fully understand the context or each underlying detail, its ability to compile a list of possible solutions can save time and resources in the debugging process.",
						"slug": "ai-driven-debugging-deciphering-errors",
						"videoResource": {
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_id": "zZ4ErPnSL4ZWs2Du0ONNOZ",
							"duration": 110.633333,
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "4yF601L02kmp2bH8IMnCQii7kvVSz42Enr6WX01Y4bwWNI",
								"muxAssetId": "JN7GwDGbWd5ONOvgruv5vNkFQ01TUhbz202GeK1vQa2qA",
								"_type": "muxAsset"
							},
							"_createdAt": "2024-02-05T17:57:07Z",
							"_rev": "WsAmd18YA1frbQaWBZpdVi",
							"title": "10.understanding-errors",
							"_updatedAt": "2024-03-19T20:23:41Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/10.understanding-errors.mp4"
						},
						"_id": "MZsarppRb0Cv7zOKd9Z2bz",
						"_updatedAt": "2024-02-05T21:24:09Z"
					},
					{
						"_type": "lesson",
						"description": "Describe in English, let AI generate SQL. Simplify database interactions.",
						"body": "Working with raw SQL queries can be a pain.\n\nLuckily, AI assistants allow you to write a comment about what you want the query to do in plain english, and then they will generate the SQL for you.\n\nFor example, I can specify that I'm working with an SQLite database and say that I want to query for users and their images:\n\n```\n// Query the SQLite database table for users and their images\n```\n\nLike before, if the response generated by the AI assistant isn't what you are looking for, you don't have to give up. Instead, try to provide additional context from the file you are in, or iterate on your prompt.\n\nWhether it's SQL or a different syntax you're not familiar with, AI assistants can help you read and write pretty much any code.",
						"solution": null,
						"_id": "zZ4ErPnSL4ZWs2Du0Okw2h",
						"_updatedAt": "2024-02-05T21:24:10Z",
						"title": "AI-Simplified SQL Queries",
						"slug": "ai-simplified-sql-queries",
						"videoResource": {
							"muxAsset": {
								"muxAssetId": "U9mx1h817Q8BikrTMHQCkptTn00L7HHXWC4bf401uH02SA",
								"_type": "muxAsset",
								"muxPlaybackId": "1YrZgVCB01nDcu1JC01100V8mcd5F8PpkPpFQXPBAMIXbo"
							},
							"_id": "zZ4ErPnSL4ZWs2Du0ONNbR",
							"_updatedAt": "2024-03-19T20:23:39Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/11.unfamiliar-syntax.mp4",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_rev": "9CeTKuUcQZRsVUft8VlX15",
							"duration": 270.133333,
							"_createdAt": "2024-02-05T17:57:09Z",
							"_type": "videoResource",
							"title": "11.unfamiliar-syntax"
						}
					},
					{
						"_id": "MZsarppRb0Cv7zOKd9Z2lf",
						"slug": "simplifying-git-commit-messages",
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:24:11Z",
						"title": "Simplifying Git Commit Messages",
						"description": "Automate git commits with AI. Paste your diff, streamline version control.",
						"body": "Writing good git commit messages used to be a time-consuming task, but not any more!\n\nAI assistants are able to generate commit messages for you based on the actual git diffs.\n\nGitHub Copilot has a built-in button that will do this for you, but you can also use other AI assistants to generate commit messages.\n\nJust copy and paste the diff and paste it into ChatGPT or another AI assistant, and ask it to generate a commit message for you.\n\nAs as bonus tip, on a Mac you can use this terminal command to copy the diff directly to your clipboard:\n\n```\ngit diff | pbcopy\n```\n\nThe generated commit message will be based on the changes in the diff, and you can then use it as-is or modify it if needed.",
						"videoResource": {
							"title": "12.commit-messages",
							"duration": 98.966667,
							"castingwords": {
								"transcript": "no"
							},
							"_rev": "6LaeqP6n94P8FD3sVhYDx5",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/12.commit-messages.mp4",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"muxAsset": {
								"muxPlaybackId": "XxX6QwoaRcER018W1TpfFBkEPXUEBQLqE2Hwlsk2xGN8",
								"muxAssetId": "00gnIoULMcMnKHeshiiRzEoDIfkuT5wjWB8oP01hDGLlE",
								"_type": "muxAsset"
							},
							"_id": "zZ4ErPnSL4ZWs2Du0ONNoJ",
							"_updatedAt": "2024-03-19T20:23:38Z",
							"_createdAt": "2024-02-05T17:57:10Z",
							"_type": "videoResource"
						},
						"solution": null
					},
					{
						"_id": "a18Kw6QuExWfoACr8K1Fcd",
						"slug": "ai-assistants-for-social-media-dynamics",
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-02-05T21:24:12Z",
						"title": "AI Assistants for Social Media Dynamics",
						"description": "Explore how AI aids in decoding slang, understanding emojis, and strategizing responses, making social media interactions more informed and strategic.",
						"body": "Another useful task for AI assistants is helping with social media.\n\nTo be clear, I don't have it write my social media posts!\n\nInstead, I use it for understanding what people mean when they use slang or emoji combinations I'm not familiar with.\n\nDepending on how I might feel about a post, I might also use an AI assistant to help me think about how I might respond.\n\nAI assistants are great for talking through tactics (as well as talking you out of raging against someone's bad takes!).",
						"videoResource": {
							"_createdAt": "2024-02-05T17:57:11Z",
							"_rev": "9CeTKuUcQZRsVUft8VlWlC",
							"duration": 102.333333,
							"castingwords": {
								"transcript": "no"
							},
							"_updatedAt": "2024-03-19T20:23:37Z",
							"muxAsset": {
								"muxAssetId": "aOy7FJW02XM2wg5lrQ8XiMEciTxWMVK6Y8Snjs5fGzz4",
								"_type": "muxAsset",
								"muxPlaybackId": "ti6HDr7lr3IASyuZjyAG1eonvlqxNS9Go1u02umv1fnY"
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/13.social-media.mp4",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"title": "13.social-media",
							"_id": "zZ4ErPnSL4ZWs2Du0ONO1B",
							"_type": "videoResource"
						}
					},
					{
						"_id": "a18Kw6QuExWfoACr8K1Fno",
						"_updatedAt": "2024-02-05T21:24:14Z",
						"title": "Concluding AI Assistance for Web Developers",
						"description": "Gain insights into the role of AI assistants in web development. Exciting possibilities await as these tools evolve for more efficient and creative projects.",
						"body": "I hope these lessons were helpful to you as you navigate the world of AI assistance that we are now living in.\n\nAssistants are an important tool in your toolbox as a web developer.\n\nI'm excited about the future, because these tools will continue to get better and better, and we'll be able to use them more effectively and efficiently to build more amazing things!",
						"_type": "lesson",
						"slug": "concluding-ai-assistance-for-web-developers",
						"videoResource": {
							"_type": "videoResource",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"muxAsset": {
								"muxPlaybackId": "NHEiDpn7Y024oKZEihN3dSwBqqZpT100kReDjLSxlCfaw",
								"muxAssetId": "MCqxXhO001Lh6QZKF9M00EhmKeammauiJyUQE4hA5pBbI",
								"_type": "muxAsset"
							},
							"_updatedAt": "2024-03-19T20:23:35Z",
							"duration": 24.433333,
							"castingwords": {
								"transcript": "no"
							},
							"_createdAt": "2024-02-05T17:57:12Z",
							"_id": "zZ4ErPnSL4ZWs2Du0ONOE3",
							"title": "14.outro",
							"_rev": "WsAmd18YA1frbQaWBZpd1u",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/ai-tutorial/14.outro.mp4"
						},
						"solution": null
					}
				],
				"resources": [],
				"_id": "87a47453-23b8-4f27-a83d-32354821d840",
				"_type": "section",
				"_updatedAt": "2024-02-05T21:46:45Z"
			}
		],
		"_type": "module",
		"slug": {
			"current": "ai-assistants",
			"_type": "slug"
		},
		"_updatedAt": "2024-03-27T14:14:10Z",
		"description": null,
		"body": "Artificial intelligence has taken the web development world by storm, and learning how to use it will be a game-changer in the job market. It's not about replacement; it's about ensuring you're the one with the upper hand by wielding AI assistants effectively.\n\nThat's why Kent has developed a tutorial to discover how AI assistants like ChatGPT and GitHub Copilot can improve your workflow as a developer. This tutorial is designed to equip you with the knowledge and skills needed to leverage AI effectively in your projects.\n\n## What to expect:\n\n**AI Mindset:** Adopt the right mindset for seamlessly integrating AI tools into your workflow.\n\n**Optimizing AI Assistants:** Unlock the full potential of ChatGPT and GitHub Copilot with expert strategies and practical advice.\n\n**Informed Decision-Making:** Utilize AI to make informed decisions, guiding you through complex technical challenges effortlessly.\n\n**AI for Streamlined Development:** Leverage AI for efficient code generation, testing automation, and debugging support.\n\n**Real-World Use Cases:** Explore real-world examples highlighting AI's role in problem-solving, debugging, and social media interactions.\n",
		"_id": "a2aed1e4-c01f-43a4-8ed3-b345159f5d75",
		"title": "AI Assistants",
		"image": "https://cdn.sanity.io/images/i1a93n76/production/2f5e5280dc3abf4b0dacf93fb6e388c35937b7ce-1000x1000.png"
	},
	{
		"instructor": {
			"_id": "976c052d-49de-4764-9177-0aa5ac98760b",
			"_type": "contributor",
			"_updatedAt": "2024-05-30T01:04:22Z",
			"bio": "I'm a hybrid designer, developer, video editor and content creator with an optimistic and enthusiastic approach to life!",
			"links": null,
			"slug": "simon-vrachliotis",
			"_createdAt": "2024-03-27T06:54:49Z",
			"name": "Simon Vrachliotis",
			"picture": {
				"url": "https://cdn.sanity.io/images/i1a93n76/production/5848a209c5890b594ed88daa53e97b6239e5c480-750x735.png",
				"alt": "Simon Vrachliotis profile picture"
			}
		},
		"_id": "b2cf48a8-c551-4d25-af2c-08916444dd2c",
		"title": "Fluid Hover Cards with Tailwind CSS",
		"slug": {
			"_type": "slug",
			"current": "fluid-hover-cards-with-tailwind-css"
		},
		"_updatedAt": "2024-03-27T14:14:06Z",
		"description": null,
		"moduleType": "tutorial",
		"state": "published",
		"_type": "module",
		"image": "https://cdn.sanity.io/images/i1a93n76/production/37841cd60c908c9d4f67885590b4fd2c71939d09-1000x1000.png",
		"_createdAt": "2024-01-23T07:23:03Z",
		"body": "Tailwind CSS, Flexbox, and CSS Grid have made it easier than ever for you to implement responsive designs.\n\nBut what about when it comes to creating components that respond to user interactions?\n\nThere are so many things to consider: What should happen when a user hovers over an element? How should other components react? How can you make sure that your animations are smooth and fluid? How can you ensure text remains readable when it's overlaid on top of an image?\n\nAnimation and interactivity are key to creating engaging user experiences, but they can be intimidating.\n\nFortunately, the same tools that make it easy to create responsive designs also make it easy to create engaging interactions.\n\nInspired by a component found on when browsing [dji.com](http://dji.com/), Simon Vrachliotis has developed this tutorial to show you how to build a set of professional-grade cards for displaying content.\n\nThere are three main sections to this tutorial:\n\n**Setting the Stage:** Create a plan by exploring the deployed card component's behavior. Dive into the starting code and prepare for the transformation.\n\n**Cart Layout:** Use Flexbox and CSS Grid properties to create a responsive layout for the cards. Work with image and text placement within each card.\n\n**Interaction Polish:** Add a subtle background overlay for better text readability. Create smooth hover animations for card expansion and text reveal effects.\n\nThere's more to this tutorial than just the code.\n\nAlong the way, Simon shares his thought process and design decisions behind each Tailwind class that gets added. You'll learn how to think like a UI designer and develop the intuition for making things look good.\n\n**Key Takeaways:**\n\n- Using Flexbox and CSS Grid for responsive layouts.\n- Understanding the role of the fractional `fr` unit.\n- Properly ordering elements to achieve a desired look.\n- Configuring Tailwind CSS for animation and transition effects.\n- Achieving full-coverage background images with proper aspect ratios.\n- Ensuring text readability with background overlays.\n\n### Ready to Elevate Your UI Skills?\n\nWhether you're a beginner or an experienced developer, this tutorial has something for you.\n\nIt's time to elevate your Tailwind CSS skills and impress your users with engaging hover interactions.\n\nLet's do this!",
		"sections": [
			{
				"resources": [],
				"_id": "d15e1fbb-d83f-4f3a-98b1-6c44debb8a56",
				"_type": "section",
				"_updatedAt": "2024-01-23T23:08:26Z",
				"title": "Implementation",
				"description": null,
				"slug": "implementation",
				"lessons": [
					{
						"body": "This UI component on dji.com has four cards:\n\n![Four cards on the DJI website](https://res.cloudinary.com/epic-web/image/upload/v1705955792/4f72882684267ba46a4c51caa7680e59.png)\n\nWhen hovering over each card, it will slightly expand in width and eat into the space of the adjacent cards. Also, the text on each card smoothly glides up and down to uncover an additional paragraph.\n\n## Recreating the Cards UI\n\nHere is our starting point for recreating the cards UI:\n\n```tsx\n<div class=\"grid min-h-screen place-items-center\">\n\t<p class=\"text-4xl font-bold\">Let's do this!</p>\n</div>\n```\n\nWe have a simple functional component named `HoverStretchCards` that returns a grid with centered items. The grid is set to take up the full screen height at a minimum.\n\nWe have a full-height grid container using `place-items-center` to center its children, which for now is a bold text paragraph that says \"Let's do this!\".\n\nOur goal is to transform this starting point into a responsive collection of cards that emulates the behavior of the DJI website cards. Let's get to it!",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:24:20Z",
							"castingwords": {
								"transcript": "no"
							},
							"_type": "videoResource",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/01-Introduction.mp4",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_createdAt": "2024-01-23T17:33:22Z",
							"_rev": "9CeTKuUcQZRsVUft8VlYoI",
							"_id": "ZB9FUjBzEPYolLuh0oPdyN",
							"title": "01-Introduction",
							"duration": 47.044,
							"muxAsset": {
								"muxPlaybackId": "77JoO3EkFhu01s00lhZO01JT8Pt02Fis7VVSqPyZdf4b2mw",
								"muxAssetId": "egAjg02yZ02GCEHxbQIJ5bapDIbkygcFkoafRLh9Fbz900",
								"_type": "muxAsset"
							}
						},
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-01-23T17:59:27Z",
						"title": "Fluid Hover Cards with Tailwind CSS",
						"_id": "L2nFczBOHr0LI7xrpQqeRR",
						"description": "Recreate an interactive cards UI with Tailwind CSS.",
						"slug": "fluid-hover-cards-with-tailwind-css"
					},
					{
						"_id": "ZB9FUjBzEPYolLuh0oRFIG",
						"_updatedAt": "2024-01-23T19:06:18Z",
						"title": "Overview of the Cards Grid",
						"description": "Understanding how to employ Flexbox for crafting fluid, horizontally adaptable UIs with reusable elements.",
						"body": "In order to have our cards be able to fluidly share horizontal space, we will use Flexbox.\n\nFirst, we'll remove the `<p>` tag from our starting point html, and replace it with a an unordered list. The classnames for the `<ul>` will be `flex` and `gap-4`:\n\n```tsx\n<div class=\"grid min-h-screen place-items-center\">\n\t<ul class=\"flex gap-4\"></ul>\n</div>\n```\n\nInside the `<ul>`, we'll be inserting four `<li>` list items. For now, each will have a custom height of 500px, be full width, and have a background color of `bg-rose-300`:\n\n```tsx\n<div class=\"grid min-h-screen place-items-center\">\n\t<ul class=\"flex gap-4\">\n\t\t<li class=\"h-[500px] w-full bg-rose-300\"></li>\n\t\t<li class=\"h-[500px] w-full bg-rose-300\"></li>\n\t\t<li class=\"h-[500px] w-full bg-rose-300\"></li>\n\t\t<li class=\"h-[500px] w-full bg-rose-300\"></li>\n\t</ul>\n</div>\n```\n\nWhen we look at our current markup in the browser, we won't be able to see anything. The cards are compressed, with the gap being the only space being utilized.\n\n![DevTools shows the list can't be seen](https://res.cloudinary.com/epic-web/image/upload/v1705955793/f6282bcaf76140c686b8320eb853b6d3.png)\n\nIn order to be able to see the list items, we'll add the full width class `w-full` to the `<ul>`:\n\n```tsx\n<ul class=\"flex gap-4 w-full\">\n```\n\nNow we can see the list items, but they're stretching across the entire width of the screen:\n\n![Stretching across the entire width of the screen](https://res.cloudinary.com/epic-web/image/upload/v1705956132/cb2ad28633a523a324a0a80464a10ceb.png)\n\nBy setting a maximum width of `6xl`, we can limit the width of the list items while still keeping them centered:\n\n```tsx\n<ul class=\"flex w-full max-w-6xl gap-4\">\n```\n\n![Setting a maximum width](https://res.cloudinary.com/epic-web/image/upload/v1705956133/014d8506f568e52bddb3ec5b444a5ccb.png)\n\nThe boxes look good, but it's going to be inconvenient to make changes across all four cards. Even with using VS Code's multiple cursor feature, we still would have to add `rounded-2xl` four times in order to round the corners of the cards.\n\nIn order to avoid repeating ourselves, let's use a loop to create the list items.",
						"slug": "overview-of-the-cards-grid",
						"_type": "lesson",
						"videoResource": {
							"title": "02-Cards-grid",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_type": "videoResource",
							"_rev": "WsAmd18YA1frbQaWBZpep0",
							"muxAsset": {
								"muxPlaybackId": "EWivFki4uBvkTsopDzwS01VPb1et9bBdHFubQ02hFWb00k",
								"muxAssetId": "xCXWlk8j01AX01pLjR7JqILY4844YSPiDlp4ib7ojU4Vo",
								"_type": "muxAsset"
							},
							"_id": "ZB9FUjBzEPYolLuh0oPe6w",
							"duration": 69.844,
							"_createdAt": "2024-01-23T17:33:24Z",
							"_updatedAt": "2024-03-19T20:24:18Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/02-Cards-grid.mp4"
						},
						"solution": null
					},
					{
						"_updatedAt": "2024-01-23T19:06:52Z",
						"title": "Use a Loop to Quickly Create a List",
						"slug": "use-a-loop-to-quickly-create-a-list",
						"videoResource": {
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_createdAt": "2024-01-23T17:33:24Z",
							"title": "03-Placeholder-loop",
							"duration": 33.044,
							"_rev": "6LaeqP6n94P8FD3sVhYEkj",
							"muxAsset": {
								"muxPlaybackId": "g01odCVkuWiJuxg6m00Frgq8Jb4zJKC9KevGPCDNkYpq8",
								"muxAssetId": "nsCCe6m7crQN3FRrmf8h9sso65gk19NA6ZZUJfI6jfo",
								"_type": "muxAsset"
							},
							"_type": "videoResource",
							"_id": "2Xou4j4vYd192Jra6hoEYz",
							"_updatedAt": "2024-03-19T20:24:17Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/03-Placeholder-loop.mp4"
						},
						"_id": "L2nFczBOHr0LI7xrpQqesR",
						"_type": "lesson",
						"description": "JavaScript's Array methods provide a quick way to create a list of items.",
						"body": "The spread operator and the `Array.keys()` method can be utilized to create a list of items.\n\nIn order to create our list of four cards inside of the `<ul>`, we'll start by using the spread operator with `Array.keys()`. This will give us an array of 0, 1, 2, and 3:\n\n```tsx\n<div class=\"grid min-h-screen place-items-center\">\n\t<ul class=\"flex w-full max-w-6xl gap-4\">\n\t\t{\n\t\t\t[...Array(4).keys()] // [0, 1, 2, 3]\n\t\t}\n\t</ul>\n</div>\n```\n\nThen we'll use the `map()` method to create an array of our `li` elements. The `key` prop in each `li` is assigned the corresponding index from our array:\n\n```tsx\n<div class=\"grid min-h-screen place-items-center\">\n\t<ul class=\"flex w-full max-w-6xl gap-4\">\n\t\t{[...Array(4).keys()].map((item) => (\n\t\t\t<li key={item} class=\"h-[500px] w-full rounded-2xl bg-rose-300\"></li>\n\t\t))}\n\t</ul>\n</div>\n```\n\nNow we will still end up with four cards and only have to change the markup in one place.\n\n![](https://res.cloudinary.com/epic-web/image/upload/v1705956284/ea4cf0e62aae6cc73293b8887071dbba.png)",
						"solution": null
					},
					{
						"_id": "ZB9FUjBzEPYolLuh0oRFWX",
						"_type": "lesson",
						"_updatedAt": "2024-01-23T17:59:30Z",
						"title": "DynamicallyAdd Images to the Cards",
						"description": "A step-by-step guide on how to fetch and display images dynamically in a gallery.",
						"body": "Using IDs from Unsplash images, we are going to add real images to our component.\n\nLet's start by creating an array containing the image IDs:\n\n```tsx\nconst imageIds = [\n\t\"1500462918059-b1a0cb512f1d\",\n\t\"1531581147762-5961e6e2e6b1\",\n\t\"1626204327506-0d3ee11d7752\",\n\t\"1549068106-b024baf5062d\",\n];\n```\n\nNext, in our `li` elements we'll ad an `img` tag with a `src` attribute containing a Unsplash URL. For now, we'll just hardcode the first image ID along with the other parts of the URL:\n\n```tsx\n// inside the map function\n\n<li key={item} class=\"h-[500px] w-full rounded-2xl bg-rose-300\">\n\t<img\n\t\tsrc={`https://images.unsplash.com/photo-1500462918059-b1a0cb512f1d?w=800&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8dmlicmFudHxlbnwwfHwwfHx8MA%3D%3D`}\n\t\talt=\"\"\n\t/>\n</li>\n```\n\nWe can see that this works with our hardcoded image:\n\n![Hardcoded images display](https://res.cloudinary.com/epic-web/image/upload/v1705956320/ead2e2ced6b169bdec07c41f08af649b.png)\n\nSince we want to use the different image ids, we'll update the `map` function to have an `index` parameter. Since we won't be using the item itself, we will follow the convention of replacing it with an underscore.\n\n```tsx\n[...Array(4).keys()].map((_, index) => (\n  // ...\n))\n```\n\nNow we can update the image `src` to be a string template literal that uses the `index` to access the image ids in the array with `${imageIds[index]}`:\n\n```tsx\n<ul class=\"flex w-full max-w-6xl gap-4\">\n\t{[...Array(4).keys()].map((_, index) => (\n\t\t<li key={index} class=\"h-[500px] w-full rounded-2xl bg-rose-300\">\n\t\t\t<img\n\t\t\t\tsrc={`https://images.unsplash.com/photo-${imageIds[index]}?w=800&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8dmlicmFudHxlbnwwfHwwfHx8MA%3D%3D`}\n\t\t\t\talt=\"\"\n\t\t\t/>\n\t\t</li>\n\t))}\n</ul>\n```\n\nEach image now displays, but they aren't taking up the entire card. They also need to take up the full width of the space since the finished cards will expand and contract:\n\n![Images aren't taking up the entire card](https://res.cloudinary.com/epic-web/image/upload/v1705956322/6dcee08cf60dc026a02347d03a33cd36.png)\n\nThe parent `li` needs to have `relative` positioning, and the image needs to have `absolute` positioning since there will be accompanying text.\n\nThe image's width and height will be full, and the inset will be zero to ensure that it covers the entire parent:\n\n```tsx\n<li key={index} class=\"relative h-[500px] w-full rounded-2xl bg-rose-300\">\n\t<img\n\t\tclass=\"absolute inset-0 w-full h-full\"\n\t\tsrc={`https://images.unsplash.com/photo-${imageIds[index]}?w=800&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8dmlicmFudHxlbnwwfHwwfHx8MA%3D%3D`}\n\t\talt=\"\"\n\t/>\n</li>\n```\n\nThis is looking better, but now the images are a bit skewed and are covering the rounded corners of the parent:\n\n![Images cover the parent's rounded corners](https://res.cloudinary.com/epic-web/image/upload/v1705956324/714719b33856ffbaf5f8dc5de4a14216.png)\n\nThe rounded corner problem can be fixed by adding `overflow-hidden` to the parent `li`, and the skewed problem can be fixed by adding `object-cover` to the image classes:\n\n```tsx\n<li\n\tkey={index}\n\tclass=\"relative h-[500px] w-full overflow-hidden rounded-2xl bg-rose-300\"\n>\n\t<img\n\t\tclass=\"absolute inset-0 w-full h-full object-cover\"\n\t\tsrc={`https://images.unsplash.com/photo-${imageIds[index]}?w=800&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8dmlicmFudHxlbnwwfHwwfHx8MA%3D%3D`}\n\t\talt=\"\"\n\t/>\n</li>\n```\n\nNow it looks nice:\n\n![Images display properly](https://res.cloudinary.com/epic-web/image/upload/v1705956325/a75e44559e76aa6ead3d052b2be86c56.png)\n\nKeep in mind we aren't going for a pixel perfect replica. Instead, we're focusing on achieving the interesting hover state effect of fluid width changes.",
						"slug": "dynamicallyadd-images-to-the-cards",
						"videoResource": {
							"_createdAt": "2024-01-23T17:33:26Z",
							"title": "04-Card-images",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/04-Card-images.mp4",
							"_updatedAt": "2024-03-19T20:24:15Z",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_rev": "6LaeqP6n94P8FD3sVhYEit",
							"_id": "L2nFczBOHr0LI7xrpQndgw",
							"duration": 144.576,
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "100vkCZpl3x02XY1cwqOcw8to1Ci2KtsaZztgc8ocT7cE",
								"muxAssetId": "rIg3BEvBjPdBzh7AOFEXHtZt00uhwa9ngZVMHOm6WD3g",
								"_type": "muxAsset"
							}
						},
						"solution": null
					},
					{
						"videoResource": {
							"_id": "ZB9FUjBzEPYolLuh0oPeLD",
							"title": "05-Text-content",
							"castingwords": {
								"transcript": "no"
							},
							"muxAsset": {
								"muxPlaybackId": "HCCQePavz7CMxKBQEqrRN012QOZHnUdhXjMwZzIi5ngE",
								"muxAssetId": "QDL5N9O01nUR2VviVI8E1D006CPVnbLGcfj1N6REBAhbM",
								"_type": "muxAsset"
							},
							"_updatedAt": "2024-03-19T20:24:13Z",
							"_rev": "6LaeqP6n94P8FD3sVhYEh3",
							"_type": "videoResource",
							"duration": 107.077333,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/05-Text-content.mp4",
							"_createdAt": "2024-01-23T17:33:27Z"
						},
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2024-01-23T17:59:31Z",
						"title": "Positioning and Styling Text in Cards",
						"description": "Add and style text within the cards.",
						"body": "Let's add text to our cards.\n\nThe card title will be an `h2` wrapped with a `div` below the `img` tag.\n\nWe could use Flexbox or Grid to align the text at the bottom, but instead we'll use absolute positioning.\n\nIn the `div`'s `class` we'll add `absolute bottom-0 inset-x-0` to position the title at the bottom of the card.\n\nThe `h2`'s class will be `text-2xl font-medium text-white` to increase the text size, set the weight to medium, and color the text white respectively:\n\n```jsx\n// Below the <img> tag in the map function\n\n<div class=\"absolute inset-x-0 bottom-0\">\n\t<h2 class=\"text-2xl font-medium text-white\">The card title is here.</h2>\n</div>\n```\n\nHere's the result:\n\n![Cards with hardcoded titles](https://res.cloudinary.com/epic-web/image/upload/v1705956409/614d2bdb5708bb64f5443efcba62364b.png)\n\nNext, we'll add a paragraph of Lorem Ipsum text below the card title. We'll give it a class of `text-white/70` to make the text white with 70% opacity:\n\n```jsx\n// <img .../>\n<div class=\"absolute inset-x-0 bottom-0\">\n\t<h2 class=\"text-2xl font-medium text-white\">The card title is here.</h2>\n\t<p>\n\t\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\t\teius.\n\t</p>\n</div>\n```\n\nThere are some additional styles we'll need to add to make the text look good.\n\nThe surrounding `div` will get `p-4` padding to give the text some space. The paragraph will get some `mt-2` margin top to give it some separation from the title, and an opacity of 70% to make it easier to read but not distract from the title:\n\n```tsx\n// <img .../>\n<div class=\"absolute inset-x-0 bottom-0 p-4\">\n\t<h2 class=\"text-2xl font-medium text-white\">The card title is here.</h2>\n\t<p class=\"mt-2 text-white/70\">\n\t\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\t\teius.\n\t</p>\n</div>\n```\n\nHere's the result:\n\n![Cards with text](https://res.cloudinary.com/epic-web/image/upload/v1705956410/c8c3aa51df893a82a17c03f18d8e49bb.png)\n\nNow that the text looks good, we can work on adding a subtle background overlay to make it more readable.",
						"_id": "ZB9FUjBzEPYolLuh0oRFf6",
						"slug": "positioning-and-styling-text-in-cards"
					},
					{
						"description": "Adding a background gradient can enhance the readability of text laid over vibrant images.",
						"slug": "enhance-text-readability-with-a-gradient",
						"_id": "L2nFczBOHr0LI7xrpQqfbR",
						"_updatedAt": "2024-01-23T19:09:29Z",
						"title": "Enhance Text Readability with a Gradient",
						"body": "Due to the intensity of the images, the small text on the cards isn't very readable:\n\n![The text is hard to read](https://res.cloudinary.com/epic-web/image/upload/v1705956449/27bf0fbbca6bead2d7a27e41277d2593.png)\n\nAdding a subtle background gradient behind the text is a common technique used to enhance readability in these types of situations.\n\nWe can see on the DJI website that they have implemented this effect. If you look closely at their card text, you can see a faint shadow starting from the bottom and gradually fading out as it climbs up:\n\n![The cards on the DJI site](https://res.cloudinary.com/epic-web/image/upload/v1705956451/4ec872907b59e50765f004ebdb0017a1.png)\n\nIn order to implement the gradient in our cards, we'll start by adding the `bg-gradient-to-t` and `from-black` classes to the div housing the title and the paragraph:\n\n```jsx\n<div class=\"absolute inset-x-0 bottom-0 bg-gradient-to-t from-black p-4\">\n\t<h2 class=\"text-2xl font-medium text-white\">The card title is here.</h2>\n\t<p class=\"mt-2 text-white/70\">\n\t\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\t\teius.\n\t</p>\n</div>\n```\n\nThe `bg-gradient-to-t` utility class tells the gradient to go from the bottom to the top, and `from-black` sets the starting color of the gradient to black:\n\n![First try at adding the gradient](https://res.cloudinary.com/epic-web/image/upload/v1705956453/0f8605e4cdbc35071303e138e726125a.png)\n\nThe gradient looks a little low, and a bit too dark. Let's move it up a bit by setting the `from` position to 30% and then set the black to 70% opacity:\n\n```jsx\n<div class=\"absolute inset-x-0 bottom-0 bg-gradient-to-t from-black/70 from-30% p-4\">\n```\n\nThis has the more subtle effect we are looking for:\n\n![The subtle gradient effect we want](https://res.cloudinary.com/epic-web/image/upload/v1705956454/2d4367da969887c478d711dc8900b2b9.png)\n\nWith the text styled and readable, we can move on to getting that cool hover effect implemented.",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/06-Text-background-overlay.mp4",
							"_id": "L2nFczBOHr0LI7xrpQne3R",
							"muxAsset": {
								"muxPlaybackId": "Gay01i18uI3e3Ofqta02lazByOM9kXOLtdcA6ezIGAzoA",
								"muxAssetId": "HfxWGr3yXJemX64rXFr18VJVjzN7bvZRrUW5S6JL00Cg",
								"_type": "muxAsset"
							},
							"_updatedAt": "2024-03-19T20:24:12Z",
							"_rev": "6LaeqP6n94P8FD3sVhYEfD",
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_createdAt": "2024-01-23T17:33:27Z",
							"duration": 122.244,
							"_type": "videoResource",
							"title": "06-Text-background-overlay"
						},
						"solution": null,
						"_type": "lesson"
					},
					{
						"description": "By adding some classes to the cards, we can hide and reveal text on hover.",
						"slug": "hide-and-reveal-text-on-hover",
						"videoResource": {
							"title": "07-Text-reveal-on-hover",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/07-Text-reveal-on-hover.mp4",
							"duration": 105.744,
							"_id": "ZB9FUjBzEPYolLuh0oPeTm",
							"transcript": {
								"text": "no",
								"srt": null
							},
							"muxAsset": {
								"muxPlaybackId": "dy00NED601Kfm8pje00BWfQghzjMrlOwvyn5U0201zhyNcKg",
								"muxAssetId": "F5sPynpqhU2023UhXUnMo7XWCxEs753FENzdF00s00fY2o",
								"_type": "muxAsset"
							},
							"_createdAt": "2024-01-23T17:33:29Z",
							"_rev": "6LaeqP6n94P8FD3sVhYEdN",
							"_type": "videoResource",
							"castingwords": {
								"transcript": "no"
							},
							"_updatedAt": "2024-03-19T20:24:10Z"
						},
						"solution": null,
						"_type": "lesson",
						"title": "Hide and Reveal Text on Hover",
						"body": "On the DJI site, initially only the card's title is visible. Once hovered, the card expands to reveal the accompanying text:\n\n![Hover behavior on the DJI site](https://res.cloudinary.com/epic-web/image/upload/v1705956507/419e3a77265e0d977b6dec1baf2163e7.png)\n\nFor now there is no built-in way to animate the height of an element from `0` to `auto` in CSS, but there are some workarounds.\n\nWe'll start by setting the height of the paragraph to `0` and specifying that the overflow should be hidden:\n\n```jsx\n<p class=\"h-0 overflow-hidden text-white/70\">\n\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia\n\tipsaeius.\n</p>\n```\n\nEach card's paragraph is now hidden:\n\n![Paragraphs are now hidden](https://res.cloudinary.com/epic-web/image/upload/v1705956508/e47a0cf3efaf8b87ed0f27b468158487.png)\n\nNow on each `<li>`, we'll add a `group` class to the `li` element:\n\n```jsx\n<li\n  key={index}\n  class=\"group relative h-[500px] w-full overflow-hidden rounded-2xl bg-rose-300\"\n>\n```\n\nThen on the `<p>` tag we'll add `group-hover:[100px]` to set the paragraph's height to 100px when the card is hovered:\n\n```jsx\n<p class=\"h-0 overflow-hidden text-white/70 group-hover:[100px]\">\n\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia\n\tipsaeius.\n</p>\n```\n\nWe are setting the height to 100px for now because if we set it to `auto` the paragraph will just jump to the full height when hovered without a smooth transition.\n\n![Paragraphs now show on hover](https://res.cloudinary.com/epic-web/image/upload/v1705956510/1f4d0be8301720227d528805d52ef334.png)\n\nNow that we have the animation working, we can move on to developing a workaround for dynamically animating the height of the paragraph.",
						"_id": "L2nFczBOHr0LI7xrpQqfow",
						"_updatedAt": "2024-01-24T16:32:32Z"
					},
					{
						"body": "Currently we are animating to a fixed height of 100px on hover since browsers can't animate from 0 to `auto`:\n\n```jsx\n<p class=\"h-0 overflow-hidden text-white/70 transition-all group-hover:h-[100px]\">\n\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\teius.\n</p>\n```\n\nLuckily there's a handy trick we can use to get around this limitation!\n\nFirst, we'll remove the the `h-0` and `group-hover:h-[100px]` and `transition-all` classes from the paragraph as we won't need them anymore. Add the `mt-2` class back to the paragraph as well to help with spacing:\n\n```jsx\n<p class=\"mt-2 overflow-hidden text-white/70\">\n\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\teius.\n</p>\n```\n\nNow we'll wrap the paragraph with a `div` containing a class of `grid`, and set the grid row to `0fr`. Then we can then animate the grid row to `1fr` on hover and the `transition-all` class from the paragraph to the new `grid` div:\n\n```jsx\n<div class=\"grid grid-rows-[0fr] transition-all  group-hover:grid-rows-[1fr]\">\n\t<p class=\"mt-2 overflow-hidden text-white/70\">\n\t\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia ipsa\n\t\teius.\n\t</p>\n</div>\n```\n\nThis will effectively animate the height of the paragraph from 0 to its full height when we hover over the card.\n\nUsing the fractional unit `fr` for the sizing is the trick that makes this work. It's a flexible unit of measurement for CSS Grid that distributes the available space evenly among the rows and columns.\n\nIn our case, setting the grid row to `0fr` will hide the paragraph since it's a zero height row. Then, when we hover over the card, we change the grid row to `1fr`, which will reveal the paragraph and have it take up all available space.\n\n![Progress on card text display](https://res.cloudinary.com/epic-web/image/upload/v1705956555/8bfd5138be8957c071307ee4f532e0af.png)\n\nWe are almost finished with the text. The last thing we need to do with it is create a nice fade-in transition for some added polish.",
						"slug": "use-css-grid-to-animate-elements-with-dynamic-height",
						"_id": "L2nFczBOHr0LI7xrpQqgBR",
						"_type": "lesson",
						"_updatedAt": "2024-01-23T19:16:00Z",
						"title": "Use CSS Grid to Animate Elements with Dynamic Height",
						"description": "Learn a CSS Grid technique to animate elements from zero to their full height on hover.",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "L81gXyK3SZ96VmFWyHt6qyD43vicmv8OsAUnWQd1alI",
								"muxAssetId": "YD63B9rcWdRLvdXB5R01kTf01BQu3pTNkPcj5EPeHJQgk",
								"_type": "muxAsset"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"castingwords": {
								"transcript": "no"
							},
							"_createdAt": "2024-01-23T17:33:30Z",
							"_type": "videoResource",
							"_id": "2Xou4j4vYd192Jra6hoEoZ",
							"title": "08-CSS-Grid-height-animation",
							"_updatedAt": "2024-03-19T20:24:09Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/08-CSS-Grid-height-animation.mp4",
							"duration": 88.410667,
							"_rev": "6LaeqP6n94P8FD3sVhYEYm"
						},
						"solution": null
					},
					{
						"_updatedAt": "2024-01-23T19:19:15Z",
						"title": "Animate Text Opacity",
						"description": "Add an interactive hover effect that changes the opacity of text.",
						"slug": "animate-text-opacity",
						"solution": null,
						"_id": "L2nFczBOHr0LI7xrpQqgOw",
						"body": "Let's create a fade in effect for our text that starts at 0% opacity then transitions to 100% opacity on hover.\n\nOn the paragraph, add the `opacity-0`, `group-hover:opacity-100`, and `transition` classes:\n\n```jsx\n<p class=\"mt-2 overflow-hidden text-white/70 opacity-0 transition group-hover:opacity-100\">\n```\n\n![The transition effect has been added](https://res.cloudinary.com/epic-web/image/upload/v1705956576/aedf9ff5b1cbc0c8d063bb37cf9f632f.png)\n\nWith these classes the transition happens very quickly.\n\nIn order to slow it down, we can add the `duration-300` class to set it to 300ms:\n\n```jsx\n<p class=\"mt-2 overflow-hidden text-white/70 opacity-0 transition duration-300 group-hover:opacity-100\">\n```\n\nNow that the text is polished up, let's work on animating the background width!",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "3zY8gY01QFR88DgHfKxKb7BEeD01hS1bLt528iya3s302E",
								"muxAssetId": "Bja8001JOItBllPxDCJGEmZfrLea5hQ01sbcKxgfhu2Xw",
								"_type": "muxAsset"
							},
							"_id": "2Xou4j4vYd192Jra6hoF49",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/09-Text-fade-in-transition.mp4",
							"duration": 46.477333,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_updatedAt": "2024-03-19T20:24:07Z",
							"_rev": "9CeTKuUcQZRsVUft8VlXwH",
							"title": "09-Text-fade-in-transition",
							"_type": "videoResource",
							"castingwords": {
								"transcript": "no"
							},
							"_createdAt": "2024-01-23T17:33:31Z"
						},
						"_type": "lesson"
					},
					{
						"_type": "lesson",
						"_updatedAt": "2024-01-23T19:36:57Z",
						"description": "Update the effect so the hovered card expands and the other cards shrink using Flexbox.",
						"body": "The cards on the DJI site have an effect where the hovered card expands and the other cards shrink:\n\n![Card behavior on the DJI website](https://res.cloudinary.com/epic-web/image/upload/v1705956609/790e5c76fca38e6238a8ea172860324f.png)\n\nIn order to emulate this effect we'll use Flexbox.\n\nWe'll start by adding the classes `flex-1` and `hover:grow-[2]` to the `li` element, and removing the `group-hover` class.\n\nThe `flex-1` class acts similarly to `fr`, which is a unit relative to the available space. Since all of the `li` items have `flex-1`, they take equal parts divided by 4.\n\nWhen we hover, the `hover:grow-[2]` class sets the `flex-grow` to 2, which means that the hovered card will take up twice the space of the other cards:\n\n```jsx\n<li\n  key={index}\n  class=\"group relative h-[500px] w-full overflow-hidden rounded-2xl bg-rose-300 flex-1 hover:grow-[2]\"\n>\n```\n\nHere's what the effect looks like now:\n\n![The hovered card takes up too much space](https://res.cloudinary.com/epic-web/image/upload/v1705956607/8dc06051ade0c1af66d0551b04b6a1c9.png)\n\nYou can see here that the hovered card is taking up a lot of space.\n\nAdjusting `hover:grow-` to `1.25` will make for a more subtle effect:\n\n```jsx\n<li\n  key={index}\n  class=\"group relative h-[500px] w-full overflow-hidden rounded-2xl bg-rose-300 flex-1 hover:grow-[1.25]\"\n>\n```\n\n![The card takes up an appropriate amount of space](https://res.cloudinary.com/epic-web/image/upload/v1705956610/6ef4131c97bb8b22cbe44efea6346aa2.png)\n\nPerfect.\n\nThe problem now is that the text jumps on hover due to the word length. We'll fix this next!",
						"slug": "animate-a-cards-width-on-hover-with-flexbox",
						"solution": null,
						"_id": "L2nFczBOHr0LI7xrpQqgcR",
						"videoResource": {
							"castingwords": {
								"transcript": "no"
							},
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_type": "videoResource",
							"title": "10-Card-fluid-width-effect",
							"_updatedAt": "2024-03-19T20:24:06Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/10-Card-fluid-width-effect.mp4",
							"duration": 155.377333,
							"_createdAt": "2024-01-23T17:33:32Z",
							"_rev": "WsAmd18YA1frbQaWBZpegs",
							"muxAsset": {
								"muxPlaybackId": "01ywA022fkj02Hf6dFBatpF39RbUsScIb6NVQ2ERgG852c",
								"muxAssetId": "A1BEIFoLBddHiTfTeY5qGIKMJX013zjFRfHJhJZNjCiI",
								"_type": "muxAsset"
							},
							"_id": "2Xou4j4vYd192Jra6hoFJj"
						},
						"title": "Animate A Card's Width on Hover with Flexbox"
					},
					{
						"title": "Prevent Line Break Jumps with Fixed Widths",
						"slug": "prevent-line-break-jumps-with-fixed-widths",
						"videoResource": {
							"transcript": {
								"text": "no",
								"srt": null
							},
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "fF4NODKebVYTbDafBlY5JUlyV8BNwx01HEnAMC2cpUMs",
								"muxAssetId": "Cn02e02dqRLu13DEzcgny3ywR0013uBFgYkHorZRA2ygUw",
								"_type": "muxAsset"
							},
							"title": "11-Preventing-line-break-jumps",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/11-Preventing-line-break-jumps.mp4",
							"duration": 64.910667,
							"_rev": "WsAmd18YA1frbQaWBZpebS",
							"_id": "ZB9FUjBzEPYolLuh0oPefC",
							"castingwords": {
								"transcript": "no"
							},
							"_createdAt": "2024-01-23T17:33:33Z",
							"_updatedAt": "2024-03-19T20:24:04Z"
						},
						"solution": null,
						"_id": "2Xou4j4vYd192Jra6hrH2T",
						"_type": "lesson",
						"_updatedAt": "2024-01-23T19:34:22Z",
						"description": "Setting a fixed width for card text will prevent line break jumps when the card expands and contracts.",
						"body": "Adding a fixed width to the text will prevent the line breaks from jumping around.\n\nHowever, we can't add a fixed width to the parent div because that would cause the background gradient to shrink with it:\n\n![Fixed width on the parent shrinks the gradient](https://res.cloudinary.com/epic-web/image/upload/v1705956649/4faeff5de0f8a641e1e2d98cc61b3858.png)\n\nInstead, we'll wrap the text content in a new `div` with a fixed width of `w-48`. We'll also polish the line height of the `<h2>` title by adding the `leading-tight` class:\n\n```jsx\n<div class=\"absolute inset-x-0 bottom-0 bg-gradient-to-t from-black/70 from-30% p-4\">\n\t<div class=\"w-48\">\n\t\t<h2 class=\"text-2xl font-medium leading-tight text-white\">\n\t\t\tThe card title is here.\n\t\t</h2>\n\t\t<div class=\"grid grid-rows-[0fr] transition-all  group-hover:grid-rows-[1fr]\">\n\t\t\t<p class=\"mt-2 overflow-hidden text-white/70 opacity-0 transition duration-300 group-hover:opacity-100\">\n\t\t\t\tLorem ipsum dolor sit, amet consectetur adipisicing elit. Minima quia\n\t\t\t\tipsa eius.\n\t\t\t</p>\n\t\t</div>\n\t</div>\n</div>\n```\n\nNow the text doesn't jump around on hover, and the cards look great:\n\n![Cards look great!](https://res.cloudinary.com/epic-web/image/upload/v1705956647/de95edae9d87f4afc6d1fbaafbccd51f.png)"
					},
					{
						"title": "Concluding the Fluid Hover Cards Tutorial",
						"slug": "concluding-the-fluid-hover-cards-tutorial",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/fluid-hover-cards/12-Wrap-up.mp4",
							"_id": "L2nFczBOHr0LI7xrpQnePw",
							"_createdAt": "2024-01-23T17:33:34Z",
							"_rev": "6LaeqP6n94P8FD3sVhYENk",
							"_type": "videoResource",
							"title": "12-Wrap-up",
							"_updatedAt": "2024-03-19T20:24:02Z",
							"muxAsset": {
								"muxAssetId": "cD70059gUxRk9O02lM02xRApHICef1jZr02yySutACctoi8",
								"_type": "muxAsset",
								"muxPlaybackId": "TR02bjlu402XNl00Jn02l8ifDrZTIk702lfpoxTASxlQbxcs"
							},
							"duration": 31.844,
							"transcript": {
								"text": "no",
								"srt": null
							},
							"castingwords": {
								"transcript": "no"
							}
						},
						"solution": null,
						"_id": "L2nFczBOHr0LI7xrpQqhCR",
						"_updatedAt": "2024-01-23T19:34:45Z",
						"body": "We've successfully recreated the fluid hover effect of the DJI.com cards UI with Tailwind CSS!\n\nThis was an iterative process, but there are two main takeaways that you should keep in mind when working on your own projects.\n\nWhen it comes to height, with CSS Grid and the `fr` fractional units you can animate the height of an element from zero to the height it needs to be.\n\nFor the fluid width, use Flexbox. Using `flex-1` on an item enables it to grow and shrink as needed while taking up the available space provided by the parent. Then when using `hover:grow-[1.25]` on the card, it will grow to 1.25 times its initial size, causing the other cards to shrink.",
						"_type": "lesson",
						"description": "This was an iterative process, but there are two main takeaways that you should keep in mind when working on your own projects."
					}
				]
			}
		]
	},
	{
		"_id": "63a6016b-ec54-4076-b318-2580cfa2c204",
		"_updatedAt": "2024-03-27T12:38:38Z",
		"moduleType": "tutorial",
		"body": "The closer you are to the server, the faster your experience will be.\n\nIt wasn't long ago that web app developers had to choose a region where they thought the bulk of their users would be located, leaving people on the other side of the world with longer waiting times.\n\nFortunately, times have changed and it's now possible to deploy an application and have it be distributed worldwide– and it's not as tough as it sounds!\n\nThis tutorial will show you how to deploy a multi-region Node.js application with a persistent database, ensuring optimal performance no matter where your users are located thanks to Fly.io.\n\nHere are some of the topics you'll tackle along the way:\n\n## Deployment with Fly.io\n\nThe first step is to get acquainted with configuring Docker and Fly's CLI tool by deploying the demo Counter app.\n\nAfter all, you have to have everything working in one region before you make it work in multiple regions!\n\n## Data Persistence and Health Checks\n\nYou'll learn how to create persisted volumes to avoid data loss, as well as the steps for setting up health checks and custom endpoints for monitoring the app.\n\n## Automatic Deployment with GitHub Actions\n\nThe deployment process is pretty easy thanks to Fly's CLI tool– but automating it makes it even easier 😎\n\n## Creating a Staging Environment\n\nTesting your app safely in a staging environment before it goes live is a great way to ensure a smoother rollout with reduced risk of downtime. This is a huge benefit that comes from a minimal amount of configuration.\n\n## Embrace the Distributed Architecture\n\nOnce you understand how the pieces fit together, you'll be ready to deploy the demo application to regions all around the world.  \nBut it's not just the app itself. Fly.io supports LiteFS, which is used to replicate SQLite databases. This means your database can live on the edge, right beside your application.\n\nEverything you learn about deployment in this tutorial is ready for you to put into action for your own applications.\n\nReady to fly?\n\nLet's do this!",
		"instructor": {
			"links": null,
			"_updatedAt": "2024-03-27T05:48:41Z",
			"_createdAt": "2024-03-27T05:48:41Z",
			"name": "Kent C. Dodds",
			"picture": {
				"url": "https://cdn.sanity.io/images/i1a93n76/production/ef97de2cb638463af3562c055ff442a917eedeba-800x800.png",
				"alt": "Kent C. Dodds"
			},
			"slug": "kent-c-dodds",
			"_id": "61052001-bcc3-4f25-bec8-4e767889924c",
			"_type": "contributor",
			"bio": "A world renowned speaker, teacher, open source contributor, created epicweb.dev, epicreact.dev, testingjavascript.com. instructs on egghead.io, frontend masters, google developer expert."
		},
		"_type": "module",
		"title": "Deploy Web Applications All Over the World",
		"slug": {
			"current": "deploy-web-applications",
			"_type": "slug"
		},
		"image": "https://cdn.sanity.io/images/i1a93n76/production/8022b1d11f6a96f05a1b6b3c7b2e98de09488263-667x664.png",
		"_createdAt": "2023-04-06T14:54:26Z",
		"description": null,
		"state": "published",
		"sections": [
			{
				"description": null,
				"slug": "project-setup-and-first-deployment",
				"lessons": [
					{
						"title": "Intro to Deploy Distributed Apps with Node.js & Fly",
						"body": "Hello everyone! My name is Kent C. Dodds and I'm thrilled that you have decided to join me on this tutorial to learn how to deploy distributed Node.js apps with your data all over the world.\n\nIn this tutorial, we'll be deploying our Node.js app and our SQLite database on fly.io.\n\nThis has multiple regions all over the world so you can get your data and your app as close to the user as possible.\n\nLet's take a quick look at our demo app before we get started.\n\nThe demo app is really simple- it's just a counter that's persisted in the database. This way, we can make sure that deploying migrations and other processes happen correctly.\n\nWe can use this to get a good idea of what Fly is capable of, and then you can apply that to more complex applications.\n\nThe application is built with the regular `http` module from Node.js, and Prisma is used for migrations. Prisma is the best ORM for JavaScript, TypeScript, and Node that I've experienced!\n\nThe application's TypeScript code is basic, and nothing to worry yourself too much about.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815777/epicweb.dev/tutorials/deploy-web-applications/images-01-intro.mp4/01-intro_18_01-42600-then-we-just-have-basic-application-code-going-on-here.png)\n\nDeploying distributed apps is actually a pretty complicated thing in general, but Fly makes it really straightforward.\n\nBut the way that I'm going to do this tutorial is that you can follow along with me in your own app and then stop at the point where you are able to apply what you've learned.\n\nFor example, if you don't need to run your application in multiple regions, you can stop partway through this tutorial.\n\nBut if you feel like your application necessitates the complexities of multi-region, then continue to follow along with the extra multi-region stuff. Maybe eventually our tools will get so good that you won't even be able to feel a difference between deploying in a single region and deploying in multiple!\n\nI hope that you enjoy this journey with me and that you can build excellent user experiences with the tools.\n\nSee you in the tutorial!",
						"_id": "6456fe5f-05a0-42fa-863f-8956bad60f8c",
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"description": "Learn how to deploy a Node.js application that persists data to multiple regions.",
						"slug": "intro-to-deploy-distributed-apps-with-node-js-and-fly",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "Qi901EEy01ZFJ2A2RoMH5aKkRkYZjDMWunKwPlov788Lw",
								"muxAssetId": "JmuuDfrDlUJLkVYifSgUKw3dnYFSB008RmbzJXVz2zIA"
							},
							"title": "Get Started Deploying Video",
							"slug": {
								"current": "get-started-deploying-video",
								"_type": "slug"
							},
							"duration": 201.766667,
							"castingwords": {
								"transcript": "Kent C. Dodds: [0:00] Hey, everybody. My name is Kent C. Dodds, and I am so excited that you have decided to join me on this tutorial to learn how to deploy distributed Node.js apps with your data all over the world. I am excited about this because I feel like it makes for better user experiences and we have some really fantastic tools.  \n  \n[0:19] In this tutorial, we're going to be deploying our Node.js app and our SQLite database on Fly.io, which has multiple regions all over the world so you can get your data and your app as close to the user as possible.  \n  \n[0:33] Their tagline is \"Deploy App Servers Close to Your Users. Run your full-stack apps all over the world. No ops required.\" Fly is amazing at this, so I'm excited to take you through this.  \n  \n[0:45] I want to give you a quick look at our demo app of what we're going to be using and then we can get into this. The demo app itself is actually really simple. It's literally just a counter, but this count is actually persisted in the database and so we can make sure that deploying migrations and stuff happens and we have persistent volumes and all of that.  \n  \n[1:06] We should be able to get pretty good exposure to what Fly is capable of and then you can extrapolate that to more complex applications. The application, like I said, very simple. We have HTTP from Node.js, so it's just like the regular HTTP module, not even using Express or Fastify or any of those.  \n  \n[1:26] We do have Prisma because Prisma has nice migrations. Frankly, Prisma is by far the best ORM for JavaScript, and TypeScript, and Node that I have experienced. It's fantastic. Then we just have basic application code going on in here.  \n  \n[1:42] Nothing that you need to worry yourself too much about. There's not a whole lot here. Like I said, it's pretty simple. It is written in TypeScript, so we make sure that we know where we run things like a build and the same sort of thing for any tests or something like that.  \n  \n[1:58] Then also we have Prisma here so that we know when it's appropriate to run migrations and that sort of thing as well. We're going to get started with this. I hope that you're ready and excited like I am to deploy your distributed app all over the world.  \n  \n[2:14] I'll add one other thing, and that is that deploying distributed apps is a pretty complicated thing in general. Fly makes it straightforward, but the way that I'm going to do this tutorial is that you can follow along with me in your own app and then stop at the point where you feel like I'm getting the right tradeoffs for my application.  \n  \n[2:35] Because in a lot of cases, you don't need to run in multiple regions. You're probably just fine with your particular constraints and use cases and requirements to just deploy to a single region.  \n  \n[2:46] If that's you, then go ahead and stop partway through this tutorial and you can feel free to watch the rest. If you feel like your application necessitates the complexities of multi-region, then continue to follow along with the extra multi-region stuff.  \n  \n[3:03] Maybe eventually our tools will get so good that you won't even be able to feel the difference between deploying in a single region and deploying in multiple.  \n  \n[3:11] With all of that said, I hope that you enjoy this journey with me and that you can build excellent user experiences with the tools that we have these days. See you in the tutorial."
							},
							"_createdAt": "2023-04-06T15:53:06Z",
							"_updatedAt": "2024-03-19T20:41:32Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/total-typescript/982d396a-8671-48cf-ad50-dbbf7a542c49/01-intro-f82tdrskj.mp4",
							"_rev": "9CeTKuUcQZRsVUft8Vmtw4",
							"_type": "videoResource",
							"_id": "4dbda3b5-7eb2-4266-8ce4-2bd7a7c8ea72",
							"transcript": {
								"text": "Kent C. Dodds: [0:00] Hey, everybody. My name is Kent C. Dodds, and I am so excited that you have decided to join me on this tutorial to learn how to deploy distributed Node.js apps with your data all over the world. I am excited about this because I feel like it makes for better user experiences and we have some really fantastic tools.  \n  \n[0:19] In this tutorial, we're going to be deploying our Node.js app and our SQLite database on Fly.io, which has multiple regions all over the world so you can get your data and your app as close to the user as possible.  \n  \n[0:33] Their tagline is \"Deploy App Servers Close to Your Users. Run your full-stack apps all over the world. No ops required.\" Fly is amazing at this, so I'm excited to take you through this.  \n  \n[0:45] I want to give you a quick look at our demo app of what we're going to be using and then we can get into this. The demo app itself is actually really simple. It's literally just a counter, but this count is actually persisted in the database and so we can make sure that deploying migrations and stuff happens and we have persistent volumes and all of that.  \n  \n[1:06] We should be able to get pretty good exposure to what Fly is capable of and then you can extrapolate that to more complex applications. The application, like I said, very simple. We have HTTP from Node.js, so it's just like the regular HTTP module, not even using Express or Fastify or any of those.  \n  \n[1:26] We do have Prisma because Prisma has nice migrations. Frankly, Prisma is by far the best ORM for JavaScript, and TypeScript, and Node that I have experienced. It's fantastic. Then we just have basic application code going on in here.  \n  \n[1:42] Nothing that you need to worry yourself too much about. There's not a whole lot here. Like I said, it's pretty simple. It is written in TypeScript, so we make sure that we know where we run things like a build and the same sort of thing for any tests or something like that.  \n  \n[1:58] Then also we have Prisma here so that we know when it's appropriate to run migrations and that sort of thing as well. We're going to get started with this. I hope that you're ready and excited like I am to deploy your distributed app all over the world.  \n  \n[2:14] I'll add one other thing, and that is that deploying distributed apps is a pretty complicated thing in general. Fly makes it straightforward, but the way that I'm going to do this tutorial is that you can follow along with me in your own app and then stop at the point where you feel like I'm getting the right tradeoffs for my application.  \n  \n[2:35] Because in a lot of cases, you don't need to run in multiple regions. You're probably just fine with your particular constraints and use cases and requirements to just deploy to a single region.  \n  \n[2:46] If that's you, then go ahead and stop partway through this tutorial and you can feel free to watch the rest. If you feel like your application necessitates the complexities of multi-region, then continue to follow along with the extra multi-region stuff.  \n  \n[3:03] Maybe eventually our tools will get so good that you won't even be able to feel the difference between deploying in a single region and deploying in multiple.  \n  \n[3:11] With all of that said, I hope that you enjoy this journey with me and that you can build excellent user experiences with the tools that we have these days. See you in the tutorial.",
								"srt": null
							}
						},
						"solution": null
					},
					{
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Signing Up for Fly.io",
						"videoResource": {
							"duration": 209.166667,
							"transcript": {
								"text": "Instructor: [0:00] To get started with Fly, you go to fly.io and click on get started. The first step here is to install flyctl. This is the Fly command line interface that you use on your local machine to configure your Fly and do all sorts of things.  \n  \n[0:16] You'll notice in the documentation for Fly that sometimes it uses flyctl and sometimes it uses simply Fly. That's because they both work. I always just use Fly. I'm not sure why they have flyctl. Just use Fly. Regardless, whatever your machine is, whether it's Mac OS, Linux, or Windows, they have instructions here on how you get things installed.  \n  \n[0:38] I already have things installed myself and I installed things using Brew because I have Homebrew to manage all sorts of things, but you install it however you see fit. I do recommend Homebrew myself if you're on Mac OS. Then you can move on to the signup.  \n  \n[0:53] I want to add also that if you do run into problems in the installation process or anything else with Fly, Fly has a really great community.  \n  \n[1:02] If you click on this Discourse icon or logo up in the top right, you'll go to community.fly.io where you can create new questions and answer other people's questions, and participate in the community. It's a really awesome community and they're really helpful if you ever run into any issues.  \n  \n[1:21] Let's move on to the signup where you run fly auth signup, so we'll run that in our terminal. This will open up the signup page where you can type in your full name, email, and password. I'll say Kody the Koala. That's me+kody@kentcdodds.com.  \n  \n[1:41] I'll type in a password. Probably I recommend a password manager. Then here it's going to ask for a payment method because remember that Fly is a business and they need to pay the bills.  \n  \n[1:53] However, their free tier is actually really good and so even though you enter a credit card number in here, you're probably not going to be charged if you're just following this tutorial, so don't worry about that. But you do need to provide a credit card if you want to go through this tutorial with me and of course if you want to actually deploy and manage an app on their hosting platform.  \n  \n[2:15] With that entered in, I'm going to go Add Card. With that card added to our account, we're now able to use Fly. Let's go back to our terminal, and we see that we successfully logged in and we're ready to rock and roll with Fly.  \n  \n[2:33] Now let's go to the Fly dashboard. If we go to fly.io and we're signed in, now this link at the top says Dashboard, which will take us to fly.io/dashboard. Now it's giving us tips on how to deploy various frameworks and things. One thing here is it says your email is not confirmed, so we need to verify our email.  \n  \n[2:53] You'll just go to your email, click on the link to verify, and you'll be verified. If at any time you need to log out, then you say fly auth logout, and that will log you out of Fly. Then if you need to log back in again, you say fly auth login. That will open up your browser again.  \n  \n[3:11] If you're logged in in your browser, then you can simply click on Continue or you can sign in as a different user. If you're not logged in in your browser, then you just enter your credentials and continue. I'm going to continue as me+kody, and now we're signed in again. That's how you sign up for Fly.",
								"srt": null
							},
							"title": "02-sign-up",
							"_rev": "WsAmd18YA1frbQaWBZudQ2",
							"_updatedAt": "2024-03-19T20:41:30Z",
							"_type": "videoResource",
							"_id": "b37629b5-c114-4281-953a-a58d7a57aaf3",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/d6ed02c0-e72e-425d-b4c8-623fba661623/02-sign-up-tzjgly34s.mp4",
							"muxAsset": {
								"muxPlaybackId": "GN4NIfj2Jc2Y6z00RifktzRmdDuLlyF4p6E63xeafR9E",
								"muxAssetId": "I0002b7hu01eDA0102sm00jFZunKHTDUpgiwLUzRsivZA01t8o"
							},
							"castingwords": {
								"transcript": "Instructor: [0:00] To get started with Fly, you go to fly.io and click on get started. The first step here is to install flyctl. This is the Fly command line interface that you use on your local machine to configure your Fly and do all sorts of things.  \n  \n[0:16] You'll notice in the documentation for Fly that sometimes it uses flyctl and sometimes it uses simply Fly. That's because they both work. I always just use Fly. I'm not sure why they have flyctl. Just use Fly. Regardless, whatever your machine is, whether it's Mac OS, Linux, or Windows, they have instructions here on how you get things installed.  \n  \n[0:38] I already have things installed myself and I installed things using Brew because I have Homebrew to manage all sorts of things, but you install it however you see fit. I do recommend Homebrew myself if you're on Mac OS. Then you can move on to the signup.  \n  \n[0:53] I want to add also that if you do run into problems in the installation process or anything else with Fly, Fly has a really great community.  \n  \n[1:02] If you click on this Discourse icon or logo up in the top right, you'll go to community.fly.io where you can create new questions and answer other people's questions, and participate in the community. It's a really awesome community and they're really helpful if you ever run into any issues.  \n  \n[1:21] Let's move on to the signup where you run fly auth signup, so we'll run that in our terminal. This will open up the signup page where you can type in your full name, email, and password. I'll say Kody the Koala. That's me+kody@kentcdodds.com.  \n  \n[1:41] I'll type in a password. Probably I recommend a password manager. Then here it's going to ask for a payment method because remember that Fly is a business and they need to pay the bills.  \n  \n[1:53] However, their free tier is actually really good and so even though you enter a credit card number in here, you're probably not going to be charged if you're just following this tutorial, so don't worry about that. But you do need to provide a credit card if you want to go through this tutorial with me and of course if you want to actually deploy and manage an app on their hosting platform.  \n  \n[2:15] With that entered in, I'm going to go Add Card. With that card added to our account, we're now able to use Fly. Let's go back to our terminal, and we see that we successfully logged in and we're ready to rock and roll with Fly.  \n  \n[2:33] Now let's go to the Fly dashboard. If we go to fly.io and we're signed in, now this link at the top says Dashboard, which will take us to fly.io/dashboard. Now it's giving us tips on how to deploy various frameworks and things. One thing here is it says your email is not confirmed, so we need to verify our email.  \n  \n[2:53] You'll just go to your email, click on the link to verify, and you'll be verified. If at any time you need to log out, then you say fly auth logout, and that will log you out of Fly. Then if you need to log back in again, you say fly auth login. That will open up your browser again.  \n  \n[3:11] If you're logged in in your browser, then you can simply click on Continue or you can sign in as a different user. If you're not logged in in your browser, then you just enter your credentials and continue. I'm going to continue as me+kody, and now we're signed in again. That's how you sign up for Fly."
							},
							"_createdAt": "2023-04-07T14:42:51Z"
						},
						"solution": null,
						"slug": "signing-up-for-fly-io",
						"_id": "6013d987-4802-4669-9462-fc5cc6cc31c3",
						"_type": "lesson",
						"description": "Fly.io is the service we'll use for deploying our app. The first step is signing up for an account. ",
						"body": "To get started with Fly, go to fly.io and click on Get Started.  \nThe first step is to install `flyctl`, the Fly command line interface that you use on your local machine to configure Fly and do all sorts of things.\n\nYou'll notice in the documentation for Fly that sometimes it uses `flyctl` and sometimes it uses simply `fly`. That's because they both work. I always use `fly`.\n\nMac OS, Linux, and Windows are all supported. If you are a Mac user, I recommend installing using `brew`.\n\nIf you run into problems in the installation process or anything else with Fly, Fly has a really great community. If you click on the Discourse icon or logo up in the top right, you'll go to <a href=\"https://community.fly.io/\" target=\"_blank\" rel=\"noopener\">community.fly.io</a> where you can create new questions and answer other people's questions and participate in the community.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815780/epicweb.dev/tutorials/deploy-web-applications/images-02-sign-up.mp4/02-sign-up_12_01-15000-where-you-can-create-new-questions-and-answer-other-peoples-questions-and-participate-in-the-community.png)\n\nLet's move on to the sign up where you run `fly auth signup` in the terminal. This will open up the signup page where you can sign up.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815793/epicweb.dev/tutorials/deploy-web-applications/images-02-sign-up.mp4/02-sign-up_14_01-27000-well-run-that-in-our-terminal.png)\n\nHere Fly is going to ask for a payment method because remember that they are a business and they need to pay the bills.\n\nHowever, their free tier is actually really good. Even though you enter a credit card number in here, you're probably not going to be charged if you're just following this tutorial. Don't worry about that. You do need to provide a credit card if you want to go through this tutorial with me.\n\nLet's go back to our terminal. We see that we successfully logged in and are ready to rock and roll with Fly!\n\nNow let's go to the Fly dashboard. If we go to fly.io and we're signed in, now this link at the top says dashboard, which will take us to fly.io/dashboard.\n\nThe dashboard has tips on how to deploy various frameworks and things. One thing here is it says your email is not confirmed, so we need to verify our email. You'll just go to your email, click on the link to verify, and you'll be verified.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815790/epicweb.dev/tutorials/deploy-web-applications/images-02-sign-up.mp4/02-sign-up_25_02-53000-one-thing-here-is-it-says-your-email-is-not-confirmed--so-we-need-to-verify-our-email.png)\n\nIf at any time you need to log out, then you say `fly auth logout` and that will log you out of Fly.\n\nIf you need to log back in, say `fly auth login` and your browser will open.\n\nIf you're already logged in, click \"continue\" or sign in as a different user. If you're not logged in, enter your credentials and continue.\n\nThat's how you sign up for Fly!"
					},
					{
						"description": "We will deploy to Fly with Docker. A Dockerfile is used to tell Docker the steps to take to run our app.",
						"body": "Fly has docs for deploying from various places, but we are going to deploy with Docker.\n\nWe will build our own Docker file, and then we'll configure Fly to use it.\n\n## Adding a `dockerignore` File\n\nThe first thing we will do is create a file that will ensure that none of these files make it into our Docker image.\n\nThis `.dockerignore` file will contain pretty much everything that's in the `.gitignore` file. We also want to add a couple of things like the `Dockerfile` that we're about to create, the `.dockerignore` file itself, and the `.git` directory because we don't need all of our git history in there.\n\nHere's what's in the `.dockerignore`:\n\n```markdown\nnode_modules\n.env\n/build\n/.vscode\nprisma/sqlite.db\nprisma/sqlite.db-journal\nDockerfile\n.dockerignore\n.git\n```\n\n## Adding the `Dockerfile`\n\nNow it's time to to make our `Dockerfile` itself.\n\nThis is a bit of a beast, so I'm actually going to just copy a whole `Dockerfile` for you, then we will make changes as needed.\n\nSo I'll kind of walk through what this Docker file does and the different pieces of it.\n\nFirst off, we're using the `bullseye-slim` version of Node 18.\n\nThis is a very slimmed down base image for us that has Node pre-installed for us.\n\nAnd then, we're going to use apt-get to get a couple of dependencies for Prisma, specifically OpenSSL and SQLite 3.\n\nWe're making another layer here for all of the dependencies, including dev dependencies.\n\nThen we'll make another layer for our production dependencies only, which basically copies everything over from the deps that we just installed, but then prunes out everything that's the dev dependencies.\n\nAnd this will be the dependencies we use for our running app.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815805/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_7_01-03280-so-ill-kind-of-walk-through-what-this-docker-file-does-and-the-different-pieces-of-it.png)\n\nThen it's time to build the app.\n\nSo based on our base image, we're going to make a directory called `/app/`. We'll move into that directory, and then we'll copy all the dependencies from our dev dependencies layer.\n\nThen we'll add to the Prisma directory so that when we run `prisma generate`, it'll have our Prisma schema. This will generate the Prisma client for us.\n\nAfter that we'll add the rest of the code to our project so that we can build everything else.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815797/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_13_02-07400-and-then-well-add-in-the-prisma-directory-so-that-when-we-run-prisma-generate--itll-have-our-prisma-schema.png)\n\nWe have our final layer, where we specify some environment variables:\n\nFor our database URL, we're going to put that at `file:/app/data/sqlite.db`. The database URL is used by Prisma, which is looking for our database by that environment variable.\n\nOur `PORT` is used by our application right here. It pulls that off of the port number, off of the environment variable.\n\nThe `NODE_ENV` variable will be `production` because we're running in a production environment. We want node to be in production mode.\n\nThen we're going to move into the `/app/` directory and copy a bunch of stuff from our production deps.\n\nWe're only going to get the node modules from production, and then from our build layer, we're going to copy the Prisma client that was generated along with the build directory that is created when we run our build.\n\nAfter that we'll add the rest of the files from our source project just to make sure we cover all of our bases and then run the `npm start` script.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815802/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_18_02-45480-and-then-our-port-is-used-by-our-application-right-here-it-pulls-that-off-of-the-port-number--off-of-the-environment-variable.png)\n\n## The `npm start` Script\n\nThe `npm start` script runs `node start.js`.\n\nInside of `start.js` we have a very important step that runs before we actually start the application, which is to run:\n\n```markdown\nnpx prisma migrate deploy\n```\n\nThis command tells Prisma to look through the migrations and compare those with the database that's actually running. Then it will ensure to run all of the migrations in the running database are missing.\n\nThat's why we run the `prisma migrate deploy` command before we actually run node on the `build` directory where our files were built\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680817370/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_24_03-52440-.png)\n\nAnd so that's what the npm start command does. And that's what we configure Docker to run when the image is ready, and we're running in a container.\n\n## Testing Docker\n\nTo test all of this out and make sure it's working, we're going to run d`ocker build` on the current directory.\n\nThis will run everything in our Docker file.\n\nBecause this is the first time, it may take a little while. It has to install all of the apt-get dependencies. It's going to install our npm dependencies and follow the rest of the steps in the Dockerfile.\n\nAfter a few moments, Docker is ready to go, and we can run `docker images` and see our Docker image.\n\nTo test it out, we will say `docker run` with `-p` to forward port 8080 along with the flags `i` and `t` and our pasted image id.\n\nHere's what the command looks like all together (your image id will be different):\n\n```markdown\ndocker run -p 8080:8080 -i -t 1fca1038c819\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815796/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_35_05-07400-and-that-gets-everything-running--we-can-go-to-port-8080-on-our-local-machine--and-this-is-being-passed-through-to-our-image.png)\n\nNow we can open a browser window to localhost:8080 and see that our app is running through the image!\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815788/epicweb.dev/tutorials/deploy-web-applications/images-03-dockerfile.mp4/03-dockerfile_36_05-11800-and-we-can-see-that-our-app-is-indeed-working-and-running-through-that-image.png)\n\nThis image is now ready to deploy to Fly, and you can adjust it as needed for your application!",
						"slug": "setting-up-docker",
						"videoResource": {
							"_createdAt": "2023-04-07T15:07:50Z",
							"_updatedAt": "2024-03-19T20:41:28Z",
							"transcript": {
								"text": "Kent C. Dodds: [0:00] Looking at our Fly Dashboard, we just have a bunch of links to Fly Docs so that we know how to deploy to these various places. Feel free to read those, but we're going to deploy with Docker. We'll just build our own Dockerfile ourselves. Then we'll get Fly configured for it.  \n  \n[0:14] The first thing we're going to do is add the easy dockerignore, which will make sure that none of these files make it into our Docker image. Pretty much, we want everything that's in the gitignore.  \n  \n[0:27] Then we also want to add a couple other things, like the Dockerfile that we're about to create, the dockerignore file itself, and the git directory, because we don't need all of our git history in there.  \n  \n[0:39] With those, we're also going to make our Dockerfile itself. This one is going to be a bit of a beast. I'm actually going to just copy a whole Dockerfile for you so you don't have to worry about it.  \n  \n[0:56] We will make changes as needed. I'll walk through what this Dockerfile does and the different pieces of it. First off, we're using the bullseye-slim version of node:18. This is a very slimmed-down base image for us that has node preinstalled for us.  \n  \n[1:15] Then we're going to use apt-get to get a couple dependencies for Prisma, specifically OpenSSL and SQLite 3. Then we're making another layer here so that we can have our deps where we get those installed, all of the dependencies, including dev dependencies.  \n  \n[1:33] Then we'll make another layer for our production dependencies only, which basically copies everything over from the deps that we just installed but then prunes out everything that's the dev dependencies. This will be the dependencies we use for our running app.  \n  \n[1:48] Then it's time to build the app. Based on our base image, we're going to make a directory called app. We'll move into that directory. Then we'll copy all the dependencies from our dev dependencies layer.  \n  \n[2:01] Then we'll add in the Prisma directory so that when we run prisma generate, it'll have our Prisma schema. This will generate the Prisma client for us. Then we'll add the rest of the code to our project so that we can build everything else.  \n  \n[2:16] Then we have our final layer, where we specify some environment variables. Our DATABASE_URL, we're going to put that in /app, which is where our working directory is, /data/sqlite.db. The DATABASE_URL is used by Prisma. It's configured right here, where it's looking for our database by that environment variable.  \n  \n[2:35] Then our PORT is used by our application right here. It pulls that off of the port number, off of the environment variable. We use that in our listen right here. That's what that's for. Then our NODE_ENV variable will be production because we're running in a production environment. We want node to be in production mode.  \n  \n[2:57] Then we're going to move into the app directory and copy a bunch of stuff from our production-deps. We're only going to get the node modules from production. Then, from our build layer, we're going to copy the Prisma client that was generated along with the build directory that is created when we run our build.  \n  \n[3:13] Then we'll add the rest of the files from our source project, just to make sure we cover all of our bases, and then run the npm start script. The npm start script, if we take a look at that, that runs the node start.js.  \n  \n[3:29] In here, we have a very important step that runs before we actually start the application. That is to run npx prisma migrate deploy. What this does is it tells Prisma to look through the migrations and compare those with the database that's actually running and make sure to run all of the missing migrations that that running database is missing.  \n  \n[3:52] That's why we run the prisma migrate deploy before we actually run node on the build directory where our files were built. That's what the npm start command does. That's what we configure Docker to run when the image is ready and we're running in a container.  \n  \n[4:09] To test all of this out and make sure it's working, we're going to run docker build on the current directory. This will run everything in our Dockerfile. Because this is the first time, it may take a little while.  \n  \n[4:21] It has to install all of the apt-get dependencies. It's going to install our npm dependencies. Then one of the layers removes the dependencies on development things, so we only have production dependencies.  \n  \n[4:38] Finally, that Docker image is ready to go. If we say, \"docker images,\" then we see we have our image ID right there. Just to test it out, let's do docker run. We need to forward along the port 8080. Then we're going to run with the flags i and t. Then we'll paste our image there. That gets everything running.  \n  \n[5:00] We can go to port 8080 on our local machine. This is being passed through to our image. We can see that our app is indeed working and running through that image. This image is ready for us to deploy to Fly, now that we have our Dockerfile all created.  \n  \n[5:17] You can see how this could change quite a bit based on the needs of your specific application, which is why I'm not going to go too deep into Docker and Dockerfiles in this tutorial. Hopefully, this is a good starting point for you, and you can adjust it as needed for your application.",
								"srt": null
							},
							"castingwords": {
								"transcript": "Kent C. Dodds: [0:00] Looking at our Fly Dashboard, we just have a bunch of links to Fly Docs so that we know how to deploy to these various places. Feel free to read those, but we're going to deploy with Docker. We'll just build our own Dockerfile ourselves. Then we'll get Fly configured for it.  \n  \n[0:14] The first thing we're going to do is add the easy dockerignore, which will make sure that none of these files make it into our Docker image. Pretty much, we want everything that's in the gitignore.  \n  \n[0:27] Then we also want to add a couple other things, like the Dockerfile that we're about to create, the dockerignore file itself, and the git directory, because we don't need all of our git history in there.  \n  \n[0:39] With those, we're also going to make our Dockerfile itself. This one is going to be a bit of a beast. I'm actually going to just copy a whole Dockerfile for you so you don't have to worry about it.  \n  \n[0:56] We will make changes as needed. I'll walk through what this Dockerfile does and the different pieces of it. First off, we're using the bullseye-slim version of node:18. This is a very slimmed-down base image for us that has node preinstalled for us.  \n  \n[1:15] Then we're going to use apt-get to get a couple dependencies for Prisma, specifically OpenSSL and SQLite 3. Then we're making another layer here so that we can have our deps where we get those installed, all of the dependencies, including dev dependencies.  \n  \n[1:33] Then we'll make another layer for our production dependencies only, which basically copies everything over from the deps that we just installed but then prunes out everything that's the dev dependencies. This will be the dependencies we use for our running app.  \n  \n[1:48] Then it's time to build the app. Based on our base image, we're going to make a directory called app. We'll move into that directory. Then we'll copy all the dependencies from our dev dependencies layer.  \n  \n[2:01] Then we'll add in the Prisma directory so that when we run prisma generate, it'll have our Prisma schema. This will generate the Prisma client for us. Then we'll add the rest of the code to our project so that we can build everything else.  \n  \n[2:16] Then we have our final layer, where we specify some environment variables. Our DATABASE_URL, we're going to put that in /app, which is where our working directory is, /data/sqlite.db. The DATABASE_URL is used by Prisma. It's configured right here, where it's looking for our database by that environment variable.  \n  \n[2:35] Then our PORT is used by our application right here. It pulls that off of the port number, off of the environment variable. We use that in our listen right here. That's what that's for. Then our NODE_ENV variable will be production because we're running in a production environment. We want node to be in production mode.  \n  \n[2:57] Then we're going to move into the app directory and copy a bunch of stuff from our production-deps. We're only going to get the node modules from production. Then, from our build layer, we're going to copy the Prisma client that was generated along with the build directory that is created when we run our build.  \n  \n[3:13] Then we'll add the rest of the files from our source project, just to make sure we cover all of our bases, and then run the npm start script. The npm start script, if we take a look at that, that runs the node start.js.  \n  \n[3:29] In here, we have a very important step that runs before we actually start the application. That is to run npx prisma migrate deploy. What this does is it tells Prisma to look through the migrations and compare those with the database that's actually running and make sure to run all of the missing migrations that that running database is missing.  \n  \n[3:52] That's why we run the prisma migrate deploy before we actually run node on the build directory where our files were built. That's what the npm start command does. That's what we configure Docker to run when the image is ready and we're running in a container.  \n  \n[4:09] To test all of this out and make sure it's working, we're going to run docker build on the current directory. This will run everything in our Dockerfile. Because this is the first time, it may take a little while.  \n  \n[4:21] It has to install all of the apt-get dependencies. It's going to install our npm dependencies. Then one of the layers removes the dependencies on development things, so we only have production dependencies.  \n  \n[4:38] Finally, that Docker image is ready to go. If we say, \"docker images,\" then we see we have our image ID right there. Just to test it out, let's do docker run. We need to forward along the port 8080. Then we're going to run with the flags i and t. Then we'll paste our image there. That gets everything running.  \n  \n[5:00] We can go to port 8080 on our local machine. This is being passed through to our image. We can see that our app is indeed working and running through that image. This image is ready for us to deploy to Fly, now that we have our Dockerfile all created.  \n  \n[5:17] You can see how this could change quite a bit based on the needs of your specific application, which is why I'm not going to go too deep into Docker and Dockerfiles in this tutorial. Hopefully, this is a good starting point for you, and you can adjust it as needed for your application."
							},
							"_rev": "WsAmd18YA1frbQaWBZudKc",
							"_type": "videoResource",
							"_id": "bd427a50-dfdd-4ecd-9686-50542f88383d",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/9902151c-fc17-4bd8-b56a-f8eee21c2054/03-dockerfile-n6aqmklpf.mp4",
							"muxAsset": {
								"muxPlaybackId": "YOPoteVmA34rDEv02qykYZFOb00h2A1BEKNjP5JaTjJBo",
								"muxAssetId": "7dLOlj7jFY6FLtfw8WJWJYZapSCdLaFINVtWILOOtD00"
							},
							"title": "03-dockerfile",
							"duration": 335.366667
						},
						"solution": null,
						"_id": "9c25bbe2-f634-4c2e-884c-37a1137feac6",
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Setting Up Docker"
					},
					{
						"solution": null,
						"_id": "880b6007-fd9f-41bc-b237-8cdf3602e55e",
						"title": "Deploying to Fly with Docker",
						"slug": "deploying-to-fly-with-docker",
						"body": "With our Docker file ready, we can now deploy our app using Fly.\n\n## Fly Deployment\n\nFly makes deploying Docker simple. To get started, run `fly launch`.\n\nIt will ask for an app name, which must be globally unique across Fly. You can enter a descriptive name or leave it blank for an autogenerated one.\n\nNext, choose the region where you want to deploy your app. By default, it highlights the region closest to you. We will set up deployment to multiple regions later.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815817/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_7_00-45480-the-one-thats-closest-to-me--which-is-san-jose.png)\n\nAfter creating the app, you can visit the admin URL to see it. Your app will be hosted at `your-app-name.fly.dev`.\n\nFly also writes a config file to `fly.toml` that contains our app name and other details. You can read more about it in their docs.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815804/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_13_01-03820-theres-our-app-name-weve-got.png)\n\nDuring the setup process, Fly may ask if you want to set up Postgres or Redis. For this example, we'll skip both.\n\nFinally, confirm that you want to deploy your app.\n\n## Remote Builder\n\nFly sets up a special Remote Builder app in your account that is responsible for building Docker images remotely. This service is free and enables seamless integration with CI/CD tools like GitHub Actions.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815819/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_23_02-04680-so-we-dont-have-to-build-them-locally.png)\n\nWhen you visit your app's dashboard, you'll be able to see the app your created as well as the builder app.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815804/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_28_02-27400-so-coming-back-to-this-.png)\n\nBack in the terminal, Fly is uploading the image and witl be deploying it when it's finished.\n\n## Useful Fly Commands\n\nThere are several useful `fly` commands to become familiar with.\n\n`fly logs` displays logs for your app based on the `fly.toml `configuration file.\n\nThe `fly status` command shows the current status of your app. Adding `--watch` will continuously update the status.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815810/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_38_03-12320-and-this-will-stay-up-to-date-as-the-deployment-proceeds.png)\n\nOnce it's deployed, `fly open` Opens your app in a browser window.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815831/epicweb.dev/tutorials/deploy-web-applications/images-04-fly-launch.mp4/04-fly-launch_41_03-26480-if-we-look-at-the-logs-now-.png)\n\nAll we did was create a Dockerfile and say `fly launch` and then Fly took care of the rest!",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/04-fly-launch.mp4",
							"castingwords": {
								"transcript": "Man: [0:00] With our Dockerfile in place, Fly knows how to deploy Docker really well. All we have to do is run Fly launch. It's going to ask us a couple of questions. It sees that we've got a Dockerfile. That's great. It's going to ask us our app name. This needs to be globally unique across all of Fly.  \n  \n[0:15] Feel free to guess something that you think might be unique, if it's more descriptive for you, or you can leave it blank. It'll generate one that's not at all descriptive, but sometimes kind of fun. We'll go ahead and let it generate one for us.  \n  \n[0:27] Then it's going to ask us what region we want our thing to be running in. It automatically highlights the one that's closest to you. Of course, we want to deploy to multiple regions, but we've got to deploy to one region first. We'll go ahead and deploy to the one that's closest to me, which is San Jose. Then it creates the app.  \n  \n[0:47] If we go to this admin URL, we're going to see the app there. We'll look at that in a moment. It also tells us where our app is going to be hosted. It's basically just your appname.fly.dev. Then it wrote a config file for us. Let's take a look at that config file. Here it is. There's our app name.  \n  \n[1:03] We've got some pretty standard Fly config stuff in here. I'm not going to belabor all of this. If you want to learn more about the Fly config, then you can go to fly.toml. Just Google fly.toml. You'll find the documentation page. I promise this is such a wonderful read.  \n  \n[1:20] You should grab a hot chocolate and a pillow and snuggle up as you read through all of this wonderful documentation about a toml file. Have fun. [laughs] With that now, it's asking us more questions. Do we want to set up Postgres? Not really. Do we want to set up Redis? Not for this.  \n  \n[1:37] Both of those services are managed and deployed to the edge with read replicas and stuff. It's actually very cool. If that's something you want to do, definitely give it a shot. Now it's asking us if we want to deploy. Let's go ahead and deploy.  \n  \n[1:49] It's going to build our image on a remote builder. What Fly is doing now, is it actually creates a special app in our account that is completely free, that is going to be responsible for building our Docker images remotely.  \n  \n[2:03] We don't have to build them locally. We can do this really easily on GitHub Actions or whatever we want to do. That's quite nice, that it will build remotely. If we go to our dashboard now, we're going to see the new app we created as well as that builder app.  \n  \n[2:17] It's quite the beast of a machine with 50 gigabytes of storage. [laughs] It works perfectly. Again, it is completely free. Coming back to this, we're seeing that we're uploading our image to Fly. Then, once Fly gets that image, it's going to deploy it for us. While we're waiting for that, I want to show you a couple of things.  \n  \n[2:37] Now that we have an app, we can say \"Fly logs.\" It will read our fly.toml to know which app we want logs for. It will start printing out logs as soon as our app starts getting logs. Now with another terminal, we'll say \"Fly status.\" This will give us the status of our app currently. Since it's not deployed, we don't have any instances of it.  \n  \n[2:57] If we want to keep up with what's going on, we can say \"Fly status --watch.\" Now this will keep us up to date with what is happening. Right now, our status is \"Deployment is running.\" This will stay up to date as the deployment proceeds.  \n  \n[3:12] Then the last thing I want to show you is Fly open. That's going to open up to our application, which by golly, it's [laughs] already deployed. We can increment, we can decrement. Everything is working perfectly.  \n  \n[3:25] If we look at the logs now, we can see that they were starting up the instance. They pulled the virtual machine we deployed, and then applied migrations, and then started up the app. As soon as our app started being able to respond to basic TCP checks, then Fly was like, \"OK. This must be ready to go.\" It deployed.  \n  \n[3:48] On HTTPS, we didn't have to worry about any of that. All we did was create a Dockerfile and then said, \"Fly launch.\" Then Fly took care of the rest. That is, to me, a really awesome experience to not have to worry about managing servers or anything like that. That's us in production on Fly."
							},
							"_updatedAt": "2024-03-19T20:41:27Z",
							"duration": 247.133333,
							"_createdAt": "2023-04-07T15:09:27Z",
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "gVCS9kM8cNObnAVh5JJfwScqi39MJhAlJqfQRZBU3nE",
								"muxAssetId": "JTL1wbNt5bG4BkjOgCZf5g0100j02Lmo1V6p6YqmkIHV02Q"
							},
							"_id": "f41fb002-d4ce-4e54-ab07-5b00d97b532f",
							"title": "04-fly-launch.",
							"_rev": "WsAmd18YA1frbQaWBZudFC",
							"transcript": {
								"text": "Man: [0:00] With our Dockerfile in place, Fly knows how to deploy Docker really well. All we have to do is run Fly launch. It's going to ask us a couple of questions. It sees that we've got a Dockerfile. That's great. It's going to ask us our app name. This needs to be globally unique across all of Fly.  \n  \n[0:15] Feel free to guess something that you think might be unique, if it's more descriptive for you, or you can leave it blank. It'll generate one that's not at all descriptive, but sometimes kind of fun. We'll go ahead and let it generate one for us.  \n  \n[0:27] Then it's going to ask us what region we want our thing to be running in. It automatically highlights the one that's closest to you. Of course, we want to deploy to multiple regions, but we've got to deploy to one region first. We'll go ahead and deploy to the one that's closest to me, which is San Jose. Then it creates the app.  \n  \n[0:47] If we go to this admin URL, we're going to see the app there. We'll look at that in a moment. It also tells us where our app is going to be hosted. It's basically just your appname.fly.dev. Then it wrote a config file for us. Let's take a look at that config file. Here it is. There's our app name.  \n  \n[1:03] We've got some pretty standard Fly config stuff in here. I'm not going to belabor all of this. If you want to learn more about the Fly config, then you can go to fly.toml. Just Google fly.toml. You'll find the documentation page. I promise this is such a wonderful read.  \n  \n[1:20] You should grab a hot chocolate and a pillow and snuggle up as you read through all of this wonderful documentation about a toml file. Have fun. [laughs] With that now, it's asking us more questions. Do we want to set up Postgres? Not really. Do we want to set up Redis? Not for this.  \n  \n[1:37] Both of those services are managed and deployed to the edge with read replicas and stuff. It's actually very cool. If that's something you want to do, definitely give it a shot. Now it's asking us if we want to deploy. Let's go ahead and deploy.  \n  \n[1:49] It's going to build our image on a remote builder. What Fly is doing now, is it actually creates a special app in our account that is completely free, that is going to be responsible for building our Docker images remotely.  \n  \n[2:03] We don't have to build them locally. We can do this really easily on GitHub Actions or whatever we want to do. That's quite nice, that it will build remotely. If we go to our dashboard now, we're going to see the new app we created as well as that builder app.  \n  \n[2:17] It's quite the beast of a machine with 50 gigabytes of storage. [laughs] It works perfectly. Again, it is completely free. Coming back to this, we're seeing that we're uploading our image to Fly. Then, once Fly gets that image, it's going to deploy it for us. While we're waiting for that, I want to show you a couple of things.  \n  \n[2:37] Now that we have an app, we can say \"Fly logs.\" It will read our fly.toml to know which app we want logs for. It will start printing out logs as soon as our app starts getting logs. Now with another terminal, we'll say \"Fly status.\" This will give us the status of our app currently. Since it's not deployed, we don't have any instances of it.  \n  \n[2:57] If we want to keep up with what's going on, we can say \"Fly status --watch.\" Now this will keep us up to date with what is happening. Right now, our status is \"Deployment is running.\" This will stay up to date as the deployment proceeds.  \n  \n[3:12] Then the last thing I want to show you is Fly open. That's going to open up to our application, which by golly, it's [laughs] already deployed. We can increment, we can decrement. Everything is working perfectly.  \n  \n[3:25] If we look at the logs now, we can see that they were starting up the instance. They pulled the virtual machine we deployed, and then applied migrations, and then started up the app. As soon as our app started being able to respond to basic TCP checks, then Fly was like, \"OK. This must be ready to go.\" It deployed.  \n  \n[3:48] On HTTPS, we didn't have to worry about any of that. All we did was create a Dockerfile and then said, \"Fly launch.\" Then Fly took care of the rest. That is, to me, a really awesome experience to not have to worry about managing servers or anything like that. That's us in production on Fly.",
								"srt": null
							}
						},
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"description": "With the Dockerfile ready to go, let's deploy our app to Fly."
					}
				],
				"resources": [
					{
						"_id": "145e027d-31c2-41ef-8a47-4444563559da",
						"title": "Fly.io",
						"slug": {
							"current": "fly-io",
							"_type": "slug"
						},
						"_updatedAt": "2023-04-16T06:22:46Z",
						"url": "https://fly.io/",
						"_createdAt": "2023-04-16T06:22:46Z",
						"_rev": "z48mc4rder98a2Em6B4eoB",
						"_type": "linkResource",
						"description": "Deploy app servers close to your users"
					}
				],
				"_id": "b14c08d4-db5e-4b41-96b2-434467b716fa",
				"_type": "section",
				"_updatedAt": "2023-04-16T06:22:59Z",
				"title": "Project Setup and First Deployment"
			},
			{
				"lessons": [
					{
						"slug": "persist-application-data-between-deployments",
						"videoResource": {
							"castingwords": {
								"transcript": "Instructor: [0:00] You may have noticed a problem with our app as you've been deploying things and whatever. We're on this page, I can increment, I can decrement, everything's awesome. We're at five.  \n  \n[0:09] If I go here and I say I want to add a console.log the current count, currentCount. Great. Then we do fly deploy of that, then it actually deploys pretty quick because we've got the builder all set up. Lots of our layers in our Dockerfile are already cached, but it does need to push a couple of changes.  \n  \n[0:32] When that finally does deploy, and we can look at our logs right here to see, it shuts down the old one, starts up the new one, in just a moment as that gets deployed. Here it is starting an instance, configuring a virtual machine, doing all of that. Now it's running node start. Now it's ready to accept traffic and it's at zero. What's going on here?  \n  \n[0:58] The problem is that we have not persisted the volume. We've got our database sitting on a hard drive and every time we deploy, that hard drive is inside of our Docker container. We're deploying a new hard drive basically every time and it's creating a brand new SQLite database.  \n  \n[1:17] You'll actually notice that here it says the following migrations have been applied. It shouldn't have to apply any migrations because last time we deployed, it applied migrations. That's what's going on. Is we are not persisting the volume. This is actually pretty straightforward and simple to do with Fly. Fly has built in support for persisted volumes that are encrypted.  \n  \n[1:38] We say fly vol create data, size 1, 1 gigabyte, and this is going to allow us to choose what region we want the volume in. We're going to want to select the region where our app lives, so we'll do sjc.  \n  \n[1:53] With that, it's going to take a moment, create the volume for us, and now the only change that we actually need to make is to go into our fly.toml and specify that we have a volume that we want to mount. We'll do that in our mounts, where we'll say source is data, that's what we called our volume right here, create data.  \n  \n[2:17] Where we want to put it is up to us but we're going to say destination is /data. That's where I like to put mine. The other thing we have to change here is our Dockerfile, we need to specify where our SQLite database lives.  \n  \n[2:33] We started out specifying that it lives under /app which is within our app directory which is new every time. We want it to just be in simply /data because that is where the mounted volume will be and that's the persisted volume.  \n  \n[2:47] Our SQLite database is now going to stay inside of this special hard drive that is going to live with our application between deploys. With those changes now, we can say fly deploy, and that's going to get our new configuration deployed both with our Docker configuration as well as our fly.toml.  \n  \n[3:07] This should persist our database between deploys, and so we'll test that out. With that successfully deployed let's come over here, refresh the app. We are at zero again because this is a new SQLite database, but now I increment it four times and I'm going to get rid of this log or change it to say current count is, and fly deploy.  \n  \n[3:32] We'll deploy it again and make sure that when we come back it is still four, just to make sure that this persisted volume actually took.  \n  \n[3:42] With that successfully deployed, let's refresh, and boom, it's still at four. That totally worked. All that we had to do was say fly vol create, and we specified what we wanted to call it, it was data, and the size we specified 1.  \n  \n[4:00] Now we've got a persistent volume that we can mount inside of the mounts config of our fly.toml, and then reference inside of our Dockerfile for that database URL so that Prisma will create the SQLite database and then reference it inside of our persisted volume."
							},
							"_createdAt": "2023-04-07T15:11:43Z",
							"duration": 256.766667,
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/05-persisted-volumes.mp4",
							"_updatedAt": "2024-03-19T20:41:25Z",
							"_rev": "WsAmd18YA1frbQaWBZud9m",
							"_type": "videoResource",
							"transcript": {
								"text": "Instructor: [0:00] You may have noticed a problem with our app as you've been deploying things and whatever. We're on this page, I can increment, I can decrement, everything's awesome. We're at five.  \n  \n[0:09] If I go here and I say I want to add a console.log the current count, currentCount. Great. Then we do fly deploy of that, then it actually deploys pretty quick because we've got the builder all set up. Lots of our layers in our Dockerfile are already cached, but it does need to push a couple of changes.  \n  \n[0:32] When that finally does deploy, and we can look at our logs right here to see, it shuts down the old one, starts up the new one, in just a moment as that gets deployed. Here it is starting an instance, configuring a virtual machine, doing all of that. Now it's running node start. Now it's ready to accept traffic and it's at zero. What's going on here?  \n  \n[0:58] The problem is that we have not persisted the volume. We've got our database sitting on a hard drive and every time we deploy, that hard drive is inside of our Docker container. We're deploying a new hard drive basically every time and it's creating a brand new SQLite database.  \n  \n[1:17] You'll actually notice that here it says the following migrations have been applied. It shouldn't have to apply any migrations because last time we deployed, it applied migrations. That's what's going on. Is we are not persisting the volume. This is actually pretty straightforward and simple to do with Fly. Fly has built in support for persisted volumes that are encrypted.  \n  \n[1:38] We say fly vol create data, size 1, 1 gigabyte, and this is going to allow us to choose what region we want the volume in. We're going to want to select the region where our app lives, so we'll do sjc.  \n  \n[1:53] With that, it's going to take a moment, create the volume for us, and now the only change that we actually need to make is to go into our fly.toml and specify that we have a volume that we want to mount. We'll do that in our mounts, where we'll say source is data, that's what we called our volume right here, create data.  \n  \n[2:17] Where we want to put it is up to us but we're going to say destination is /data. That's where I like to put mine. The other thing we have to change here is our Dockerfile, we need to specify where our SQLite database lives.  \n  \n[2:33] We started out specifying that it lives under /app which is within our app directory which is new every time. We want it to just be in simply /data because that is where the mounted volume will be and that's the persisted volume.  \n  \n[2:47] Our SQLite database is now going to stay inside of this special hard drive that is going to live with our application between deploys. With those changes now, we can say fly deploy, and that's going to get our new configuration deployed both with our Docker configuration as well as our fly.toml.  \n  \n[3:07] This should persist our database between deploys, and so we'll test that out. With that successfully deployed let's come over here, refresh the app. We are at zero again because this is a new SQLite database, but now I increment it four times and I'm going to get rid of this log or change it to say current count is, and fly deploy.  \n  \n[3:32] We'll deploy it again and make sure that when we come back it is still four, just to make sure that this persisted volume actually took.  \n  \n[3:42] With that successfully deployed, let's refresh, and boom, it's still at four. That totally worked. All that we had to do was say fly vol create, and we specified what we wanted to call it, it was data, and the size we specified 1.  \n  \n[4:00] Now we've got a persistent volume that we can mount inside of the mounts config of our fly.toml, and then reference inside of our Dockerfile for that database URL so that Prisma will create the SQLite database and then reference it inside of our persisted volume.",
								"srt": null
							},
							"muxAsset": {
								"muxPlaybackId": "rsmoEH02chGhxlcgSgnBv1U17Azz5dfjLp37O6LQBpw4",
								"muxAssetId": "hJTVPoywBZh1Cdi4oJI01HooxVNFr02ZpWywPAjgKx5yQ"
							},
							"title": "05-persisted-volumes",
							"_id": "15c42143-a441-4fbc-b6fd-f8384191b172"
						},
						"_id": "0a688145-7ee4-4f20-a38b-d194affc5275",
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"description": "Currently the counter state is reset to zero after every deployment. Creating a persistent volume with Fly solves this problem.",
						"title": "Persist Application Data Between Deployments",
						"body": "As of right now, incrementing and decrementing the counter works.\n\nBut when we make a change to add a `console.log` and redeploy, the counter is back to zero.\n\nThe count resets because we have not persisted the volume. Every time we deploy, the hard drive for our app is basically brand new.\n\nIn the deployment logs you can see that migrations have been applied, even though they shouldn't have been because they were applied the last time we deployed.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815818/epicweb.dev/tutorials/deploy-web-applications/images-05-persisted-volumes.mp4/05-persisted-volumes_14_01-21360-says-the-following-migrations-have-been-applied.png)\n\nWe will solve this problem by creating a persisted volume with Fly.\n\n## Creating a Persisted Volume with Fly\n\nFly has built-in support for encrypted persisted volumes.\n\nWe'll create a 1 GB volume called `data` by running the following command:\n\n```sh\nfly vol create data --size 1\n```\n\nSelect the region where your app is hosted when prompted, then wait for the volume to be created.\n\n## Updating the Fly Configuration\n\nNow we need to update the `fly.toml` file to specify that we have a newly created volume to mount.\n\nWe'll add the following under the `mounts` section:\n\n```yaml\nsource = \"data\"\ndestination = \"/data\"\n```\n\nThe `source` is the name of the volume we created earlier, and the `destination` is the desired path within the container. You can set it however you want, but I like to use `/data`.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815818/epicweb.dev/tutorials/deploy-web-applications/images-05-persisted-volumes.mp4/05-persisted-volumes_14_01-21360-says-the-following-migrations-have-been-applied.png)\n\n## Updating the Dockerfile\n\nOver in the Dockerfile, we need to modify the database URL to point to the new persisted volume by changing the database URL from `file:/app/data/sqlite.db` to `file:/data/sqlite.db`.\n\nWe're making this change because `/app/` is new every time but `/data` will be persisted.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815823/epicweb.dev/tutorials/deploy-web-applications/images-05-persisted-volumes.mp4/05-persisted-volumes_33_02-53200-to-stay-inside-of-this-special-hard-drive-that.png)\n\n## Deploying the Changes\n\nWith the changes made, run `fly deploy` to update the app with the new configuration.\n\n```sh\nfly deploy\n```\n\nThis will deploy the updated Docker configuration and `fly.toml` file, persisting the database between deploys.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815827/epicweb.dev/tutorials/deploy-web-applications/images-05-persisted-volumes.mp4/05-persisted-volumes_36_03-13840-and-so-well-test-that-out.png)\n\n## Testing Persistence\n\nAfter successfully deploying the updated configuration, refresh the app.\n\nIncrement the count a few times, then make a small change to the code, like removing the `console.log`, then deploy the app again using `fly deploy`.\n\nOnce the new deployment is complete, refresh the app and verify that the count remains the same, confirming that the persisted volume works as intended!\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815827/epicweb.dev/tutorials/deploy-web-applications/images-05-persisted-volumes.mp4/05-persisted-volumes_45_03-55880-and-all-that-we-had-to-do-was-say-fly-vol-create.png)\n\nTo recap, we created a persistent volume with Fly. We then updated the configurations in `fly.toml` and the `Dockerfile` allow the app's SQLite database to persist between deployments. This allows us to ensure the count doesn't reset to zero on each deploy.",
						"solution": null
					},
					{
						"_type": "lesson",
						"title": "Adding Deployment Health Checks",
						"body": "Applications that you care about should be tested.\n\nWhile we won't get into testing in this tutorial, Fly does feature health checks that can be thought of as a last line of defense when deploying an application.\n\n## TCP Health Check\n\nIn our `fly.toml` file we can see that Fly already has a TCP health check in place.This ensures the application is ready to accept network connections, but it doesn't actually make a request to the application.\n\n```yaml\n[[services.tcp_checks]]\n  grace_period = \"1s\"\n  interval = \"15s\"\n  restart_limit = 0\n  timeout = \"2s\"\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815840/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_5_00-22040-called-a-tcp-check-which-will-just-make-sure-that-our-application-is-ready-to.png)\n\n## Adding HTTP Health Checks\n\nTo further enhance our checks, we can add an HTTP check to make a request to the application and deploy it only if it responds with a 200 OK status.\n\nInside of the Fly.io docs is more information about `services.http_checks` along with a starter we will copy.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815841/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_11_01-16200-were-going-to-copy-this-and-bring-it-over-here-right-below-our.png)\n\nAdd the following lines below the TCP check in `fly.toml`:\n\n```yaml\n[[services.http_checks]]\ninterval = 10000\ngrace_period = \"1s\"\nmethod = \"get\"\npath = \"/\"\nprotocol = \"http\"\nrestart_limit = 0\ntimeout = 500\ntls_skip_verify = false\n```\n\nThis configuration will perform an HTTP GET request every 10 seconds. The grace period of 1 seconds is how long it waits to check after the application starts up. You might need to make this longer for larger apps.\n\nThe restart limit is set to 0, which means it won't automatically restart the server after failed HTTP checks. You might want to increase this in your own applications.\n\nThere is also the option to add HTTP headers to your checks.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815835/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_29_03-15560-you-want-to-make-authenticated-requests-or-something-like-that-you-can-add.png)\n\n## Testing Health Checks\n\nLet's do a `fly deploy` to test our health checks.\n\nAfter deploying your application with the new HTTP health check, notice that 2 health checks ran and passed.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815835/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_36_03-57360-so-now-there-are-two-of-them-and-after-a-while-theyre-both-passing-so-if-we-go.png)\n\nTo test the health checks, you can deliberately introduce an error in your code and deploy the application.\n\nFor example, you can misspell \"GET\" in the `index.ts` application code and redeploy the app.\n\nFly will try to deploy the broken version, but since the HTTP health check fails the terminal shows us 1 passing, 1 critical:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815869/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_44_04-43600-all-right-so-now-we-see-that-we-do-still-have-those-two-health-checks-one-is.png)\n\nBecause the health check failed, Fly will roll back to the previously good deployment.\n\n## Adding Custom Health Check Endpoints\n\nYou can add multiple health checks to further ensure that your application is working.\n\nFor example, we an create a custom health check endpoint that checks the database connection by making some queries.\n\nInside of `fly.toml` copy and paste the existing `services.http_checks` block and change the path to `/healthcheck`:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815838/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_67_07-19880-not-only-will-check-that-i-can-accept-traffic-on-my-home-page-but-also-will.png)\n\nThen inside of `index.ts` we'll add a case to the `switch` statement for `GET /healthcheck` that will get the current count:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815845/epicweb.dev/tutorials/deploy-web-applications/images-06-healthcheck.mp4/06-healthcheck_73_08-12680-res-right-head-200-and-res-end-okay-so-were-all-set-there-and-so-now-we-have.png)\n\nWith this custom health check, we can verify not only that the application can accept traffic on the home page, but also that it can make queries to the database.\n\n## Wrapping Up\n\nOur application now has three health checks: the TCP health check, the HTTP health check for the home page, and the additional health check endpoint that checks the database connection.\n\nRemember, health checks are not a replacement for tests but serve as a helpful last line of defense before deploying an application!",
						"videoResource": {
							"_rev": "WsAmd18YA1frbQaWBZubtC",
							"_id": "7748a731-1d14-4898-ad2a-3ec9e2ae4c20",
							"title": "06-healthcheck",
							"duration": 534.733333,
							"_type": "videoResource",
							"_updatedAt": "2024-03-19T20:41:22Z",
							"muxAsset": {
								"muxPlaybackId": "1Z02zsxYng2cVo7nrbCUdx00ja9Kt02Lomtw4tW025E3RW4",
								"muxAssetId": "Yy3zYeDeHz4f27KwiaYNDgB014L9VP5Ivtvo1RWdKgfM"
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/06-healthcheck.mp4",
							"castingwords": {
								"transcript": "Lecturer: [0:00] Applications that you care about should be tested, and we're not going to get into testing specifically in this tutorial. However, we do have a little bit of a last line of defense that is offered to us by Fly in the form of health checks.  \n  \n[0:14] In fact, we already have one health check here. It's called a TCP check, which will make sure that our application is ready to accept network connections, but it doesn't actually connect to our application, make a request, or anything like that.  \n  \n[0:28] What would be really nice is if, right before deploying, we could just make a quick HTTP call to our application and say, \"Can you accept traffic yet?\" If it responds with a 200 OK, then let's go ahead and deploy the app. If it doesn't, then let's not deployed the app. Let's keep the old one around and then go figure out why it didn't respond.  \n  \n[0:47] This is a last line of defense. It's not intended to replace regular testing, but it can be really helpful in deploying applications where there's a lot of moving parts and you want to have just one last little check. This is called HTTP checks. If you look up fly.toml and go to the configuration documentation page, then you can find the services.http_checks config.  \n  \n[1:12] We're going to copy this and bring it over here, right below our TCP check. This interval is how frequently this HTTP check is going to be made, because it's not just when the application starts. We want to make sure that our application is healthy over time, and so every 10 seconds, it will say, \"Hey, are you still healthy?\"  \n  \n[1:33] Then, the grace_period is how long after our application has started up does this HTTP check get called. I'm actually fine with it being one second for this simple application, but some applications take a little while to boot up. Depending on how long it takes for your application to boot up, you'll adjust that grace_period.  \n  \n[1:53] The method, we can do a GET. You could also do a HEAD request here. We'll just do a GET. We'll do the path to our home page. That seems like a pretty reasonable thing, that people should be able to get to our home page. That should cover a fair bit of ground as a last line of defense.  \n  \n[2:07] The protocol we're going to leave as HTTP because this is a behind-at-the-network request that's going to be happening from a Fly server to our server, so there's no certificate going on there. We'll leave that as HTTP.  \n  \n[2:18] Restart_limit. This is how many times, after a failed HTTP check, should Fly automatically restart the server for you. The default is zero. I'm going to leave it there. What zero means is it disables that behavior. It will never restart your application, once the HTTP checks start happening.  \n  \n[2:36] Depending on your use case or use situation, you might bump that up a couple of times. After a couple failures, go ahead and try and restart, and magically maybe it'll work. Then, the timeout. It's making requests to your application. How long are you OK with it waiting to get a response before it decides, \"Oh, must be failing\"? I'll just leave that as 200.  \n  \n[2:57] Something more reasonable, I suppose, might be 500 milliseconds. It depends on how fast your application responds. The tls_skip_verify. This doesn't actually matter all that much because our protocol is HTTP. Then, any headers. If you want to make authenticated requests or something like that, you can add headers down here.  \n  \n[3:17] With all of that, let's do a fly deploy to get a basic health check out there. Then, after this is successfully deployed and it's running, let's go ahead and see what the experience would be like if we actually did deploy something that totally blew up our home page, for example. We'll speed up here a little bit to get it deployed, and then we'll give that a test.  \n  \n[3:46] Here, you'll see that it has two health checks. Before, it only had the one, TCP, but now we've added our HTTP check, and so now there are two of them. After a while, they're both passing, so if we go and take a look at our application, we can see that it's still running. Everything is working just fine and things are happy.  \n  \n[4:04] Of course, you probably ran your test as well, hopefully, and so you are pretty confident that things are good. This just double-checks that everything is going to work out. Now let's make little mistake here. What if I misspell GET right here? That would not be good, and I don't have any TypeScript that's going to double-check that that's right.  \n  \n[4:23] Maybe I should, but let's go ahead and do a fly deploy right now and see what happens as that gets deployed. Let's expand this so we can see all of the output. We'll speed that up for you too. Now we see that we do still have those two health checks.  \n  \n[4:42] One is passing, one is critical. It's not passing, and if we look at our logs over here, we're going to see things are not looking super great for our application. The reason is, if we look at our code, when we had that typo on GET, what happens is it passes this case.  \n  \n[5:01] It continues down in here, and it's going to give us a 404, not found, which is not going to pass our health check. Eventually, Fly is going to decide, \"OK, that's not going to fly,\" literally, and it will roll back to the previous version, so we'll wait for that. Great. Our deploy failed due to unhealthy allocations, so it rolled back to job version 18 and deployed as version 20.  \n  \n[5:31] We tried to deploy the version that had the breakage in our index here, where we misspelled GET, and that deployed, but Fly didn't actually start sending traffic to it until after the health checks passed. They never passed, and so it rolled back and restarted our application with the previous working version, the version that was up before.  \n  \n[5:56] It shut down the version that was not working and wasn't able to accept traffic, and it started up a previous version that was working before, and now our application is working. There are various strategies for deploying with Fly that you can use. There's the rolling strategy, which is the default for a Node application that has a persisted volume, and that's the one that we have.  \n  \n[6:19] There are other strategies you can employ to reduce the amount of time that your deployment is waiting on health checks. In our case, the right thing happened. We did not deploy a app that didn't work. We just made traffic hold off while the deploy was happening and while those health checks were being checked.  \n  \n[6:38] Because they didn't successfully happen, we ended up shutting down the new deploy, starting up the previous version, so that we could accept traffic. Then, hopefully, we get some alerts and say, \"Hey, that deploy did not work, and so you're going to want to take a look at it and figure out what you did wrong.\"  \n  \n[6:55] Another thing that I want to do with this is add another health check to make it really certain that things are working, because you can add multiple of these, and that can be quite useful. For example, on my own personal website, I have a health check endpoint, so we'll do the same thing here, /healthcheck.  \n  \n[7:13] What this does is it not only will check that I can accept traffic on my home page, but also will make some queries to the database to make sure that the database is in a healthy state as well. We'll go ahead and add a health check endpoint here at the top. Case, it'll be a GET request to /healthcheck.  \n  \n[7:35] With this, we'll just do a try-catch. Of course, we need to add a break. It's a switch statement, after all. In the catch, we'll add a console.error. We'll log the error, and then we'll say, res.writeHead(500) and res.end. We can do \"Internal Server Error\" or just \"ERROR,\" whatever you want to do there.  \n  \n[7:55] The actual check that we want to do, let's just say we want to do this query that we're making on the back end. We'll say, await getCurrentCount, and res.writeHead(200) and res.end(\"OK\"). We're all set there, and so now we have three health checks.  \n  \n[8:13] We've got the TCP health check that came with the initial fly.toml that we started with, the HTTP health check for the home page, and the additional health check endpoint that we can do whatever types of health checks we want. Make sure we have connections with the right services, all of that stuff happening inside of our health check endpoint.  \n  \n[8:34] That's how you make sure that you don't deploy a broken app to production, as a last line of defense. Again, this is not a replacement for tests, but it's a really good last line of defense, right before you switch from it is almost deployed to actually being deployed and accepting traffic. Really helpful feature from Fly."
							},
							"_createdAt": "2023-04-07T15:12:49Z",
							"transcript": {
								"text": "Lecturer: [0:00] Applications that you care about should be tested, and we're not going to get into testing specifically in this tutorial. However, we do have a little bit of a last line of defense that is offered to us by Fly in the form of health checks.  \n  \n[0:14] In fact, we already have one health check here. It's called a TCP check, which will make sure that our application is ready to accept network connections, but it doesn't actually connect to our application, make a request, or anything like that.  \n  \n[0:28] What would be really nice is if, right before deploying, we could just make a quick HTTP call to our application and say, \"Can you accept traffic yet?\" If it responds with a 200 OK, then let's go ahead and deploy the app. If it doesn't, then let's not deployed the app. Let's keep the old one around and then go figure out why it didn't respond.  \n  \n[0:47] This is a last line of defense. It's not intended to replace regular testing, but it can be really helpful in deploying applications where there's a lot of moving parts and you want to have just one last little check. This is called HTTP checks. If you look up fly.toml and go to the configuration documentation page, then you can find the services.http_checks config.  \n  \n[1:12] We're going to copy this and bring it over here, right below our TCP check. This interval is how frequently this HTTP check is going to be made, because it's not just when the application starts. We want to make sure that our application is healthy over time, and so every 10 seconds, it will say, \"Hey, are you still healthy?\"  \n  \n[1:33] Then, the grace_period is how long after our application has started up does this HTTP check get called. I'm actually fine with it being one second for this simple application, but some applications take a little while to boot up. Depending on how long it takes for your application to boot up, you'll adjust that grace_period.  \n  \n[1:53] The method, we can do a GET. You could also do a HEAD request here. We'll just do a GET. We'll do the path to our home page. That seems like a pretty reasonable thing, that people should be able to get to our home page. That should cover a fair bit of ground as a last line of defense.  \n  \n[2:07] The protocol we're going to leave as HTTP because this is a behind-at-the-network request that's going to be happening from a Fly server to our server, so there's no certificate going on there. We'll leave that as HTTP.  \n  \n[2:18] Restart_limit. This is how many times, after a failed HTTP check, should Fly automatically restart the server for you. The default is zero. I'm going to leave it there. What zero means is it disables that behavior. It will never restart your application, once the HTTP checks start happening.  \n  \n[2:36] Depending on your use case or use situation, you might bump that up a couple of times. After a couple failures, go ahead and try and restart, and magically maybe it'll work. Then, the timeout. It's making requests to your application. How long are you OK with it waiting to get a response before it decides, \"Oh, must be failing\"? I'll just leave that as 200.  \n  \n[2:57] Something more reasonable, I suppose, might be 500 milliseconds. It depends on how fast your application responds. The tls_skip_verify. This doesn't actually matter all that much because our protocol is HTTP. Then, any headers. If you want to make authenticated requests or something like that, you can add headers down here.  \n  \n[3:17] With all of that, let's do a fly deploy to get a basic health check out there. Then, after this is successfully deployed and it's running, let's go ahead and see what the experience would be like if we actually did deploy something that totally blew up our home page, for example. We'll speed up here a little bit to get it deployed, and then we'll give that a test.  \n  \n[3:46] Here, you'll see that it has two health checks. Before, it only had the one, TCP, but now we've added our HTTP check, and so now there are two of them. After a while, they're both passing, so if we go and take a look at our application, we can see that it's still running. Everything is working just fine and things are happy.  \n  \n[4:04] Of course, you probably ran your test as well, hopefully, and so you are pretty confident that things are good. This just double-checks that everything is going to work out. Now let's make little mistake here. What if I misspell GET right here? That would not be good, and I don't have any TypeScript that's going to double-check that that's right.  \n  \n[4:23] Maybe I should, but let's go ahead and do a fly deploy right now and see what happens as that gets deployed. Let's expand this so we can see all of the output. We'll speed that up for you too. Now we see that we do still have those two health checks.  \n  \n[4:42] One is passing, one is critical. It's not passing, and if we look at our logs over here, we're going to see things are not looking super great for our application. The reason is, if we look at our code, when we had that typo on GET, what happens is it passes this case.  \n  \n[5:01] It continues down in here, and it's going to give us a 404, not found, which is not going to pass our health check. Eventually, Fly is going to decide, \"OK, that's not going to fly,\" literally, and it will roll back to the previous version, so we'll wait for that. Great. Our deploy failed due to unhealthy allocations, so it rolled back to job version 18 and deployed as version 20.  \n  \n[5:31] We tried to deploy the version that had the breakage in our index here, where we misspelled GET, and that deployed, but Fly didn't actually start sending traffic to it until after the health checks passed. They never passed, and so it rolled back and restarted our application with the previous working version, the version that was up before.  \n  \n[5:56] It shut down the version that was not working and wasn't able to accept traffic, and it started up a previous version that was working before, and now our application is working. There are various strategies for deploying with Fly that you can use. There's the rolling strategy, which is the default for a Node application that has a persisted volume, and that's the one that we have.  \n  \n[6:19] There are other strategies you can employ to reduce the amount of time that your deployment is waiting on health checks. In our case, the right thing happened. We did not deploy a app that didn't work. We just made traffic hold off while the deploy was happening and while those health checks were being checked.  \n  \n[6:38] Because they didn't successfully happen, we ended up shutting down the new deploy, starting up the previous version, so that we could accept traffic. Then, hopefully, we get some alerts and say, \"Hey, that deploy did not work, and so you're going to want to take a look at it and figure out what you did wrong.\"  \n  \n[6:55] Another thing that I want to do with this is add another health check to make it really certain that things are working, because you can add multiple of these, and that can be quite useful. For example, on my own personal website, I have a health check endpoint, so we'll do the same thing here, /healthcheck.  \n  \n[7:13] What this does is it not only will check that I can accept traffic on my home page, but also will make some queries to the database to make sure that the database is in a healthy state as well. We'll go ahead and add a health check endpoint here at the top. Case, it'll be a GET request to /healthcheck.  \n  \n[7:35] With this, we'll just do a try-catch. Of course, we need to add a break. It's a switch statement, after all. In the catch, we'll add a console.error. We'll log the error, and then we'll say, res.writeHead(500) and res.end. We can do \"Internal Server Error\" or just \"ERROR,\" whatever you want to do there.  \n  \n[7:55] The actual check that we want to do, let's just say we want to do this query that we're making on the back end. We'll say, await getCurrentCount, and res.writeHead(200) and res.end(\"OK\"). We're all set there, and so now we have three health checks.  \n  \n[8:13] We've got the TCP health check that came with the initial fly.toml that we started with, the HTTP health check for the home page, and the additional health check endpoint that we can do whatever types of health checks we want. Make sure we have connections with the right services, all of that stuff happening inside of our health check endpoint.  \n  \n[8:34] That's how you make sure that you don't deploy a broken app to production, as a last line of defense. Again, this is not a replacement for tests, but it's a really good last line of defense, right before you switch from it is almost deployed to actually being deployed and accepting traffic. Really helpful feature from Fly.",
								"srt": null
							}
						},
						"solution": null,
						"_id": "71f8a7d8-142d-4979-8e60-bbc02fd309bf",
						"description": "While not a replacement for testing, Fly offers health checks to prevent broken code from being deployed.",
						"slug": "adding-deployment-health-checks",
						"_updatedAt": "2023-06-07T08:25:56Z"
					},
					{
						"body": "With a long-running node server and a database, sometimes it's useful to ssh into the virtual machine to explore the file system, and look at the database.\n\nYou can do that with Fly!\n\n### Connecting to the Running Virtual Machine\n\nTo get a console into the running virtual machine, use the `fly ssh console` command.\n\nOnce connected, you can list the directories and files with `ls -al`.\n\nThe application is in the `/app` directory, and the SQLite database in the `/data`:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815846/epicweb.dev/tutorials/deploy-web-applications/images-07-ssh-sqlite-console.mp4/07-ssh-sqlite-console_6_00-31840-we-know-that-our-app-is-running-inside-of-app-.png)\n\n### Running Bash and SQLite 3\n\nRunning the `bash` command gives us access to everything Bash can do. We can also interact with the database by running the `sqlite3` command and pointing it to our database:\n\n```sh\nsqlite3 /data/sqlite.db\n```\n\nYou can view tables and execute database queries directly from there.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815844/epicweb.dev/tutorials/deploy-web-applications/images-07-ssh-sqlite-console.mp4/07-ssh-sqlite-console_16_01-24320-and-its-really-nice-to-be-able-to-do-that-.png)\n\n### Simplifying Database Access with a Dockerfile Modification\n\nIt's useful to run these commands, but it does introduce some overhead every time we want to run them.\n\nTo make database access easier, we can add a command to the Dockerfile.\n\nInside of the Dockerfile where the environment variables are set, add this command:\n\n```text\nRUN echo '#!/bin/sh\\nset -xe\\nsqlite3 \\$DATABASE_URL' > /usr/local/bin/database_cli && chmod +x /usr/local/bin/database_cli\n```\n\nThis command will create a new file inside `user/local/bin/database_cli`. This file will contain a script that runs SQLite 3 on the database URL, and makes sure the script is executable.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815846/epicweb.dev/tutorials/deploy-web-applications/images-07-ssh-sqlite-console.mp4/07-ssh-sqlite-console_27_02-23840-and-then-we-make-sure-that-that-is-executable.png)\n\nSave these changes, then push and redeploy the app.\n\n### Testing the SQLite CLI Shortcut\n\nAfter deploying the changes with `fly deploy`, you can use `fly ssh console -C database_cli` to jump straight into the SQLite CLI.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815851/epicweb.dev/tutorials/deploy-web-applications/images-07-ssh-sqlite-console.mp4/07-ssh-sqlite-console_33_02-58840-and-whatever-other-sql-commands-that-i-want-to.png)\n\nUse this technique with discretion, but it's a convenient way to access your database using the simple echo command in the Dockerfile.\n\nYou can also adapt this for any other commands that you want to easily execute within your Docker VM running on Fly.",
						"videoResource": {
							"muxAsset": {
								"muxPlaybackId": "zGIHmU8NdBarIgzvMYLykeH02dQNEIo5cn00501gt5PCnc",
								"muxAssetId": "yAycEze1bDh4aPZCCWghgZvF5VeeBPjPJJ7T4PxpzU8"
							},
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/07-ssh-sqlite-console.mp4",
							"_id": "23d8c675-98f4-4975-a1b5-141f33bf790d",
							"title": "07-ssh-sqlite-console",
							"castingwords": {
								"transcript": "Instructor: [0:00] With a long-running node server and a database, sometimes it could be really nice to be able to dive into that virtual machine, SSH into it, explore the file system, and look at the database. You can do that with Fly using the Fly SSH command. It will say fly SSH console to get a console into our running virtual machine.\r  \n\r  \n[0:23] Now that we're connected, we can ls -al and see here are all of the directories that we've got. Now, because of our dockerfile, we know that our app is running inside of app, and we also created the mounts config in our fly.toml so we know our database is inside of data. We could ls -al everything in app, and there's our application. List everything in data and there's our sqlite like database.\r  \n\r  \n[0:47] Things are looking great here. If we now run bash, now we have access to bash and all the cool things that bash can do because SSH is not quite enough. We also can run sqlite3 because we installed that as part of our dockerfile as well. We do an app to get to install sqlite3 and we can interact directly with our database, which is in data/sqlite.db\r  \n\r  \n[1:09] If we do tables, we can see here are all of the tables. We can select * from count, and there's our count just sitting in our database. We could do whatever database queries we want to directly in here.\r  \n\r  \n[1:22] It's really nice to be able to do that, but it's a little bit of work that I don't want to have to do every time I want to look into my database to have to first run the fly SSH console command and then do sqlite.db. I might forget where that is and all of that. We're actually going to do a little handy thing inside of our dockerfile to make this a bit easier.\r  \n\r  \n[1:46] In our dockerfile, right below where we set up these environment variables, I'm going to paste in this little command that basically creates a new file inside of usr/local/bin/database-cli. That file is going to contain a simple script that uses set -xe that'll make sure to print out what is being run and also make sure it has the proper exit code based on the command that's being run here.\r  \n\r  \n[2:13] Then it'll run sqlite3 across the database URL. That is the contents of the file that goes into usr/local/bin/database-cli. Then we make sure that is executable. With that now, we simply deploy this. Then we can run fly SSH console, give it the uppercase C flag for the first command that is executed.\r  \n\r  \n[2:35] Then we run it right into our database-cli. Let's get that deployed with fly deploy. With that deployed now, we can run fly SSH, console -C database-cli, and boom, here I am in the sqlite. I can do tables and select * from count and whatever other SQL commands that I want to execute against my production database.\r  \n\r  \n[3:01] Of course, use that with discretion, but it is really nice to be able to jump straight into that using this really simple echo command to create a new file right in our dockerfile. You could do this with any other sorts of commands that you want to be able to easily execute within your Docker VM running on fly. That's how you make sqlite -cli accessible via the fly SSH consol"
							},
							"_createdAt": "2023-04-07T15:14:06Z",
							"_type": "videoResource",
							"duration": 205.7,
							"_rev": "9CeTKuUcQZRsVUft8VmtWe",
							"transcript": {
								"text": "Instructor: [0:00] With a long-running node server and a database, sometimes it could be really nice to be able to dive into that virtual machine, SSH into it, explore the file system, and look at the database. You can do that with Fly using the Fly SSH command. It will say fly SSH console to get a console into our running virtual machine.\r  \n\r  \n[0:23] Now that we're connected, we can ls -al and see here are all of the directories that we've got. Now, because of our dockerfile, we know that our app is running inside of app, and we also created the mounts config in our fly.toml so we know our database is inside of data. We could ls -al everything in app, and there's our application. List everything in data and there's our sqlite like database.\r  \n\r  \n[0:47] Things are looking great here. If we now run bash, now we have access to bash and all the cool things that bash can do because SSH is not quite enough. We also can run sqlite3 because we installed that as part of our dockerfile as well. We do an app to get to install sqlite3 and we can interact directly with our database, which is in data/sqlite.db\r  \n\r  \n[1:09] If we do tables, we can see here are all of the tables. We can select * from count, and there's our count just sitting in our database. We could do whatever database queries we want to directly in here.\r  \n\r  \n[1:22] It's really nice to be able to do that, but it's a little bit of work that I don't want to have to do every time I want to look into my database to have to first run the fly SSH console command and then do sqlite.db. I might forget where that is and all of that. We're actually going to do a little handy thing inside of our dockerfile to make this a bit easier.\r  \n\r  \n[1:46] In our dockerfile, right below where we set up these environment variables, I'm going to paste in this little command that basically creates a new file inside of usr/local/bin/database-cli. That file is going to contain a simple script that uses set -xe that'll make sure to print out what is being run and also make sure it has the proper exit code based on the command that's being run here.\r  \n\r  \n[2:13] Then it'll run sqlite3 across the database URL. That is the contents of the file that goes into usr/local/bin/database-cli. Then we make sure that is executable. With that now, we simply deploy this. Then we can run fly SSH console, give it the uppercase C flag for the first command that is executed.\r  \n\r  \n[2:35] Then we run it right into our database-cli. Let's get that deployed with fly deploy. With that deployed now, we can run fly SSH, console -C database-cli, and boom, here I am in the sqlite. I can do tables and select * from count and whatever other SQL commands that I want to execute against my production database.\r  \n\r  \n[3:01] Of course, use that with discretion, but it is really nice to be able to jump straight into that using this really simple echo command to create a new file right in our dockerfile. You could do this with any other sorts of commands that you want to be able to easily execute within your Docker VM running on fly. That's how you make sqlite -cli accessible via the fly SSH consol",
								"srt": null
							},
							"_updatedAt": "2024-03-19T20:41:20Z"
						},
						"_id": "b1f6b39b-3597-4d21-8e6f-8822a33f5169",
						"_type": "lesson",
						"title": "Add a SQLite Console Shortcut with the Dockerfile",
						"description": "Fly allows you to SSH into your app's virtual machine and run commands. Updating the Dockerfile to create a shortcut script makes the process run more smoothly.",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"slug": "add-a-sqlite-console-shortcut-with-the-dockerfile",
						"solution": null
					},
					{
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Configure GitHub Actions for Automatic Deployment",
						"description": null,
						"body": "Instead of having to run `fly deploy` manually every time we make changes, we can set up automatic deployment using GitHub Actions.\n\n## Creating the GitHub Action Workflow\n\nFirst, we need to create the GitHub Action Workflow.\n\nCreate a new `.github/` directory, then inside of it create a `workflows` directory.\n\nInside of `.github/workflows `create a new `deploy.yml` file.\n\n### Configuring the `deploy.yaml`\n\nIn the `deploy.yaml`, we need to configure the following for the GitHub Action.\n\nThe action name will be `Deploy`, and it should only run on pushes to the main branch.\n\nUnder the `jobs` section we'll create a job called `deploy` that runs on `ubuntu-latest`. This job will have two steps: checking out the repo and then running the `deploy` command with Fly.\n\nHowever, the deployment step isn't as simple as just running `fly deploy`.\n\nFirst we'll have to install it, which we'll do with the `superfly/flyctl-actions` tool. This will allow us to run the deploy command with specific args of `--remote-only` so that the Fly builder runs it instead of the GitHub Action running it.\n\nWe'll also add an env variable for the Fly API Token. \n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815773/epicweb.dev/tutorials/deploy-web-applications/images-08-simple-github-deploy-action.mp4/08-simple-github-deploy-action_20_01-51840-that-would-not-be-a-good-thing.png)\n\nAll together, the `deploy.yaml` should look like this:\n\n```yaml\nname: 🚀 Deploy\non:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    name: 🚀 Deploy\n    runs-on: ubuntu-latest\n    steps:\n      - name: ⬇️ Checkout repo\n        uses: actions/checkout@v3\n\n      - name: 🚀 Deploy Production\n        uses: superfly/flyctl-actions@1.3\n        with:\n          args: \"deploy --remote-only\"\n        env:\n          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n```\n\n## Setting Up the Fly API Token\n\nIn order to authenticate with Fly.io from within the GitHub Action, we need to create an API token and provide it to the action.\n\nThe first step is to visit your Fly.io dashboard, then go to `Account > Access Tokens`.\n\nCreate a new token called \"GitHub Action\", then copy it to your clipboard.\n\n\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815775/epicweb.dev/tutorials/deploy-web-applications/images-08-simple-github-deploy-action.mp4/08-simple-github-deploy-action_22_02-08520-well-call-create--well-copy-that-.png)\n\nNext, go to your GitHub repository settings and go to `Secrets and Variables > Actions`. \n\nCreate a new repository secret called `FLY_API_TOKEN` to match the Action config, and paste in the copied token: \n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815850/epicweb.dev/tutorials/deploy-web-applications/images-08-simple-github-deploy-action.mp4/08-simple-github-deploy-action_25_02-33600-add-that-secret--and-now-were-ready-to-commit-this.png)\n\n## Committing and Pushing the Workflow\n\nOnce we have set up the `deploy.yaml` and added the API token to our repo secrets, we can commit and push the changes.\n\nAfter pushing the changes, when we go to the `Actions` tab of the repo we can see that Deploy is running:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815857/epicweb.dev/tutorials/deploy-web-applications/images-08-simple-github-deploy-action.mp4/08-simple-github-deploy-action_30_03-16200-and-now-its-going-to-push-that-image-up-to-fly-.png)\n\nOnce the action completes, we can check the Fly dashboard to ensure that the deployment was successful.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815778/epicweb.dev/tutorials/deploy-web-applications/images-08-simple-github-deploy-action.mp4/08-simple-github-deploy-action_32_03-30600-there-we-go--its-shutting-down-the-virtual-machine-.png)\n\nNow any time the main branch changes, the app will be automatically deployed to Fly.io!",
						"_id": "1d87df8a-e3f1-409b-a2f0-f0f6f6c75178",
						"_type": "lesson",
						"slug": "configure-github-actions-for-automatic-deployment",
						"videoResource": {
							"transcript": {
								"text": "Lecturer: [0:00] As much as I love typing fly deploy every time I want to deploy my app, I'd much rather have my main branch of my Git repo always be deployable. Anytime I merge anything into the main branch, it deploys automatically.  \n  \n[0:14] I've already got a GitHub repo here set up. All I need is a GitHub Action that runs the fly deploy command for me. Now, I don't have to worry about it anymore.  \n  \n[0:23] We're going to make that in a GitHub directory. Under that, we'll have a workflows directory. Under that, we'll have our deploy.yml. This will have our config for our action. We're going to call it deploy. We only want this to run on main branch pushes.  \n  \n[0:44] Then, for our job, we'll have a job called deploy. It runs on Ubuntu, and we have just two steps. All we have to do is check out the repo, and then run the deploy command with Fly.  \n  \n[0:58] Now, this one's a little bit different. It's not as simple as run, fly deploy for a couple of reasons. First of all, we need to get Fly installed. We get Fly installed using the superfly/flyctl-actions. That gets it installed for us, but this specifically will allow us to run a command with specific args.  \n  \n[1:20] The args we want to pass to Fly is deploy. We're also going to pass --remote-only. That way, we're going to use the Fly builder instead of running within the action itself to build the Dockerfile on everything. Fly will manage all of that for us. That's why we're specifying remote only.  \n  \n[1:38] The final thing is our Fly API token needs to be provided, because we're not authenticated on Fly within the context of this GitHub Action. If anybody could make a GitHub Action and deploy it to our app, that would not be a good thing. We need to have a Fly token. We need to have it accessible for our actions.  \n  \n[1:57] Let's go to our Fly dashboard. We'll go to account and access tokens. Then, we'll make one called GitHub Action. We'll call create, we'll copy that, and then we'll go to our GitHub repo. We'll go to the settings, and over to our secrets and variables. Under actions, make a new repository secret. We'll call this our FLY...  \n  \n[2:22] Actually, we need to make sure it's the same, the FLY_API_TOKEN. We'll paste in the secret there, add that secret. Now, we're ready to commit this. We'll git status. We added the GitHub directory, so we'll git add everything. Then, we'll run git commit, add deploy workflow. Now, we push.  \n  \n[2:49] With that, we can go over to our actions. In a moment, we should get a running deploy action. Now, we can go to that running action. We can look at the deploy. This should only take a little time here.  \n  \n[3:03] We get to the deploy-to-production piece. Obviously, we've been authenticated because it is running. It verifies our config. It's running with our Dockerfile. Now, it's going to push that image up to Fly. Then, Fly will trigger the release, which is happening right now. We can open up the logs, and take a look at how our application is doing.  \n  \n[3:26] There we go. It's shutting down the virtual machine. Now, it's starting our instance, pulling up our new instance of our application, and it is listening. It's already responding to health checks. Let's go to our app, refresh, and everything is running swimmingly.  \n  \n[3:44] All we needed to do was create a GitHub repo for our application. Create a workflow that has a simple deploy that all it does is check out the repo. It runs the deploy command with the superfly/flyctl-actions. We created our API token by going to our Fly dashboard, and going to account, access tokens, creating a new token.  \n  \n[4:09] Then, going to the settings of our GitHub repo. Going to secrets and variables, actions. Then, creating a new repository secret that has the same name as the name in our action right here. Then, we simply commit that workflow, push it up, and watch it fly.",
								"srt": null
							},
							"_createdAt": "2023-04-24T22:20:45Z",
							"title": "FIX 08 simple github deploy action video ",
							"_updatedAt": "2024-03-19T20:40:59Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/a07dc8fa-1cf4-4777-82de-613e31651f65/08-fixed-configuregithubactionsforautomaticdeployment-7oq3xxg4y.mp4",
							"duration": 266.833333,
							"_rev": "9CeTKuUcQZRsVUft8VmsbS",
							"muxAsset": {
								"muxPlaybackId": "nHOum8cvHUIyvfv7zhQuMEg2PdlkTlhvQ16CqjfBOSo",
								"muxAssetId": "ZnjephmA0099172EnXjxt7F3rbeW39TsqmimTuJFiXzM"
							},
							"slug": {
								"current": "fix-08-simple-github-deploy-action-video",
								"_type": "slug"
							},
							"_type": "videoResource",
							"_id": "1fa2b1e1-5b3a-4f3e-b344-742faa5107eb",
							"castingwords": {
								"transcript": "Lecturer: [0:00] As much as I love typing fly deploy every time I want to deploy my app, I'd much rather have my main branch of my Git repo always be deployable. Anytime I merge anything into the main branch, it deploys automatically.  \n  \n[0:14] I've already got a GitHub repo here set up. All I need is a GitHub Action that runs the fly deploy command for me. Now, I don't have to worry about it anymore.  \n  \n[0:23] We're going to make that in a GitHub directory. Under that, we'll have a workflows directory. Under that, we'll have our deploy.yml. This will have our config for our action. We're going to call it deploy. We only want this to run on main branch pushes.  \n  \n[0:44] Then, for our job, we'll have a job called deploy. It runs on Ubuntu, and we have just two steps. All we have to do is check out the repo, and then run the deploy command with Fly.  \n  \n[0:58] Now, this one's a little bit different. It's not as simple as run, fly deploy for a couple of reasons. First of all, we need to get Fly installed. We get Fly installed using the superfly/flyctl-actions. That gets it installed for us, but this specifically will allow us to run a command with specific args.  \n  \n[1:20] The args we want to pass to Fly is deploy. We're also going to pass --remote-only. That way, we're going to use the Fly builder instead of running within the action itself to build the Dockerfile on everything. Fly will manage all of that for us. That's why we're specifying remote only.  \n  \n[1:38] The final thing is our Fly API token needs to be provided, because we're not authenticated on Fly within the context of this GitHub Action. If anybody could make a GitHub Action and deploy it to our app, that would not be a good thing. We need to have a Fly token. We need to have it accessible for our actions.  \n  \n[1:57] Let's go to our Fly dashboard. We'll go to account and access tokens. Then, we'll make one called GitHub Action. We'll call create, we'll copy that, and then we'll go to our GitHub repo. We'll go to the settings, and over to our secrets and variables. Under actions, make a new repository secret. We'll call this our FLY...  \n  \n[2:22] Actually, we need to make sure it's the same, the FLY_API_TOKEN. We'll paste in the secret there, add that secret. Now, we're ready to commit this. We'll git status. We added the GitHub directory, so we'll git add everything. Then, we'll run git commit, add deploy workflow. Now, we push.  \n  \n[2:49] With that, we can go over to our actions. In a moment, we should get a running deploy action. Now, we can go to that running action. We can look at the deploy. This should only take a little time here.  \n  \n[3:03] We get to the deploy-to-production piece. Obviously, we've been authenticated because it is running. It verifies our config. It's running with our Dockerfile. Now, it's going to push that image up to Fly. Then, Fly will trigger the release, which is happening right now. We can open up the logs, and take a look at how our application is doing.  \n  \n[3:26] There we go. It's shutting down the virtual machine. Now, it's starting our instance, pulling up our new instance of our application, and it is listening. It's already responding to health checks. Let's go to our app, refresh, and everything is running swimmingly.  \n  \n[3:44] All we needed to do was create a GitHub repo for our application. Create a workflow that has a simple deploy that all it does is check out the repo. It runs the deploy command with the superfly/flyctl-actions. We created our API token by going to our Fly dashboard, and going to account, access tokens, creating a new token.  \n  \n[4:09] Then, going to the settings of our GitHub repo. Going to secrets and variables, actions. Then, creating a new repository secret that has the same name as the name in our action right here. Then, we simply commit that workflow, push it up, and watch it fly."
							}
						},
						"solution": null
					},
					{
						"_id": "d35563df-6f00-476d-8d0f-9f340be6e656",
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"videoResource": {
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/09-staging-envrionment.mp4",
							"_createdAt": "2023-04-07T15:17:38Z",
							"transcript": {
								"text": "Instructor: [0:00] Every commit that goes into main is going to go to production. What if I wanted to have an environment that's a special, really close-to-production-like environment, that I can test changes out to before going to production?  \n  \n[0:11] This is common for teams, and it's normally called a staging environment. Often they'll just make a dev branch and everything that goes to the dev branch goes into the staging environment where you can play around with and test things out.  \n  \n[0:23] We're going to make that work now. As soon as I created this dev config on our branches, I'm automatically sending things to the production environment because of the way that things are configured down here. I don't want to do that, so we'll make sure to handle that but before we do, we need to create that staging environment.  \n  \n[0:41] Let's say, fly apps create. The staging environment, I want to have the same name as my app just with a -staging at the end. We'll say fly apps create, that app-staging. Then we also need a persistent volume for our staging environment for the SQL-like database.  \n  \n[0:58] We'll say fly vol create data with the app as our staging environment, and the size as 1. Then we'll choose the region that's closest to me. You choose the region that's closest to you or whatever region you prefer. Now we're ready to deploy to this environment.  \n  \n[1:18] We'll come back here and we'll add a bit of config to this Deploy Production to make sure that we only deploy the main branch to production. We'll say, if the github.ref is refs/heads/main.  \n  \n[1:34] Now, this step will only execute if our ref is refs/heads/main, which is the main branch. Now, we can copy this and make a staging version of this. If the refs/heads is dev, now this one will run.  \n  \n[1:51] Now it's a little bit tricky here because this deploy command is going to look up our app name inside of the fly.toml. We don't want it to use that one. That's the production one, so we're going to add an app flag here with our staging app name.  \n  \n[2:04] Now, right here, I get a little bit concerned because I don't like having the configuration duplicated between two different places. Like those two strings are really important and close, and they're supposed to be that way. I'd rather have that be encoded in the way that things are written.  \n  \n[2:23] Instead of hard-coding this, I'm going to use another action step that will read the app name, from our fly.toml. This is the SebRollen/toml-action, which has an ID of app_name that I specify.  \n  \n[2:35] Then we say we want them to read the fly.toml and get us the field app and it will expose that to us. Instead of hard-coding this we can use this as a variable, and the variable name is going to be steps.app_name -- that's referencing that ID there -- .outputs.value. That's going to get the value of the app property in the fly.toml.  \n  \n[3:01] Now I don't have to duplicate that app name and I can use this in other apps that I deploy on Fly as well, which is quite nice. With all of that set up, we can now say git commit add staging environment. We'll push that, and again, we're pushing to main.  \n  \n[3:19] This is going to actually deploy to our production environment, but we can look at that action as it comes in and ensure that indeed only the production step is the one that gets run here and the staging step gets skipped.  \n  \n[3:35] We'll just let that run, and here we can see the deploy staging step got skipped, but the deploy production step is running just as we would expect.  \n  \n[3:44] Now, let's just test this out really quick. We'll say Demo App staging, in parentheses, and then we'll make a branch called dev. We'll git commit staging and push that dev branch to GitHub. We'll look at our actions again, and we see that staging commit in dev has kicked off a new workflow run.  \n  \n[4:08] This one, hopefully, will just run the staging and will not run the production step so that we deploy to staging instead of production. Indeed it is running the staging step.  \n  \n[4:20] Now it's releasing our v2. We can look at our logs here, pop that open on our brand-new staging environment, which is currently setting up the volume data. It's executed our migrations because it just barely created this new database. Now we're all set. It's already starting to give us logs for those health checks, so we can go to overview and look at this.  \n  \n[4:50] Hostname, open that up. Our staging environment is looking exactly the way it should. It has our changes, and our production app does not have those changes, so we can safely experiment in the staging environment without worrying about the production environment getting affected.  \n  \n[5:05] Inner view, what we did to make this work is we ran the fly apps create command to create our staging environment. Then we ran fly vol create to create the persistent volume for our staging environment.  \n  \n[5:18] Then we updated our deploy.yml so that we would only deploy to production if we're on the main branch and we added a staging step that deploys if we're on the dev branch, and then we also read the app name from the fly.toml so we could use that as part of the deploy command of our staging step.  \n  \n[5:37] Then we pushed our changes, made some changes specifically for our dev branch, and pushed the dev branch. That's how you get a staging environment automatically deployed with Fly.",
								"srt": null
							},
							"castingwords": {
								"transcript": "Instructor: [0:00] Every commit that goes into main is going to go to production. What if I wanted to have an environment that's a special, really close-to-production-like environment, that I can test changes out to before going to production?  \n  \n[0:11] This is common for teams, and it's normally called a staging environment. Often they'll just make a dev branch and everything that goes to the dev branch goes into the staging environment where you can play around with and test things out.  \n  \n[0:23] We're going to make that work now. As soon as I created this dev config on our branches, I'm automatically sending things to the production environment because of the way that things are configured down here. I don't want to do that, so we'll make sure to handle that but before we do, we need to create that staging environment.  \n  \n[0:41] Let's say, fly apps create. The staging environment, I want to have the same name as my app just with a -staging at the end. We'll say fly apps create, that app-staging. Then we also need a persistent volume for our staging environment for the SQL-like database.  \n  \n[0:58] We'll say fly vol create data with the app as our staging environment, and the size as 1. Then we'll choose the region that's closest to me. You choose the region that's closest to you or whatever region you prefer. Now we're ready to deploy to this environment.  \n  \n[1:18] We'll come back here and we'll add a bit of config to this Deploy Production to make sure that we only deploy the main branch to production. We'll say, if the github.ref is refs/heads/main.  \n  \n[1:34] Now, this step will only execute if our ref is refs/heads/main, which is the main branch. Now, we can copy this and make a staging version of this. If the refs/heads is dev, now this one will run.  \n  \n[1:51] Now it's a little bit tricky here because this deploy command is going to look up our app name inside of the fly.toml. We don't want it to use that one. That's the production one, so we're going to add an app flag here with our staging app name.  \n  \n[2:04] Now, right here, I get a little bit concerned because I don't like having the configuration duplicated between two different places. Like those two strings are really important and close, and they're supposed to be that way. I'd rather have that be encoded in the way that things are written.  \n  \n[2:23] Instead of hard-coding this, I'm going to use another action step that will read the app name, from our fly.toml. This is the SebRollen/toml-action, which has an ID of app_name that I specify.  \n  \n[2:35] Then we say we want them to read the fly.toml and get us the field app and it will expose that to us. Instead of hard-coding this we can use this as a variable, and the variable name is going to be steps.app_name -- that's referencing that ID there -- .outputs.value. That's going to get the value of the app property in the fly.toml.  \n  \n[3:01] Now I don't have to duplicate that app name and I can use this in other apps that I deploy on Fly as well, which is quite nice. With all of that set up, we can now say git commit add staging environment. We'll push that, and again, we're pushing to main.  \n  \n[3:19] This is going to actually deploy to our production environment, but we can look at that action as it comes in and ensure that indeed only the production step is the one that gets run here and the staging step gets skipped.  \n  \n[3:35] We'll just let that run, and here we can see the deploy staging step got skipped, but the deploy production step is running just as we would expect.  \n  \n[3:44] Now, let's just test this out really quick. We'll say Demo App staging, in parentheses, and then we'll make a branch called dev. We'll git commit staging and push that dev branch to GitHub. We'll look at our actions again, and we see that staging commit in dev has kicked off a new workflow run.  \n  \n[4:08] This one, hopefully, will just run the staging and will not run the production step so that we deploy to staging instead of production. Indeed it is running the staging step.  \n  \n[4:20] Now it's releasing our v2. We can look at our logs here, pop that open on our brand-new staging environment, which is currently setting up the volume data. It's executed our migrations because it just barely created this new database. Now we're all set. It's already starting to give us logs for those health checks, so we can go to overview and look at this.  \n  \n[4:50] Hostname, open that up. Our staging environment is looking exactly the way it should. It has our changes, and our production app does not have those changes, so we can safely experiment in the staging environment without worrying about the production environment getting affected.  \n  \n[5:05] Inner view, what we did to make this work is we ran the fly apps create command to create our staging environment. Then we ran fly vol create to create the persistent volume for our staging environment.  \n  \n[5:18] Then we updated our deploy.yml so that we would only deploy to production if we're on the main branch and we added a staging step that deploys if we're on the dev branch, and then we also read the app name from the fly.toml so we could use that as part of the deploy command of our staging step.  \n  \n[5:37] Then we pushed our changes, made some changes specifically for our dev branch, and pushed the dev branch. That's how you get a staging environment automatically deployed with Fly."
							},
							"duration": 349.866667,
							"_type": "videoResource",
							"title": "09-staging-envrionment",
							"_updatedAt": "2024-03-19T20:41:16Z",
							"_rev": "6LaeqP6n94P8FD3sVhYln3",
							"_id": "729528c6-eb4f-46dc-82ba-4eb19f3e3eb0",
							"muxAsset": {
								"muxAssetId": "kKWH57GvQqz9Z9dP6AkhAngQXxr6qPFuOjYS7vf00jIQ",
								"muxPlaybackId": "8MXIOzPjTfeLIBojM7f8YbQKkkqBnbp2P9MEmHxIS6A"
							}
						},
						"solution": null,
						"title": "Add a Staging Environment",
						"description": "A staging environment is useful for play testing before pushing changes to production. This can be quickly set up with GitHub Actions and Fly",
						"body": "As of now, every commit that goes into main is going to go to production.\n\nHowever, it's nice to be able to play around in a staging environment based on a dev branch.\n\nThe first step is to add an entry for the `dev` branch inside of `deploy.yml`\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815779/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_9_00-35080-because-of-the-way-that-things-are-configured-down-here.png)\n\nThere will be more changes inside of `deploy.yml` later.\n\n## Creating a Staging Environment\n\nWe need to create a new Fly app for the staging environment.\n\nUse the same app name with `-staging` appended to the end:\n\n```markdown\nfly apps create weathered-morning-92-staging\n```\n\nNext we'll create a persistent volume for the SQLite database in the staging environment. We'll use a size of 1, then choose your region.\n\n```markdown\nfly vol create data --app [your-app-name]-staging --size 1\n```\n\n## Updating the GitHub Actions Workflow\n\nNow we're ready to update the Deploy Production step to run only if the branch being pushed is `main`.\n\nTo do this we'll add an \"if\" line that checks the ref:\n\n```yaml\nif: ${{ github.ref == 'refs/heads/main' }}\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815782/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_21_01-38320-so-now--this-step-will-only-execute.png)\n\nTo create the staging version, we'll copy and paste the production version and make some changes so it runs when the `dev` branch is pushed to.\n\nAdd the `--app` flag with the staging app name:\n\n```yaml\nif: github.ref == 'refs/heads/dev'\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815784/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_26_01-58400-to-look-up-our-app-name-inside-of-the-fly-toml.png)\n\nThe tricky part now is that it's going to look up our app name from `fly.toml`, but we don't want to use our production app's name.\n\nTo avoid duplicating the app name, we will use the `SebRollen/toml-action@v1.0.2` action.\n\nThis will allow us to set our app name as a variable called `steps.app_name`.\n\nThen we can update the deploy command to get the output value from the `fly.toml` file and add \"-staging\" to the end.\n\n```yaml\ndeploy --remote-only ${{ steps.app_name.outputs.value }}-staging\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815783/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_42_03-00840-and-thats-going-to-get-the-value-of-the-app-property.png)\n\n## Commit the Changes\n\nNow we'll commit and push these changes to the main branch, which will deploy to the production environment.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815778/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_46_03-18440-well-push-that.png)\n\nWe should verify that only the deploy production step runs, and the staging step is skipped.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815778/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_52_03-41520-and-here-we-can-see-the-deploy-staging-step-got-skipped-.png)\n\n## Test the Staging Environment\n\nIn order to test the staging environment, create a new `dev` branch, then make a change like adding \"Staging\" to the demo app's title.\n\nAfter committing and pushing the dev branch to GitHub, we can check the GitHub Actions workflow and ensure that only the deploy staging step runs.\n\nThis is also confirmed by the Overview tab in the Fly.io dashboard:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815785/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_63_04-52640-open-that-up.png)\n\nAnd our staging app domain shows our updated title as well:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815780/epicweb.dev/tutorials/deploy-web-applications/images-09-staging-envrionment.mp4/09-staging-envrionment_64_04-55360-and-our-staging-environment-is-looking-exactly-the-way.png)\n\nFollow this process to create a staging deployment for any of your future apps!",
						"slug": "add-a-staging-environment"
					}
				],
				"resources": [],
				"_id": "c4e5787b-9630-4be5-a7e0-f7321dcdf7af",
				"_type": "section",
				"_updatedAt": "2023-04-10T21:23:43Z",
				"title": "Persisting Data and Automatic Deployment",
				"description": null,
				"slug": "persisting-data-and-automatic-deployment"
			},
			{
				"slug": "multi-region-data-and-deployment",
				"lessons": [
					{
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Prepare for Multi-Region Data with LiteFS",
						"body": "Fly.io has built-in support for distributed Postgres and Redis, but by using LiteFS we'll be able to use SQLite as well.\n\nLiteFS depends on the Fuse package, which acts as a virtual file system.\n\nThis means that our SQLite database will be moved into this virtual file system, which is how our application will access it. Behind the scenes, Fuse and LiteFS will write the actual database to a separate directory. LiteFS will intercept the reads and writes that happen in the virtual file system, and propagate changes to read replicas all over the world.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815792/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_10_01-16200-move-our-sqlite-database-into-this-virtual-file-system.png)\n\n## Adding Fuse to Our App\n\nIn our Dockerfile, we need to add Fuse as dependency by adding it to the `apt-get` line:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815795/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_14_01-47280-the-next-thing-that-it-wants-us-to-do.png)\n\nWe'll copy over the LiteFS binary, then create and add a litefs.yml file to our Dockerfile. This config file specifies the virtual file system directory and data directory.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680882321/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/mpv-shot0003.jpg)\n\nInside of the `litefs.yml` file, we add entries for a `fuse` directory and a `data` directory.\n\nIn order to avoid duplicating configuration across files, we can create a `LITEFS_DIR` environment variable in the Dockerfile:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815797/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_27_03-00720-to-access-our-data--this-database-url.png)\n\nThen in the `litefs.yml` file we can point the `fuse` directory to our `LITEFS_DIR` variable and put our actual data at `/data/litefs`:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815787/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_37_03-58720-so-this-is-where-the-actual-database-will-exist.png)\n\n## Lease Configuration\n\nNow that Fuse and LiteFS are installed and configured, the last thing we need to do lease configuration for LiteFS. This is important because it determines the primary region in a distributed architecture. The primary region will be the only one that can write to the database– all other regions are just read replicas.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815786/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_42_04-32520-and-when-that-region-writes-to-the-database-.png)\n\nFor now we'll use a static lease configuration just so we can make sure LiteFS is set up correctly.\n\n## Updating the Start Script\n\nInside the Dockerfile we need to update the command to mount LiteFS before running the `npm start` command:\n\n```markdown\nCMD [\"litefs\", \"mount\", \"--\", \"npm\", \"start.js\"]\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815789/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_55_05-46520-and-with-that-all-configured--we-can-deploy-this-and-run.png)\n\nNow we will commit our changes and push them to kick off a new deployment.\n\nIn the Fly Dashboard's logs we can see that LiteFS mounted and we have a static primary lease that will be ready to connect to clusters for read replicas.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815789/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_72_06-54800-so-we-specified-we-only-have-one-static-primary--and-this-is-it.png)\n\nOur demo app has been redeployed, though we did clear our existing data in this process.\n\nIf you want to migrate to this architecture from an existing database, you'll want to make sure you make a backup of your existing database and import the old data into LiteFS.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815796/epicweb.dev/tutorials/deploy-web-applications/images-10-init-litefs.mp4/10-init-litefs_82_08-13440-in-our-view--the-way-that-we-made-this-work.png)",
						"slug": "prepare-for-multi-region-data-with-litefs",
						"solution": null,
						"_id": "78fc007c-dcf7-4828-8fac-252fef956b5b",
						"videoResource": {
							"slug": {
								"current": "fix-10-prepare-litefs",
								"_type": "slug"
							},
							"transcript": {
								"text": "Lecturer: [0:00] Our application is humming along nicely, but we have one problem.  \n  \n[0:04] Let's say that we have users who are all over the world. How do we decide where we deploy our application based on our users? Do we think, \"Let's look at the analytics, how many users are there in certain parts of the world? We'll put the application wherever the most users are\"?  \n  \n[0:20] That is a strategy that works out pretty well for some use cases. If we want to maximize the performance of our application, then co-locating our application in all the different regions throughout the world, we'll do some horizontal scaling, and put it -- the different nodes of our application -- all throughout the world.  \n  \n[0:36] That would be a lot better, except for the data. If we don't also do the same to our data, then our applications have to reach all the way around the world to get the data as well. It's not a whole lot better. It is a little bit better, but not a whole lot.  \n  \n[0:50] We have to distribute our data. Once we start distributing our data, now, we have conflict resolution. What happens if the user up here is updating data, and the user up here is updating data? Which one wins? It can be a really complicated problem. However, there are some good solutions to this. These solutions are built-in to Fly.  \n  \n[1:09] With Fly, you can have PostgreSQL clusters. You get automatic read replicas. The idea is you have one node that is the primary node. It's the only one that is able to write to the database. All the other ones can get their own database, but they can only read from it. Any writes have to be sent to the primary node.  \n  \n[1:27] Fly has a great built-in way to accomplish this with very little impact on your application code. Luckily, for us, because we're using SQLite, Fly also has a solution for this with LiteFS.  \n  \n[1:40] LiteFS basically acts as a virtual file system. Remember, SQLite is just a file in the file system. What LiteFS does is we have this virtual file system that proxies any reads and writes. So that we can make sure that any writes go to the proper region, and get propagated to the proper spot.  \n  \n[2:02] We're going to set up LiteFS in our application. Then, eventually, we'll get to multiple regions. To get started, we need to have fuse3 installed. We are using our Dockerfile with apt-get. We'll add a fuse3 in our apt-get requirements there.  \n  \n[2:17] Then, because we're running with Docker, we can use the LiteFS distribution on Docker Hub to get the official Docker image. We're using the latest version that is in here. Use whatever the latest version is at the time that you're following along if you are following along.  \n  \n[2:31] We'll stick that toward the bottom of our Dockerfile. We need the LiteFS binary right when we start our application. We'll bring that in, and then the next thing that we need is configuration.  \n  \n[2:44] The configuration default location is in /etc/litefs.yml. We'll put our own YML configuration in the root of our project, and then we'll add it. We'll say add litefs.yml to that location where it's expecting it. Let's make that litefs.yml right here. We'll copy this configuration over here. Let's talk about this a little bit.  \n  \n[3:07] Fuse is the library that's responsible for creating the virtual file system. That's the thing that we installed right here. It's going to create the virtual file system for us so that LiteFS can intercept to all of the writes so that we get that propagation of all of those database writes.  \n  \n[3:26] The directory where we want this virtual file system to be is where our application should be accessing our database. The application should not access the actual data anymore. It should access it through the proxy so that we make sure that we're doing all the writes, and propagating those.  \n  \n[3:45] When we specify the DIR, this is saying, \"Hey, fuse. This is the directory I want the virtual file system be set up in.\" Because we've set LiteFS here, we need to update our database URL to use LiteFS instead.  \n  \n[3:59] Now, I don't like this configuration having to be the same, and being in two different places, I think that'd be easy to mess up. Like, I just decide I want it to be Lite-FS, I could break things there.  \n  \n[4:10] What we're going to do instead is we'll make an env called LITEFS_DIR. This will be...I want to spell it right, though. Otherwise, that'll be confusing. We'll say LiteFS, and we'll stick LITEFS_DIR right there. That'll be interpolated. Then, over here, we can interpolate this as well, so LITEFS_DIR. That configuration will be interpolated for our fuse configuration.  \n  \n[4:35] Now, the directory for the data is where the actual data is going to lease. We do need to have the underlying SQLite database. It needs to be somewhere. This is where LiteFS is going to put that SQLite database. Currently, our SQLite database, before we made this change, was in /data/sqlite.db.  \n  \n[4:58] We could put it in the same place. It's possible you may want to have it in a very specific place. For us, we're going to create a brand-new location, so we don't have any problems when we're deploying this for the first time. Then, you can do an import later if you want to import existing data.  \n  \n[5:16] I should also note that I'm going to be pushing this directly to our main branch, but we already set up the staging environment. If you wanted to test this out in staging, that'd probably be a good idea. Especially, if this is a long-running application that you've got existing data and existing users, and stuff.  \n  \n[5:31] For us, we have mounted our data persistence -- our persistent volume that we're calling data -- to /data. That is where our data needs to live inside of LiteFS. Where LiteFS creates a SQLite database and performs actual writes to, that needs to be under our data directory.  \n  \n[5:53] We're going to call this, simply, data. We're going to put our SQLite database under the LiteFS directory to communicate, \"Hey, LiteFS is the one that's writing to this actual file here.\"  \n  \n[6:04] With that, we have one other thing that we need to do in our LiteFS config and another thing to do in our Dockerfile. Let's come over here, and let's talk about lease configuration.  \n  \n[6:15] The idea is that we only have one region or one instance of our application that is allowed to write to its database. The rest are read replicas. This works well for most applications and allows us to co-locate our data as close to the users as possible.  \n  \n[6:34] The challenge here is, how do you decide which region get, or which instance of your application gets to be the primary instance? The one that actually gets to write to its database.  \n  \n[6:44] There are a couple of strategies that you can apply here. We have static and consul. We're going to start with static because it's a lot simpler. Then, we'll bring in consul because it has some nice capabilities we'll talk about later.  \n  \n[6:56] For the static, we basically are saying, \"Hey, this one, 100 percent, is going to be the primary node.\" Because we're only deploying to one region, we don't have to do anything special with the candidate here. We can hard code it to say true.  \n  \n[7:09] That will mean that the instance that starts up here can assume that it is the primary, and it can write to its database. We don't have to worry about synchronizing writes, or anything. That's not a thing in LiteFS here. Our type is static. We are the candidate for being the primary node.  \n  \n[7:26] Then, in our Dockerfile, we need to make sure that the virtual file system is up and ready, and the SQLite database is accessible through that before we start our application. To do that, we're going to run our application under the LiteFS binary, which we installed right here.  \n  \n[7:44] There are instructions for doing this. There are a couple of ways you can do it. You can simply run LiteFS mount, and then configure the exec property in your LiteFS config. You can run LiteFS mount with a double hyphen here, and then whatever you want it to exec.  \n  \n[8:01] In our case, we're going to go with this approach here. LiteFS mount, and then the double dash. Then, npm-start is what we want it to execute. With that, we are ready to rock and roll.  \n  \n[8:14] Let's git status, see what we changed. We modified the Dockerfile, added a new file, so git add all the things. Then, we'll git commit everything with the commit, add LiteFS. Now, let's push that, and come over to our actions.  \n  \n[8:34] Here, we've got our add LiteFS action. That is going to deploy our application. Once that's done, we're going to get some interesting logs that I want to talk about. We'll wait for that to show up.  \n  \n[8:46] The logs are saying it's shutting down the virtual machine. We're closing down the old app, starting up the new one. We're pulling the container image, unpacking it, setting up all the firecracker stuff in here. There are a couple of logs in here I want to focus on as this application gets started up.  \n  \n[9:02] The part where we start seeing some interesting stuff is right here where we're determining the primary region. Here, we're using static primary. That's because we configured our lease configuration to be static. We are the primary.  \n  \n[9:19] We're reading the config file from /app/litefs. The LiteFS version is printed out here. We've got our primary lease acquired, because we're the only instance that can be the primary instance. The /litefs is where our LiteFS directory is going to be mounted to.  \n  \n[9:36] We've got a couple of other logs here related to determining the primary node and the cluster. Then, we're starting the subprocess. This is where our code starts. Here, we're running node start.js, which loads our Prisma schema.  \n  \n[9:51] It identifies the SQLite database under file /litefs/sqlite.db. Then, the database file is zero length on initialization. We didn't have a database yet, because this is a brand-new location. The SQLite database, sqlite.db, is created at file /litefs/sqlite.db.  \n  \n[10:13] It found one migration because, again, this is a brand-new database. We're not using the existing one. We're applying this migration. Then, it's telling us it applied this migration. These logs are all coming from Prisma. Then, we get to start our app. Our server is listening, and health checks are passing.  \n  \n[10:32] In review, what we did to make all of this work was we updated our Dockerfile to include fuse3, so we can have that virtual file system all up and loaded. That's a dependency of LiteFS.  \n  \n[10:45] Then, we're copying the LiteFS binary from the latest version, adding our LiteFS config. Then, running our command, LiteFS mount, so that LiteFS can get all set up before our actual application starts.  \n  \n[10:57] Then, our LiteFS configuration specifies the directory to be LITEFS_DIR, which was configured in our Dockerfile right here so that those could be shared for both our database URL as well as our LiteFS configuration.  \n  \n[11:12] Then, the data is pointing to the /data, which is the destination for our persistent volume called data. We're putting this inside of the LiteFS directory. Then, we specify the lease type of static to make things as easy as possible. That gets our application running within LiteFS.",
								"srt": null
							},
							"_type": "videoResource",
							"muxAsset": {
								"muxPlaybackId": "nozK802Fo02vN5QLhhJShy52JcIRa1SKA7eWm3Catc324",
								"muxAssetId": "sR0101ok4K6dmBVKAs5Brtqzh8qdQd6hzzni8cCJqy9500"
							},
							"_rev": "6LaeqP6n94P8FD3sVhYlQz",
							"_id": "857ea2cc-94a5-48e4-ab73-270877edb326",
							"_updatedAt": "2024-03-19T20:40:57Z",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/1ae858d6-0090-48e0-bcaf-c2a2c23309e4/10-fixed-prepareformulti-regiondatawithlitefs-qfwo8hjjd.mp4",
							"duration": 694.633333,
							"_createdAt": "2023-04-24T22:24:44Z",
							"title": "FIX 10 prepare litefs",
							"castingwords": {
								"transcript": "Lecturer: [0:00] Our application is humming along nicely, but we have one problem.  \n  \n[0:04] Let's say that we have users who are all over the world. How do we decide where we deploy our application based on our users? Do we think, \"Let's look at the analytics, how many users are there in certain parts of the world? We'll put the application wherever the most users are\"?  \n  \n[0:20] That is a strategy that works out pretty well for some use cases. If we want to maximize the performance of our application, then co-locating our application in all the different regions throughout the world, we'll do some horizontal scaling, and put it -- the different nodes of our application -- all throughout the world.  \n  \n[0:36] That would be a lot better, except for the data. If we don't also do the same to our data, then our applications have to reach all the way around the world to get the data as well. It's not a whole lot better. It is a little bit better, but not a whole lot.  \n  \n[0:50] We have to distribute our data. Once we start distributing our data, now, we have conflict resolution. What happens if the user up here is updating data, and the user up here is updating data? Which one wins? It can be a really complicated problem. However, there are some good solutions to this. These solutions are built-in to Fly.  \n  \n[1:09] With Fly, you can have PostgreSQL clusters. You get automatic read replicas. The idea is you have one node that is the primary node. It's the only one that is able to write to the database. All the other ones can get their own database, but they can only read from it. Any writes have to be sent to the primary node.  \n  \n[1:27] Fly has a great built-in way to accomplish this with very little impact on your application code. Luckily, for us, because we're using SQLite, Fly also has a solution for this with LiteFS.  \n  \n[1:40] LiteFS basically acts as a virtual file system. Remember, SQLite is just a file in the file system. What LiteFS does is we have this virtual file system that proxies any reads and writes. So that we can make sure that any writes go to the proper region, and get propagated to the proper spot.  \n  \n[2:02] We're going to set up LiteFS in our application. Then, eventually, we'll get to multiple regions. To get started, we need to have fuse3 installed. We are using our Dockerfile with apt-get. We'll add a fuse3 in our apt-get requirements there.  \n  \n[2:17] Then, because we're running with Docker, we can use the LiteFS distribution on Docker Hub to get the official Docker image. We're using the latest version that is in here. Use whatever the latest version is at the time that you're following along if you are following along.  \n  \n[2:31] We'll stick that toward the bottom of our Dockerfile. We need the LiteFS binary right when we start our application. We'll bring that in, and then the next thing that we need is configuration.  \n  \n[2:44] The configuration default location is in /etc/litefs.yml. We'll put our own YML configuration in the root of our project, and then we'll add it. We'll say add litefs.yml to that location where it's expecting it. Let's make that litefs.yml right here. We'll copy this configuration over here. Let's talk about this a little bit.  \n  \n[3:07] Fuse is the library that's responsible for creating the virtual file system. That's the thing that we installed right here. It's going to create the virtual file system for us so that LiteFS can intercept to all of the writes so that we get that propagation of all of those database writes.  \n  \n[3:26] The directory where we want this virtual file system to be is where our application should be accessing our database. The application should not access the actual data anymore. It should access it through the proxy so that we make sure that we're doing all the writes, and propagating those.  \n  \n[3:45] When we specify the DIR, this is saying, \"Hey, fuse. This is the directory I want the virtual file system be set up in.\" Because we've set LiteFS here, we need to update our database URL to use LiteFS instead.  \n  \n[3:59] Now, I don't like this configuration having to be the same, and being in two different places, I think that'd be easy to mess up. Like, I just decide I want it to be Lite-FS, I could break things there.  \n  \n[4:10] What we're going to do instead is we'll make an env called LITEFS_DIR. This will be...I want to spell it right, though. Otherwise, that'll be confusing. We'll say LiteFS, and we'll stick LITEFS_DIR right there. That'll be interpolated. Then, over here, we can interpolate this as well, so LITEFS_DIR. That configuration will be interpolated for our fuse configuration.  \n  \n[4:35] Now, the directory for the data is where the actual data is going to lease. We do need to have the underlying SQLite database. It needs to be somewhere. This is where LiteFS is going to put that SQLite database. Currently, our SQLite database, before we made this change, was in /data/sqlite.db.  \n  \n[4:58] We could put it in the same place. It's possible you may want to have it in a very specific place. For us, we're going to create a brand-new location, so we don't have any problems when we're deploying this for the first time. Then, you can do an import later if you want to import existing data.  \n  \n[5:16] I should also note that I'm going to be pushing this directly to our main branch, but we already set up the staging environment. If you wanted to test this out in staging, that'd probably be a good idea. Especially, if this is a long-running application that you've got existing data and existing users, and stuff.  \n  \n[5:31] For us, we have mounted our data persistence -- our persistent volume that we're calling data -- to /data. That is where our data needs to live inside of LiteFS. Where LiteFS creates a SQLite database and performs actual writes to, that needs to be under our data directory.  \n  \n[5:53] We're going to call this, simply, data. We're going to put our SQLite database under the LiteFS directory to communicate, \"Hey, LiteFS is the one that's writing to this actual file here.\"  \n  \n[6:04] With that, we have one other thing that we need to do in our LiteFS config and another thing to do in our Dockerfile. Let's come over here, and let's talk about lease configuration.  \n  \n[6:15] The idea is that we only have one region or one instance of our application that is allowed to write to its database. The rest are read replicas. This works well for most applications and allows us to co-locate our data as close to the users as possible.  \n  \n[6:34] The challenge here is, how do you decide which region get, or which instance of your application gets to be the primary instance? The one that actually gets to write to its database.  \n  \n[6:44] There are a couple of strategies that you can apply here. We have static and consul. We're going to start with static because it's a lot simpler. Then, we'll bring in consul because it has some nice capabilities we'll talk about later.  \n  \n[6:56] For the static, we basically are saying, \"Hey, this one, 100 percent, is going to be the primary node.\" Because we're only deploying to one region, we don't have to do anything special with the candidate here. We can hard code it to say true.  \n  \n[7:09] That will mean that the instance that starts up here can assume that it is the primary, and it can write to its database. We don't have to worry about synchronizing writes, or anything. That's not a thing in LiteFS here. Our type is static. We are the candidate for being the primary node.  \n  \n[7:26] Then, in our Dockerfile, we need to make sure that the virtual file system is up and ready, and the SQLite database is accessible through that before we start our application. To do that, we're going to run our application under the LiteFS binary, which we installed right here.  \n  \n[7:44] There are instructions for doing this. There are a couple of ways you can do it. You can simply run LiteFS mount, and then configure the exec property in your LiteFS config. You can run LiteFS mount with a double hyphen here, and then whatever you want it to exec.  \n  \n[8:01] In our case, we're going to go with this approach here. LiteFS mount, and then the double dash. Then, npm-start is what we want it to execute. With that, we are ready to rock and roll.  \n  \n[8:14] Let's git status, see what we changed. We modified the Dockerfile, added a new file, so git add all the things. Then, we'll git commit everything with the commit, add LiteFS. Now, let's push that, and come over to our actions.  \n  \n[8:34] Here, we've got our add LiteFS action. That is going to deploy our application. Once that's done, we're going to get some interesting logs that I want to talk about. We'll wait for that to show up.  \n  \n[8:46] The logs are saying it's shutting down the virtual machine. We're closing down the old app, starting up the new one. We're pulling the container image, unpacking it, setting up all the firecracker stuff in here. There are a couple of logs in here I want to focus on as this application gets started up.  \n  \n[9:02] The part where we start seeing some interesting stuff is right here where we're determining the primary region. Here, we're using static primary. That's because we configured our lease configuration to be static. We are the primary.  \n  \n[9:19] We're reading the config file from /app/litefs. The LiteFS version is printed out here. We've got our primary lease acquired, because we're the only instance that can be the primary instance. The /litefs is where our LiteFS directory is going to be mounted to.  \n  \n[9:36] We've got a couple of other logs here related to determining the primary node and the cluster. Then, we're starting the subprocess. This is where our code starts. Here, we're running node start.js, which loads our Prisma schema.  \n  \n[9:51] It identifies the SQLite database under file /litefs/sqlite.db. Then, the database file is zero length on initialization. We didn't have a database yet, because this is a brand-new location. The SQLite database, sqlite.db, is created at file /litefs/sqlite.db.  \n  \n[10:13] It found one migration because, again, this is a brand-new database. We're not using the existing one. We're applying this migration. Then, it's telling us it applied this migration. These logs are all coming from Prisma. Then, we get to start our app. Our server is listening, and health checks are passing.  \n  \n[10:32] In review, what we did to make all of this work was we updated our Dockerfile to include fuse3, so we can have that virtual file system all up and loaded. That's a dependency of LiteFS.  \n  \n[10:45] Then, we're copying the LiteFS binary from the latest version, adding our LiteFS config. Then, running our command, LiteFS mount, so that LiteFS can get all set up before our actual application starts.  \n  \n[10:57] Then, our LiteFS configuration specifies the directory to be LITEFS_DIR, which was configured in our Dockerfile right here so that those could be shared for both our database URL as well as our LiteFS configuration.  \n  \n[11:12] Then, the data is pointing to the /data, which is the destination for our persistent volume called data. We're putting this inside of the LiteFS directory. Then, we specify the lease type of static to make things as easy as possible. That gets our application running within LiteFS."
							}
						},
						"description": "Deploying sites to multiple regions is less of a challenge than handling data. Fortunately Fly makes it easy to prepare for multi-region data."
					},
					{
						"_id": "cbeb8873-c4d4-4333-824e-94bef45fe0ef",
						"body": "If you're moving an existing application over to LiteFS, you probably have some existing data in your SQLite database you don't want to lose.\n\nThe Fly.io Docs has information about the LiteFS Import Command:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815799/epicweb.dev/tutorials/deploy-web-applications/images-11-importing-old-data.mp4/11-importing-old-data_10_00-39880-we-can-import-the-old-data-because-we-do-still-have-that-old-data.png)\n\nLet's work this process using our Counter demo.\n\n## SSH into the Virtual Machine\n\nFirst, SSH into the virtual machine by running the following command:\n\n```markdown\nfly ssh console -C bash\n```\n\nThis will create a secure tunnel between you and the virtual machine that's currently running.\n\nOnce connected, we can get a listing of the `/data` directory that we specified in `fly.toml`:\n\n```markdown\nls -al /data\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815797/epicweb.dev/tutorials/deploy-web-applications/images-11-importing-old-data.mp4/11-importing-old-data_20_01-17520-thats-where-our-new-database-is-found-.png)\n\nIn this directory, you'll find the old SQLite database file `sqlite.db` as well as the newer `litefs` directory.\n\n## Import Old Data to New Database\n\nTo import the data from the old SQLite database to the new LiteFS-backed database, we'll need some information from the `Dockerfile`.\n\nWe can see from the `LITEFS_DIR` and `DATABASE_URL` environment variables that our current db file is located at `/data/sqlite.db`.\n\nSo our command to import our old data will be:\n\n```markdown\nlitefs import --name sqlite.db /data/sqlite.db\n```\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815799/epicweb.dev/tutorials/deploy-web-applications/images-11-importing-old-data.mp4/11-importing-old-data_41_02-34080-now-if-i-come-over-here-and-i-hit-refresh-.png)\n\nThe import should only take a few milliseconds.\n\nAfter the import is complete, refresh your application, and the old data should be restored!",
						"slug": "import-existing-data-into-litefs",
						"solution": null,
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Import Existing Data into LiteFS",
						"description": "If you're migrating to LiteFS from an existing application, you may have data you'd like to keep. Luckily, there's an easy process for importing it into LiteFS.",
						"videoResource": {
							"duration": 203.466667,
							"muxAsset": {
								"muxPlaybackId": "cO7Ccg007j94NhlAvm5t5DU9g40101gT6nJ01SkXiuB78ds",
								"muxAssetId": "022BBzmJzPweHA3kv4QBFl8w3bAjozXbUlRD2E4JWIkE"
							},
							"title": "11-importing-old-data",
							"transcript": {
								"text": "Instructor: [0:00] If you're moving an existing application over to LiteFS, you probably have some existing data in your SQLite database that you don't want to lose. In our case, this is just a simple counter app, but I thought it'd be useful for me to show you how you can accomplish an import once you've made this migration. \r  \n\r  \n[0:17] This is the app before I deployed LiteFS. You'll see that the count is four. I hit refresh, and now the count is set to zero. That's because we lost some data, which is something no business person ever wants to hear. \r  \n\r  \n[0:30] What we're going to do is use the LiteFS Import command so that we can import the old data, because we do still have that old data. Let's SSH into our box so that we can look at the persistent volumes. Here we have our fly.toml, we have the app, and it's specified here. \r  \n\r  \n[0:47] I can simply say fly ssh console. I'm going to execute the command bash so that we run into bash and not just a simple shell. This will give us a couple more commands that are useful. I just make a habit out of doing this. Once it creates a secure tunnel between me and our virtual machine that's currently running, we can take a look at the data directory. \r  \n\r  \n[1:10] This, we see our litefs directory. That's where our new database is found, and this is where our old database is. The old database is still around. We haven't deleted it. We just made a new one that doesn't have all the data that's in here. \r  \n\r  \n[1:23] We need to import the data that's in here into the database that's under this litefs directory. Another interesting thing to look at while we're here is the litefs directory, which is where we have that virtual file system. Here is where the virtual database lives. This is what our application is accessing, but this file doesn't actually exist. \r  \n\r  \n[1:43] The real file is under this LiteFS directory, which we configured right here. To do this import, we're going to do litefs import -name, and the name is going to be the name of the database that we have configured that we're accessing. \r  \n\r  \n[2:00] If we look at our dockerfile, we have our database URL. This piece right here is what determines that name. It's the file name of the database that we're creating, so the name is going to be sqlite.db. \r  \n\r  \n[2:13] The source is going to be our old database, which is under data, so we'll say data/sqlite.db. This will very quickly import all of the data from the old database into the new one. Now, if I come over here and I hit refresh, we're going to see we have our old data all back and it works wonderfully. \r  \n\r  \n[2:37] I personally have had a little experience with this. I lost over half a million rows in a SQLite database. It was the worst day ever. Then I realized, oh, I do still have all of this data. I ran this import. It ran in milliseconds, and I got all my data back. This is how you accomplish importing your old data into your new LiteFS-backed SQLite database. \r  \n\r  \n[3:01] In review, to get all of this imported, you simply fly SSH into your virtual machine, and then you run litefs import, specify the name argument is the name of the database that you have running, and then the path to the old database that you had before. In just a couple of milliseconds, you should have all of your data back, and it works great.",
								"srt": null
							},
							"castingwords": {
								"transcript": "Instructor: [0:00] If you're moving an existing application over to LiteFS, you probably have some existing data in your SQLite database that you don't want to lose. In our case, this is just a simple counter app, but I thought it'd be useful for me to show you how you can accomplish an import once you've made this migration. \r  \n\r  \n[0:17] This is the app before I deployed LiteFS. You'll see that the count is four. I hit refresh, and now the count is set to zero. That's because we lost some data, which is something no business person ever wants to hear. \r  \n\r  \n[0:30] What we're going to do is use the LiteFS Import command so that we can import the old data, because we do still have that old data. Let's SSH into our box so that we can look at the persistent volumes. Here we have our fly.toml, we have the app, and it's specified here. \r  \n\r  \n[0:47] I can simply say fly ssh console. I'm going to execute the command bash so that we run into bash and not just a simple shell. This will give us a couple more commands that are useful. I just make a habit out of doing this. Once it creates a secure tunnel between me and our virtual machine that's currently running, we can take a look at the data directory. \r  \n\r  \n[1:10] This, we see our litefs directory. That's where our new database is found, and this is where our old database is. The old database is still around. We haven't deleted it. We just made a new one that doesn't have all the data that's in here. \r  \n\r  \n[1:23] We need to import the data that's in here into the database that's under this litefs directory. Another interesting thing to look at while we're here is the litefs directory, which is where we have that virtual file system. Here is where the virtual database lives. This is what our application is accessing, but this file doesn't actually exist. \r  \n\r  \n[1:43] The real file is under this LiteFS directory, which we configured right here. To do this import, we're going to do litefs import -name, and the name is going to be the name of the database that we have configured that we're accessing. \r  \n\r  \n[2:00] If we look at our dockerfile, we have our database URL. This piece right here is what determines that name. It's the file name of the database that we're creating, so the name is going to be sqlite.db. \r  \n\r  \n[2:13] The source is going to be our old database, which is under data, so we'll say data/sqlite.db. This will very quickly import all of the data from the old database into the new one. Now, if I come over here and I hit refresh, we're going to see we have our old data all back and it works wonderfully. \r  \n\r  \n[2:37] I personally have had a little experience with this. I lost over half a million rows in a SQLite database. It was the worst day ever. Then I realized, oh, I do still have all of this data. I ran this import. It ran in milliseconds, and I got all my data back. This is how you accomplish importing your old data into your new LiteFS-backed SQLite database. \r  \n\r  \n[3:01] In review, to get all of this imported, you simply fly SSH into your virtual machine, and then you run litefs import, specify the name argument is the name of the database that you have running, and then the path to the old database that you had before. In just a couple of milliseconds, you should have all of your data back, and it works great."
							},
							"_rev": "9CeTKuUcQZRsVUft8VmtJw",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/11-importing-old-data.mp4",
							"_updatedAt": "2024-03-19T20:41:13Z",
							"_createdAt": "2023-04-07T15:47:26Z",
							"_type": "videoResource",
							"_id": "a0d2517e-1144-4f9e-88f7-9ae559ee6735"
						}
					},
					{
						"videoResource": {
							"castingwords": {
								"transcript": "[0:00] Earlier, I mentioned that there are a couple of different configuration options for your lease configuration. The reason for this is that static lease configuration is easy. It's like this instance is always going to be the primary instance. \r  \n\r  \n[0:15] The problem is, if that instance ever falls over for any reason or during deploys, you're going to have some downtime. We'd much rather be able to have instances, or the primary instance, change over time so that if one comes down because some sort of error happens or we're doing a deploy, another instance can take over as the primary.\r  \n\r  \n[0:36] That said, we still don't want to just have the instance be any of our instances all over the world in many cases. Maybe this doesn't apply to you, but in my case, I have most of my users in one part of the world, and I have a lot of users in other parts that I want it to be fast for them, but I always want to have the primary instance be in this part of the world. \r  \n\r  \n[0:57] What we're going to do is we're going to set up consul, which is a service that runs alongside our app that is in charge of keeping track of which is the primary instance. Anytime one of the instances falls over, that is the primary, this consul service will say, hey, you need to be the new instance now or the new primary instance. Then that one can take over, and we don't have any downtime. \r  \n\r  \n[1:21] We're going to switch over from our static configuration to our consul configuration, so this consul leasing. There's one thing we need to change in our dockerfile, and that is we need to add ca-certificates because our consul service relies on this package. \r  \n\r  \n[1:39] Then, we also need to update our fly.toml to say enable_consul=true inside of our experimental configuration because this is technically still an experimental feature. You may just double check the docs to make sure that this is still necessary at the time that you're running through this.\r  \n\r  \n[1:55] The rest of our configuration is going to be inside of our LiteFS. The first thing we'll change, of course, is we're no longer doing static. We're doing consul. Then we have the advertise-url that specifies this is the URL that we are going to be advertising that we can be the candidate. \r  \n\r  \n[2:14] We're going to specify the hostname.vm.${fly_app_name}.internal: 20202. I don't know how you say that without it being 2--2-2, 20202. This is a unique URL that is accessible within the Fly network, so .internal is actually not a top level domain except within Fly's network. That's how different virtual machines can communicate with each other. \r  \n\r  \n[2:44] Then our consul configuration specifies the URL. We simply can go with the default of fly_consul_URL. Then the key is just saying this is a unique identifier for this particular instance of consul. We're just going to say litefs/${fly_app_name} to keep things pretty simple. \r  \n\r  \n[3:02] With that, that's everything we need to do to switch from a static configuration to consul. Of course, we probably are going to want to commit all of this. So let's commit, enable consul, and then we can push. With that pushed, we can go over to our actions and watch that deploy. \r  \n\r  \n[3:20] Here we have enable consul and that deploy process should just take a moment. We can go over here to our logs, and we'll speed this up so that we can take a look at our logs. All right, here we go. We've got our shutdown and restart all of this. Lots of these logs should be very familiar, but there are a couple of things that are a little different I wanted to look at. \r  \n\r  \n[3:41] Here now, we're using consul to determine the primary. We've specified our key, we've got that advertise URL, and all of that stuff is specified in here now. We've got our LiteFS mounted and the HTTP servers listening on this port, all of that stuff. Right here is interesting. It says primary lease acquired. \r  \n\r  \n[4:02] It acquired the primary lease from the consul service that was running for us, now it's connected to the cluster, and now it can run the sub-process. It basically says, hey consul, I can be the primary, and consul's like, well, sweet. There are no other primaries right now, so I'm going to let you be the primary. \r  \n\r  \n[4:17] Alternatively, it'd say, oh, actually there's already a primary node so you're going to be a replica. It'd be like, okay that's fine because I'm just a candidate. I was not elected. \r  \n\r  \n[4:27] We have our Prisma output here where it's found the migration but it didn't need to apply it because that migration existed already in the database. Now, when I come and refresh, we're going to have the count is still four, and we can still increment it. Everything is working perfectly well running behind LiteFS using consul to determine the primary as our lease configuration. \r  \n\r  \n[4:51] In review, to make this work, we just made a couple of changes. First, we added ca-certificates to our dockerfile because consul service needs that. In Fly, we enabled consul in the experimental options here. Then LiteFS, we added some configuration to our lease. We changed the type to consul. \r  \n\r  \n[5:09] We added an advertise URL. We specified the consul config for the URL as the Fly consul URL, an environment variable that is exposed to us. The key is litefs/${fly_app_name} to keep it unique. That's how you get LiteFS running with the consul lease strategy."
							},
							"_createdAt": "2023-04-07T16:04:37Z",
							"_type": "videoResource",
							"_updatedAt": "2024-03-19T20:41:11Z",
							"_rev": "9CeTKuUcQZRsVUft8VmtDa",
							"_id": "24b89a7c-d632-4aed-a867-37f2fa45f6c3",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/12-consul-leasing.mp4",
							"duration": 328.9,
							"muxAsset": {
								"muxPlaybackId": "GZ00qw4wtu00Qj6JfpZhkMuBtDUNmxUGDq34yiWbiIN02Q",
								"muxAssetId": "XGg4WA00tQ9x4F5hp8a41LAyYfjLq6L8ptiwO6YN2vc8"
							},
							"title": "12-consul-leasing.",
							"transcript": {
								"text": "[0:00] Earlier, I mentioned that there are a couple of different configuration options for your lease configuration. The reason for this is that static lease configuration is easy. It's like this instance is always going to be the primary instance. \r  \n\r  \n[0:15] The problem is, if that instance ever falls over for any reason or during deploys, you're going to have some downtime. We'd much rather be able to have instances, or the primary instance, change over time so that if one comes down because some sort of error happens or we're doing a deploy, another instance can take over as the primary.\r  \n\r  \n[0:36] That said, we still don't want to just have the instance be any of our instances all over the world in many cases. Maybe this doesn't apply to you, but in my case, I have most of my users in one part of the world, and I have a lot of users in other parts that I want it to be fast for them, but I always want to have the primary instance be in this part of the world. \r  \n\r  \n[0:57] What we're going to do is we're going to set up consul, which is a service that runs alongside our app that is in charge of keeping track of which is the primary instance. Anytime one of the instances falls over, that is the primary, this consul service will say, hey, you need to be the new instance now or the new primary instance. Then that one can take over, and we don't have any downtime. \r  \n\r  \n[1:21] We're going to switch over from our static configuration to our consul configuration, so this consul leasing. There's one thing we need to change in our dockerfile, and that is we need to add ca-certificates because our consul service relies on this package. \r  \n\r  \n[1:39] Then, we also need to update our fly.toml to say enable_consul=true inside of our experimental configuration because this is technically still an experimental feature. You may just double check the docs to make sure that this is still necessary at the time that you're running through this.\r  \n\r  \n[1:55] The rest of our configuration is going to be inside of our LiteFS. The first thing we'll change, of course, is we're no longer doing static. We're doing consul. Then we have the advertise-url that specifies this is the URL that we are going to be advertising that we can be the candidate. \r  \n\r  \n[2:14] We're going to specify the hostname.vm.${fly_app_name}.internal: 20202. I don't know how you say that without it being 2--2-2, 20202. This is a unique URL that is accessible within the Fly network, so .internal is actually not a top level domain except within Fly's network. That's how different virtual machines can communicate with each other. \r  \n\r  \n[2:44] Then our consul configuration specifies the URL. We simply can go with the default of fly_consul_URL. Then the key is just saying this is a unique identifier for this particular instance of consul. We're just going to say litefs/${fly_app_name} to keep things pretty simple. \r  \n\r  \n[3:02] With that, that's everything we need to do to switch from a static configuration to consul. Of course, we probably are going to want to commit all of this. So let's commit, enable consul, and then we can push. With that pushed, we can go over to our actions and watch that deploy. \r  \n\r  \n[3:20] Here we have enable consul and that deploy process should just take a moment. We can go over here to our logs, and we'll speed this up so that we can take a look at our logs. All right, here we go. We've got our shutdown and restart all of this. Lots of these logs should be very familiar, but there are a couple of things that are a little different I wanted to look at. \r  \n\r  \n[3:41] Here now, we're using consul to determine the primary. We've specified our key, we've got that advertise URL, and all of that stuff is specified in here now. We've got our LiteFS mounted and the HTTP servers listening on this port, all of that stuff. Right here is interesting. It says primary lease acquired. \r  \n\r  \n[4:02] It acquired the primary lease from the consul service that was running for us, now it's connected to the cluster, and now it can run the sub-process. It basically says, hey consul, I can be the primary, and consul's like, well, sweet. There are no other primaries right now, so I'm going to let you be the primary. \r  \n\r  \n[4:17] Alternatively, it'd say, oh, actually there's already a primary node so you're going to be a replica. It'd be like, okay that's fine because I'm just a candidate. I was not elected. \r  \n\r  \n[4:27] We have our Prisma output here where it's found the migration but it didn't need to apply it because that migration existed already in the database. Now, when I come and refresh, we're going to have the count is still four, and we can still increment it. Everything is working perfectly well running behind LiteFS using consul to determine the primary as our lease configuration. \r  \n\r  \n[4:51] In review, to make this work, we just made a couple of changes. First, we added ca-certificates to our dockerfile because consul service needs that. In Fly, we enabled consul in the experimental options here. Then LiteFS, we added some configuration to our lease. We changed the type to consul. \r  \n\r  \n[5:09] We added an advertise URL. We specified the consul config for the URL as the Fly consul URL, an environment variable that is exposed to us. The key is litefs/${fly_app_name} to keep it unique. That's how you get LiteFS running with the consul lease strategy.",
								"srt": null
							}
						},
						"solution": null,
						"_updatedAt": "2023-06-07T08:25:56Z",
						"body": "One of the issues with the static lease configuration is that if the primary instance goes down, all other instances will go down as well.\n\nA more resilient option is to use the Consul lease type, which switches the primary instance to a different one if the primary fails.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815819/epicweb.dev/tutorials/deploy-web-applications/images-12-setup-consul.mp4/11-setup-consul_8_00-36120-and-the-first-thing-that-we-need-to-do.png)\n\n## Setting Up Consul\n\nTo enable consul leasing we'll need to make a few changes.  \nInside of the `fly.toml` file under `experimental`, add `enable_consul = true`.\n\nThis enables the consul service for the application.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815823/epicweb.dev/tutorials/deploy-web-applications/images-12-setup-consul.mp4/11-setup-consul_10_00-48320-with-that--it-gives-us-a-couple-of-things-.png)\n\nNext inside of `litefs.yml`, change the lease type to `consul` instead of `static`.\n\nWe'll then add the `advertise-url` to the configuration, which is a unique URL identifying each instance for communication.\n\nThen for the consul configuration section we'll specify the URL as an environment variable that Fly creates once consul leasing is enabled. We'll also set a unique key for the consul configuration, using `lightfs/${FLY_APP_NAME}` to ensure it is unique.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815818/epicweb.dev/tutorials/deploy-web-applications/images-12-setup-consul.mp4/11-setup-consul_15_01-12500-where-we-specify-the-url-to-be-an-environment-variable.png)\n\nNow we need to update our Dockerfile to add `ca-certificates`, because consul needs access to them.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815847/epicweb.dev/tutorials/deploy-web-applications/images-12-setup-consul.mp4/11-setup-consul_17_01-33700-so-well-add-that-to-our-docker-file.png)\n\nAfter making these changes, commit and push the updates to the repository.\n\nTo verify the changes, check the logs for the new consul configuration.\n\nThe logs should display that LiteFS is using the consul lease strategy, initializing the consul, and acquiring the primary lease. The primary instance will then advertise its URL and connect to the cluster, allowing the app to run with the consul lease strategy in place.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815841/epicweb.dev/tutorials/deploy-web-applications/images-12-setup-consul.mp4/11-setup-consul_29_02-40740-and-it-advertises-the-fact-that-its-the-primary.png)",
						"title": "Set Up the Consul Lease Type",
						"description": "The Consul lease type provides a resilient solution in case the primary instance of our application goes down.",
						"slug": "set-up-the-consul-lease-type",
						"_id": "40c94784-6906-4cf8-859a-991206383e8e",
						"_type": "lesson"
					},
					{
						"_id": "e43bfd1b-8e80-49e7-9274-06190cc339d7",
						"_type": "lesson",
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Set Up a Proxy Server for Multi-Region Database Support",
						"description": "In order to make sure only our primary region attempts to write data, we'll set up a proxy server configuration for LiteFS",
						"body": "Our application is still running in a single instance because we haven't actually set up multiple instances yet.\n\nThere's a bit more setup we have to do.\n\nWe need to make sure that when a request comes that will lead to a database write that the instance will be able to handle it.\n\nWith LiteFS we can set up a proxy server that sits between our user and the actual app. When the proxy server receives a request, it will check to make sure the instance is on the primary node so the write can take place. If it's not, then the request will be replayed into the primary node.\n\n## Updating Dockerfile Environment Variables\n\nInside of the Dockerfile, we'll update the existing `PORT` environment variable to be named `INTERNAL_PORT` of 8080, then add a new `PORT` variable set to 8081.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815858/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_30_02-30520-and-then-proxy-forwards-traffic-to-our-own-server-thats.png)\n\nNow our application will start on port 8081, and the proxy will be port 8080 that's accessible outside of our virtual machine. The proxy takes traffic from the outside world, and forwards it to our own server running on the virtual machine.\n\nChecking the Dockerfile, we can see our `DATABASE_URL` is pointing at a file that takes in the `LITEFS_DIR` environment variable.\n\nIn order to more cleanly access the database URL, we'll create a new `DATABASE_FILENAME` environment variable in the Dockerfile, then update the URL to use the new filename variable:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815860/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_49_04-04240-environment-variable-and-interpolate-it-right-here.png)\n\nRenaming these environment variables will help us keep track as we update the LiteFS config.\n\n## Add LiteFS Proxy Config\n\nInside of `litefs.yml` we'll add a new `proxy` section with an `addr` set to our `INTERNAL_PORT` and a target of `localhost` on `PORT`.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815859/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_34_02-50360-of-our-application-at-that-port.png)\n\nThen we'll add a `db` entry that will point to the file that our database is running in using the `DATABASE_FILENAME` from the Dockerfile\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815856/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_50_04-10280-to-proxy-any-requests-that-are-coming-in-to-our-target-server.png)\n\n## Specify Candidates in LiteFS Config\n\nNext in the `litefs.yml` we need to specify whether a node can be a candidate or not. We need to do this to ensure that write requests only go to the primary node.\n\nSince this example uses San Jose for the primary node, we'll update the `lease` section with a conditional check for the current Fly region:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815856/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_60_05-21320-that-were-ok-with-it-being-in.png)\n\nRemember, your primary node should be the one that's closest to where most of your users will be.\n\n## Updating the Start Script\n\nThe `start` script is going to run regardless of which instance it's on, but Prisma migrations can only be run on the primary instance.\n\nWe need to update the script to add a check to make sure we're on the primary instance before trying to run the migrations.\n\nLiteFS provides a way of determining this by checking a file, but an easier way is to add the `litefs-js` library.\n\nAfter adding the package with `npm i litefs-js`, we can import `getInstanceInfo` from it at the top of `start.js`\n\n```jsx\nconst { getInstanceInfo } = require('litefs-js')\n```\n\nCalling `getInstanceInfo` function will return `currentInstance`, `currentIsPrimary`, and `primaryInstance`.\n\nWe can add some conditional logic that checks if `currentIsPrimary`, then will execute the migration. If not, that step will be skipped:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815859/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_117_11-16120-youll-see-that-in-the-logs-right-there-as-well.png)\n\n## Testing Our Application\n\nWe're still not technically running in multiple regions, but we'll still commit and push our changes.\n\nNow when in the deployment logs, we can see that our primary instance is being detected:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815859/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_117_11-16120-youll-see-that-in-the-logs-right-there-as-well.png)\n\nWe can also see our proxy server is listening on port 8080, and our application is listening on port 8081.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815863/epicweb.dev/tutorials/deploy-web-applications/images-13-multi-region.mp4/13-multi-region_123_11-36680-and-so-we-have-our-proxy-server-listening-at-port-8080.png)\n\nWith these changes, our application is now prepared to run in multiple regions.\n\nNow we are ready to deploy our application to multiple regions and ensure that it functions seamlessly across them.",
						"slug": "set-up-a-proxy-server-for-multi-region-database-support",
						"videoResource": {
							"_createdAt": "2023-04-07T16:06:40Z",
							"muxAsset": {
								"muxPlaybackId": "wobARkjMaDu14tvyEqamF01V8qOt2hjCdj3KfP37un3s",
								"muxAssetId": "Bdw7tqu7BUFfYk2nxLdLXuYKQ02SI00wzGfjAUPb3k6Ts"
							},
							"transcript": {
								"text": "Instructor: [0:00] Our app is actually not yet running in multiple regions, and we are not quite ready to do that. We're almost there. What we need to do is make sure that when a request comes in to make a write request, like I hit the increment button, or the decrement button here, that's going to write to the database.  \n  \n[0:17] We need to make sure that request gets routed to the instance that is going to actually be able to handle that write request. It's pretty much all non-get requests we can assume are going to write to the database. For the most part, we can assume that get request will not.  \n  \n[0:33] This is actually a built-in feature to litefs. Basically, what litefs will do, as we can set up a proxy server that sits between our user and our actual app. That proxy server when it receives a request, we'll do a couple of things to make sure that we're OK to proceed to the actual instance.  \n  \n[0:54] If the instance is not the primary node, and we're about to do a write request, then it will actually replay that request into the primary note. It's actually a really nice setup. To get this setup, we update our litefs config to add a proxy.  \n  \n[1:12] This configures the proxy that litefs is going to create for us. The couple of configuration options that we need is adder. For the address, we need this to be the internal port that is used in our fly configuration. If we look at our fly tunnel, this is the internal port.  \n  \n[1:31] I'm actually going to set this up inside of my docker as an environment variable to communicate that is important, that it is the same. In fact, we're going to rename this port to internal port, and then we're going to have a new port for our application to be listening on. This is going to be 8081.  \n  \n[1:50] Now our application's going to start up on port 8081, but then the proxy is going to be the port 8080 that is accessible outside of our virtual machine. The proxy takes traffic from the outside world and then proxy forwards traffic to our own server that's running on our virtual machine.  \n  \n[2:11] The adder here is going to be Cohen internal port. This is the publicly accessible address. The target is going to be localhost:${Port}. This is going to be the locally running instance of our application at that port.  \n  \n[2:29] Then the database, db, is going to need to point to the file that our database is running in. If we look at our Docker file, that's going to be at LITEFS_DIR/sqlite.db. I'm actually going to make another environment variable, ENV DATABASE_FILENAME, and that will equal that. Then we can simply say DATABASE_FILENAME for our database URL.  \n  \n[2:56] With that now, we can take that DATABASE_FILENAME environment variable and interpolate it right here. Now our proxy server is set up to proxy any request that are coming in to our target server and make sure that any write requests don't actually hit non-primary nodes.  \n  \n[3:13] The other thing that we need to do while we're here in the litefs EMO, we need to specify whether something can be a candidate or not. We don't want everything to be a candidate. We put a node in Amsterdam and one in San Jose and we put another in Hong Kong.  \n  \n[3:28] We don't want them all to just say, \"Hey, I can be a candidate.\" Because we want to keep our candidate in a region that most of our users are in. For us, most of our users are going to be around San Jose. I'm going to specify this is ${FLY_REGION = = sjc}, so San Jose.  \n  \n[3:50] Now if you're an Amsterdam region, you're going to say, \"No, I can't be a candidate.\" When you're talking to Consul, it's going to say, \"I can't be a candidate who is the primary.\" That way we keep the primary in sjc.  \n  \n[4:01] Now we are going to deploy two regions to sjc to make sure that we can have one fall over and the other takeover. This is one way that we can make sure that the primary region is in a part of the world that we're OK with it being in.  \n  \n[4:18] The last thing that we need to do here is make sure that our start script is going to work. Because here we're running the Prisma migrations and this will not work if we're not on the primary instance. We need to have some way to say if (isPrimaryInstance), then we can run the migrations, otherwise we don't.  \n  \n[4:36] We need to know some way whether we're currently running in the primary instance. There's a file that you can read that litefs sets up for you to make this really easy. To make it even easier, there's actually a library we can install and we'll save this as a regular dependency called litefs-js.  \n  \n[4:55] The reason this needs to be saved as a regular dependency is because this is going to be running in production. We're going to get the { getInstanceInfo } = require('litefs-js'). This get instance info is going to give us exactly what we want {currentIsPrimary} = is await getInstanceInfo ().  \n  \n[5:19] If the currentIsPrimary then we'll go ahead and run the migrations, otherwise we'll simply start the app. We'll skip that part because those migrations should have already been run by the primary instance. We can add a bunch of logs here too, which I am going to copy paste because I don't think anybody wants to watch me write out a bunch of logs, so we'll just stick all of those there.  \n  \n[5:38] We get the currentInstance, the currentIsPrimary, and the primaryInstance, and we can use those in our logs where you say that this is in the primary region deploying migrations, or this is saying it's not in the primary region, so we're skipping migrations.  \n  \n[5:51] That should be enough for us to go ahead and deploy this. We are still not technically running in multiple regions, but let's commit this multi region. We'll push this up and we'll make sure that those logs appear the way that we expect them to.  \n  \n[6:07] Coming over to our actions, now we can go to multi-region and we get our deploy going right here. Then we can take a look at our logs and we'll see the logs that happen as this new version of our app is deployed.  \n  \n[6:23] Our application is now shutting down and starting back up with our new configuration, and we can see some of these logs coming in. Right here it's disconnecting from the primary retrying and it eventually it reconnects. No problem there. It manages.  \n  \n[6:38] That's the benefit of consoles. It makes sure that you always have a good primary. Don't worry about those logs right there. Then we'll notice here that instance, ebb yada, yada. You'll see that in the logs right there as well in sjc, that's the fly region, is primary deploying migrations.  \n  \n[6:56] There weren't any actual migrations to deploy. You can see the logs right here that we did actually execute fly to deploy those migrations. Now you'll notice also server listening at port 8081. We have our proxy server listening at port 8080.  \n  \n[7:12] Now all traffic is going to go through that proxy server and it's going to make sure that we never actually execute a write against the non-primary region when we're doing like a post, put, delete or anything like that.  \n  \n[7:25] Get requests can go to the region that it's going to, but the proxy will make sure that we don't write to any region that it's not supposed to go to. Now, there's actually a lot to this that the proxy is doing for us.  \n  \n[7:37] If you want to look at the README for litefs-js, then you can take a look at the challenge of Reed replicas and consistency to understand why this proxy and why litefs-js are so useful for us. This is all pretty much handled for us and it's really nice.  \n  \n[7:55] There are some caveats. If you ever try to write to the database inside of a get request, you do need to use a couple other utilities from litefs-js. For the most part, 99 percent of the time you should not have to actually change any other of your application code for this to work. Your application should work swimmingly in multiple regions with this setup.  \n  \n[8:16] In review, the things that we changed here was we updated our docker file to have a couple different environment variables. One, we specified the internal port, which equals the internal port that we have specified here.  \n  \n[8:28] We're using that internal port for the adder of our proxy so it knows what port it should be listening at. Then we updated the port for our actual application to be port 8081. In our litefs, we say our target is going to be at that port.  \n  \n[8:44] We also update our database URL to use a database file name environment variable so that we could use that same database file name in our litefs configuration here, so litefs knows which database it should be handling transactional consistency.  \n  \n[9:00] Because you actually can have multiple databases in a single litefs instance, but only one proxy for this support to ensure we're writing to the right instance. Then we also updated our start script to use litefs-js to make sure that we're only running the migrations on the primary instance, and our application is now running in a way that we're ready to actually deploy to multiple regions.  \n  \n[9:29] The last thing we updated, this is actually optional, but the last thing we updated was we set the candidate to be sjc. It can only be this region. What we're going to do is what we have right here is actually basically static leasing because we're saying only one region can be a candidate.  \n  \n[9:49] What we'll do is we'll actually deploy to two regions, or two instances in the sjc region so that if one of those falls over, the other can pick it up. That's what we did to get our application ready to go multi-region, and we're going to go multi-region next.",
								"srt": null
							},
							"duration": 605.333333,
							"_id": "ff3cb4b2-47d6-4380-9478-664481879fc3",
							"title": "13-multi-region",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/13-multi-region.mp4",
							"castingwords": {
								"transcript": "Instructor: [0:00] Our app is actually not yet running in multiple regions, and we are not quite ready to do that. We're almost there. What we need to do is make sure that when a request comes in to make a write request, like I hit the increment button, or the decrement button here, that's going to write to the database.  \n  \n[0:17] We need to make sure that request gets routed to the instance that is going to actually be able to handle that write request. It's pretty much all non-get requests we can assume are going to write to the database. For the most part, we can assume that get request will not.  \n  \n[0:33] This is actually a built-in feature to litefs. Basically, what litefs will do, as we can set up a proxy server that sits between our user and our actual app. That proxy server when it receives a request, we'll do a couple of things to make sure that we're OK to proceed to the actual instance.  \n  \n[0:54] If the instance is not the primary node, and we're about to do a write request, then it will actually replay that request into the primary note. It's actually a really nice setup. To get this setup, we update our litefs config to add a proxy.  \n  \n[1:12] This configures the proxy that litefs is going to create for us. The couple of configuration options that we need is adder. For the address, we need this to be the internal port that is used in our fly configuration. If we look at our fly tunnel, this is the internal port.  \n  \n[1:31] I'm actually going to set this up inside of my docker as an environment variable to communicate that is important, that it is the same. In fact, we're going to rename this port to internal port, and then we're going to have a new port for our application to be listening on. This is going to be 8081.  \n  \n[1:50] Now our application's going to start up on port 8081, but then the proxy is going to be the port 8080 that is accessible outside of our virtual machine. The proxy takes traffic from the outside world and then proxy forwards traffic to our own server that's running on our virtual machine.  \n  \n[2:11] The adder here is going to be Cohen internal port. This is the publicly accessible address. The target is going to be localhost:${Port}. This is going to be the locally running instance of our application at that port.  \n  \n[2:29] Then the database, db, is going to need to point to the file that our database is running in. If we look at our Docker file, that's going to be at LITEFS_DIR/sqlite.db. I'm actually going to make another environment variable, ENV DATABASE_FILENAME, and that will equal that. Then we can simply say DATABASE_FILENAME for our database URL.  \n  \n[2:56] With that now, we can take that DATABASE_FILENAME environment variable and interpolate it right here. Now our proxy server is set up to proxy any request that are coming in to our target server and make sure that any write requests don't actually hit non-primary nodes.  \n  \n[3:13] The other thing that we need to do while we're here in the litefs EMO, we need to specify whether something can be a candidate or not. We don't want everything to be a candidate. We put a node in Amsterdam and one in San Jose and we put another in Hong Kong.  \n  \n[3:28] We don't want them all to just say, \"Hey, I can be a candidate.\" Because we want to keep our candidate in a region that most of our users are in. For us, most of our users are going to be around San Jose. I'm going to specify this is ${FLY_REGION = = sjc}, so San Jose.  \n  \n[3:50] Now if you're an Amsterdam region, you're going to say, \"No, I can't be a candidate.\" When you're talking to Consul, it's going to say, \"I can't be a candidate who is the primary.\" That way we keep the primary in sjc.  \n  \n[4:01] Now we are going to deploy two regions to sjc to make sure that we can have one fall over and the other takeover. This is one way that we can make sure that the primary region is in a part of the world that we're OK with it being in.  \n  \n[4:18] The last thing that we need to do here is make sure that our start script is going to work. Because here we're running the Prisma migrations and this will not work if we're not on the primary instance. We need to have some way to say if (isPrimaryInstance), then we can run the migrations, otherwise we don't.  \n  \n[4:36] We need to know some way whether we're currently running in the primary instance. There's a file that you can read that litefs sets up for you to make this really easy. To make it even easier, there's actually a library we can install and we'll save this as a regular dependency called litefs-js.  \n  \n[4:55] The reason this needs to be saved as a regular dependency is because this is going to be running in production. We're going to get the { getInstanceInfo } = require('litefs-js'). This get instance info is going to give us exactly what we want {currentIsPrimary} = is await getInstanceInfo ().  \n  \n[5:19] If the currentIsPrimary then we'll go ahead and run the migrations, otherwise we'll simply start the app. We'll skip that part because those migrations should have already been run by the primary instance. We can add a bunch of logs here too, which I am going to copy paste because I don't think anybody wants to watch me write out a bunch of logs, so we'll just stick all of those there.  \n  \n[5:38] We get the currentInstance, the currentIsPrimary, and the primaryInstance, and we can use those in our logs where you say that this is in the primary region deploying migrations, or this is saying it's not in the primary region, so we're skipping migrations.  \n  \n[5:51] That should be enough for us to go ahead and deploy this. We are still not technically running in multiple regions, but let's commit this multi region. We'll push this up and we'll make sure that those logs appear the way that we expect them to.  \n  \n[6:07] Coming over to our actions, now we can go to multi-region and we get our deploy going right here. Then we can take a look at our logs and we'll see the logs that happen as this new version of our app is deployed.  \n  \n[6:23] Our application is now shutting down and starting back up with our new configuration, and we can see some of these logs coming in. Right here it's disconnecting from the primary retrying and it eventually it reconnects. No problem there. It manages.  \n  \n[6:38] That's the benefit of consoles. It makes sure that you always have a good primary. Don't worry about those logs right there. Then we'll notice here that instance, ebb yada, yada. You'll see that in the logs right there as well in sjc, that's the fly region, is primary deploying migrations.  \n  \n[6:56] There weren't any actual migrations to deploy. You can see the logs right here that we did actually execute fly to deploy those migrations. Now you'll notice also server listening at port 8081. We have our proxy server listening at port 8080.  \n  \n[7:12] Now all traffic is going to go through that proxy server and it's going to make sure that we never actually execute a write against the non-primary region when we're doing like a post, put, delete or anything like that.  \n  \n[7:25] Get requests can go to the region that it's going to, but the proxy will make sure that we don't write to any region that it's not supposed to go to. Now, there's actually a lot to this that the proxy is doing for us.  \n  \n[7:37] If you want to look at the README for litefs-js, then you can take a look at the challenge of Reed replicas and consistency to understand why this proxy and why litefs-js are so useful for us. This is all pretty much handled for us and it's really nice.  \n  \n[7:55] There are some caveats. If you ever try to write to the database inside of a get request, you do need to use a couple other utilities from litefs-js. For the most part, 99 percent of the time you should not have to actually change any other of your application code for this to work. Your application should work swimmingly in multiple regions with this setup.  \n  \n[8:16] In review, the things that we changed here was we updated our docker file to have a couple different environment variables. One, we specified the internal port, which equals the internal port that we have specified here.  \n  \n[8:28] We're using that internal port for the adder of our proxy so it knows what port it should be listening at. Then we updated the port for our actual application to be port 8081. In our litefs, we say our target is going to be at that port.  \n  \n[8:44] We also update our database URL to use a database file name environment variable so that we could use that same database file name in our litefs configuration here, so litefs knows which database it should be handling transactional consistency.  \n  \n[9:00] Because you actually can have multiple databases in a single litefs instance, but only one proxy for this support to ensure we're writing to the right instance. Then we also updated our start script to use litefs-js to make sure that we're only running the migrations on the primary instance, and our application is now running in a way that we're ready to actually deploy to multiple regions.  \n  \n[9:29] The last thing we updated, this is actually optional, but the last thing we updated was we set the candidate to be sjc. It can only be this region. What we're going to do is what we have right here is actually basically static leasing because we're saying only one region can be a candidate.  \n  \n[9:49] What we'll do is we'll actually deploy to two regions, or two instances in the sjc region so that if one of those falls over, the other can pick it up. That's what we did to get our application ready to go multi-region, and we're going to go multi-region next."
							},
							"_rev": "6LaeqP6n94P8FD3sVhYllD",
							"_type": "videoResource",
							"_updatedAt": "2024-03-19T20:41:10Z"
						},
						"solution": null
					},
					{
						"videoResource": {
							"_rev": "6LaeqP6n94P8FD3sVhYljN",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/14-scaling-multi-region.mp4",
							"duration": 409.7,
							"_createdAt": "2023-04-07T16:42:58Z",
							"title": "14-scaling-multi-region",
							"transcript": {
								"text": "Instructor: [0:00] With all the work we've done leading up to this point, I'm really excited to finally go multi-region with you. We're going to set up a couple of things in our terminals so that we can watch what's going on.  \n  \n[0:10] First, we're going to say fly status with a watch flag. This will keep us up to date with what regions are currently active and all of that. Then, we'll make another one that is fly logs, so we can keep our logs going. Now we're going to create multiple regions. In Fly, you can run a simple command.  \n  \n[0:29] If we look right here, you can say fly regions add yada yada yada, but when we're using persistent volumes, you actually manage your regions by what regions you specify persistent volumes to be in. To create a persistent volume, you say fly vol. We'll just see what the output there for that is.  \n  \n[0:48] Fly vol, we want to create, and for the region that we want it to be in, we're going to deploy another region in the sjc region, so we'll have two instances in sjc. That way, either one of them can be the candidate, and if one falls over, the other can take over. We'll say fly vol create, and we want to call this data. We'll say region is sjc.  \n  \n[1:13] We're calling this data because we've configured that in our fly.toml, so we want to keep it to be the same name, so the region is sjc. We're going to say the size is one because this is just a demo app. Depending on how much data you have, you may want to increase that size.  \n  \n[1:27] We'll execute this, and Fly is going to create a persistent volume for us, and now that's a part of our app. We can say fly vol list, and this will list all of the volumes we have in our app. Only one of them is attached to a VM.  \n  \n[1:45] If we look at our fly scale settings here, we can say fly scale show, and our current resources for our app are set to a single instance, so a count of one. Now, if we want to have two instances so that we can connect this to another VM, then we can say fly scale count one. We'll take a look at what this does to our logs here in just a second.  \n  \n[2:12] We'll also see some output in our status, where it actually starts booting up a new instance. Here, we actually changed it to one, so let's change it to two. We want two instances, and now we can see it here in just a second. Our logs will have it popping up a new instance, and we'll see that new instance showing up here.  \n  \n[2:33] Great. In our logs, we can see we're starting a new instance. We'll see our previous instance was this ebb instance ID, that's the host name, and our new instance is running in this runner. Our new instance is here, this 0d8-yada-yada. You'll see our logs are now all mixed up, and you can run logs specifically for a specific instance and things, which can be quite useful.  \n  \n[2:55] You'll see some really familiar logs in here, but one thing that will be new is right here. We see a existing primary found, connecting as a replica. This new instance that we just created is going to be a read replica instance. It's a little hard to demonstrate that we're actually running against a primary or a read replica here, but the application does still work.  \n  \n[3:24] That's a good sign. We can take a look at our watch output. We see that we have two instances running now. That's cool to have some horizontal scaling, hooray for that, but let's actually deploy to another region, a completely different region. We're going to just go up, up, up, up, and our region, we'll say, is ams.  \n  \n[3:45] We're going to go to Amsterdam, and create that volume with a size one, so exactly the same sort of thing, but we still only have a scale of two. We're going to increase this to a scale of three, so fly scale count three. Once Fly realizes, \"Oh, we've got another instance possibility here, we've got a volume there, so let's go ahead and get that started,\" and there it is.  \n  \n[4:07] Our runner is starting an instance in ams, and in just a couple of seconds, we'll get that instance started up. You'll see some more of those logs. It'll say it's connecting as a read replica. Here we go, uninitialized volume data. It's initializing, it's encrypted, and opening encrypted volume. Now it can actually start up our app.  \n  \n[4:28] We've got LiteFS running. It connected as a replica right here, and now our application is running in Amsterdam. If you really want to test this out, I use a extension called ModHelper, that allows you to specify a region, so you can prefer a specific region. What this effectively is doing is it's simply going to add a request header called fly-prefer-region, and the value is ams.  \n  \n[5:00] You can see that I've done this a lot for different regions to test this out. If you pull up your DevTools, and here we refresh, you'll notice this takes a little bit longer, because we are going around the world now. You'll see, in the request headers, we see our fly-prefer-region right there, so we are sending that.  \n  \n[5:19] Fly will recognize that and make sure that we're routing to the proper region. With that, if I do my POST request here, and that triggers a full refresh, but if we look at that POST request, we get that fly-prefer-region. There's nothing in here for us to notice exactly which region handled it, but if we take a look at our logs, we'll look for that POST.  \n  \n[5:41] You'll see that that was handled by sjc, appropriately, because that's the only one that is the primary region. If you recall, this is the instance that's been running the longest. The ebb one that we have right here, which has been running for longer than the rest of them, so this is the primary region.  \n  \n[6:00] That's how you add a bunch of new regions to your application. When we're dealing with a persistent volume, you're going to say fly vol create, you give it the name that we have in our fly.toml. Here, we call it data, and so that's why we said fly vol create data.  \n  \n[6:18] Then we specify the region, we specify the size. That creates it, and then we use the fly scale count command so that we can say, \"Hey, we want to have another region.\" You can do this for all 26 regions, or however many Fly has, and you'll have this running all over the world.  \n  \n[6:34] Then, of course, redeploys work really well and make sure to just take the primary region and get a new primary region up and going. Then the rest of them come up, and it's amazing. That is how you get multiple regions running in Fly.",
								"srt": null
							},
							"_type": "videoResource",
							"_id": "45869a9a-de7c-441c-98de-244e7c680b8c",
							"_updatedAt": "2024-03-19T20:41:08Z",
							"castingwords": {
								"transcript": "Instructor: [0:00] With all the work we've done leading up to this point, I'm really excited to finally go multi-region with you. We're going to set up a couple of things in our terminals so that we can watch what's going on.  \n  \n[0:10] First, we're going to say fly status with a watch flag. This will keep us up to date with what regions are currently active and all of that. Then, we'll make another one that is fly logs, so we can keep our logs going. Now we're going to create multiple regions. In Fly, you can run a simple command.  \n  \n[0:29] If we look right here, you can say fly regions add yada yada yada, but when we're using persistent volumes, you actually manage your regions by what regions you specify persistent volumes to be in. To create a persistent volume, you say fly vol. We'll just see what the output there for that is.  \n  \n[0:48] Fly vol, we want to create, and for the region that we want it to be in, we're going to deploy another region in the sjc region, so we'll have two instances in sjc. That way, either one of them can be the candidate, and if one falls over, the other can take over. We'll say fly vol create, and we want to call this data. We'll say region is sjc.  \n  \n[1:13] We're calling this data because we've configured that in our fly.toml, so we want to keep it to be the same name, so the region is sjc. We're going to say the size is one because this is just a demo app. Depending on how much data you have, you may want to increase that size.  \n  \n[1:27] We'll execute this, and Fly is going to create a persistent volume for us, and now that's a part of our app. We can say fly vol list, and this will list all of the volumes we have in our app. Only one of them is attached to a VM.  \n  \n[1:45] If we look at our fly scale settings here, we can say fly scale show, and our current resources for our app are set to a single instance, so a count of one. Now, if we want to have two instances so that we can connect this to another VM, then we can say fly scale count one. We'll take a look at what this does to our logs here in just a second.  \n  \n[2:12] We'll also see some output in our status, where it actually starts booting up a new instance. Here, we actually changed it to one, so let's change it to two. We want two instances, and now we can see it here in just a second. Our logs will have it popping up a new instance, and we'll see that new instance showing up here.  \n  \n[2:33] Great. In our logs, we can see we're starting a new instance. We'll see our previous instance was this ebb instance ID, that's the host name, and our new instance is running in this runner. Our new instance is here, this 0d8-yada-yada. You'll see our logs are now all mixed up, and you can run logs specifically for a specific instance and things, which can be quite useful.  \n  \n[2:55] You'll see some really familiar logs in here, but one thing that will be new is right here. We see a existing primary found, connecting as a replica. This new instance that we just created is going to be a read replica instance. It's a little hard to demonstrate that we're actually running against a primary or a read replica here, but the application does still work.  \n  \n[3:24] That's a good sign. We can take a look at our watch output. We see that we have two instances running now. That's cool to have some horizontal scaling, hooray for that, but let's actually deploy to another region, a completely different region. We're going to just go up, up, up, up, and our region, we'll say, is ams.  \n  \n[3:45] We're going to go to Amsterdam, and create that volume with a size one, so exactly the same sort of thing, but we still only have a scale of two. We're going to increase this to a scale of three, so fly scale count three. Once Fly realizes, \"Oh, we've got another instance possibility here, we've got a volume there, so let's go ahead and get that started,\" and there it is.  \n  \n[4:07] Our runner is starting an instance in ams, and in just a couple of seconds, we'll get that instance started up. You'll see some more of those logs. It'll say it's connecting as a read replica. Here we go, uninitialized volume data. It's initializing, it's encrypted, and opening encrypted volume. Now it can actually start up our app.  \n  \n[4:28] We've got LiteFS running. It connected as a replica right here, and now our application is running in Amsterdam. If you really want to test this out, I use a extension called ModHelper, that allows you to specify a region, so you can prefer a specific region. What this effectively is doing is it's simply going to add a request header called fly-prefer-region, and the value is ams.  \n  \n[5:00] You can see that I've done this a lot for different regions to test this out. If you pull up your DevTools, and here we refresh, you'll notice this takes a little bit longer, because we are going around the world now. You'll see, in the request headers, we see our fly-prefer-region right there, so we are sending that.  \n  \n[5:19] Fly will recognize that and make sure that we're routing to the proper region. With that, if I do my POST request here, and that triggers a full refresh, but if we look at that POST request, we get that fly-prefer-region. There's nothing in here for us to notice exactly which region handled it, but if we take a look at our logs, we'll look for that POST.  \n  \n[5:41] You'll see that that was handled by sjc, appropriately, because that's the only one that is the primary region. If you recall, this is the instance that's been running the longest. The ebb one that we have right here, which has been running for longer than the rest of them, so this is the primary region.  \n  \n[6:00] That's how you add a bunch of new regions to your application. When we're dealing with a persistent volume, you're going to say fly vol create, you give it the name that we have in our fly.toml. Here, we call it data, and so that's why we said fly vol create data.  \n  \n[6:18] Then we specify the region, we specify the size. That creates it, and then we use the fly scale count command so that we can say, \"Hey, we want to have another region.\" You can do this for all 26 regions, or however many Fly has, and you'll have this running all over the world.  \n  \n[6:34] Then, of course, redeploys work really well and make sure to just take the primary region and get a new primary region up and going. Then the rest of them come up, and it's amazing. That is how you get multiple regions running in Fly."
							},
							"muxAsset": {
								"muxPlaybackId": "TpxVU900myZvV2L01ODJoPUXHUP9FXymerysIwP00V9Cdg",
								"muxAssetId": "4KT48lS45reiq7p3k01V4bgfHhBnMUeh4I3dLoDRTtEQ"
							}
						},
						"solution": null,
						"_id": "80a18fb8-84ec-4fa1-ab54-a7df450adc52",
						"_type": "lesson",
						"_updatedAt": "2023-07-09T23:22:27Z",
						"description": "With all the setup in place, it's time to scale our deployment to multiple regions, and test to confirm it's working as expected.",
						"body": "We're finally ready to go multi-region!\n\nFirst, let's set up a couple of things in our terminals to monitor what's happening.\n\nWe'll run `fly status --watch` in one terminal and `fly logs` in another to keep an eye on everything.\n\n### Creating Persistent Volumes\n\nIn Fly, you manage your regions by specifying persistent volumes in different regions.\n\nTo create a persistent volume, use the command `fly vol create`.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815863/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_9_00-56480-and-for-the-region-that-we-want-it-to-be-in-.png)\n\nFirst, we'll deploy another region in SJC and have two instances there. That way, either one of them can be the candidate, and if one falls over, there's another to take over.\n\nHere's the command to create the volume:\n\n```markdown\nfly vol create data --region SJC --size 1\n```\n\nWe name it \"data\" because that's how we configured it in our Fly configuration.\n\nAfter executing this command, Fly creates a persistent volume for us.\n\n### Scaling Instances\n\nTo list all volumes in our app, we use `fly vol list`.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815863/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_21_01-46440-and-thats-because-we-currently-.png)\n\nRight now, only one is attached to a VM.\n\nLooking at our scale settings by running `fly scale show`, the current resources for the app are set to a single instance.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815864/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_25_02-05040-so-that-we-can-connect-this-to-another-vm-.png)\n\n**IMPORTANT NOTE**: Fly's platform has been updated to \"v2\". The v2 platform does not yet support the `fly scale count` commands that we will be using for this part of the tutorial. Instead, you will need to use `fly machine clone`. [Learn more on why](https://community.fly.io/t/how-do-i-scale-my-app-with-a-volume-to-multiple-instances/13893). [Learn about scaling up with `fly machine clone`](https://fly.io/docs/apps/scale-count/#scale-up-with-fly-machine-clone).\n\nRunning `fly scale count 1` will show information about the first instance in the logs.\n\nIn order to boot up the secton instance, run `fly scale count 2` (with v2, you'll use `fly machine list` and then use the ID in your existing machine with `fly machine clone <machine_id>`). The logs will then show a new instance booting up:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815872/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_40_03-02160-so-our-new-instance-is-here-.png)\n\nIn this case the `ebbb5dcb` is the first instance, and the `0d8620a4` is the second.\n\nHere you can see that our second instance in SJC is not the primary, so Fly skips the migrations:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815871/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_47_03-33160-now--its-actually-a-little-hard-to-demonstrate-that-were-actually.png)\n\n### Deploying to Another Region\n\nLet's deploy to a completely different region.\n\nWe'll start by creating a new volume for Amsterdam:\n\n```markdown\nfly vol create data --region ams --size 1\n```\n\nThen we'll run `fly scale count 3`. (Note again, you'll use `fly machine clone <machine_id>`)\n\nFly will start an instance in Amsterdam, and the logs will show the initialization process. Our application is now running in Amsterdam!\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815873/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_63_04-50520-it-connected-as-a-replica-right-here.png)\n\n### Testing Regions with ModHeader\n\nThe <a href=\"https://modheader.com/\" target=\"_blank\" rel=\"noopener\">ModHeader browser extension</a> helps us to easily test our deployment by specifying our preferred region.\n\nThis will add a request header called `fly-prefer-region` with a value of the desired region of `ams`.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815863/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_64_04-57480-and-if-you-really-want-to-test-this-out-.png)\n\nNow when we look at the DevTools Network tab we can see the `fly-prefer-region` header is present, and since it's in Amsterdam the app took a little bit longer to load:\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815861/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_73_05-36320-so-we-are-sending-that.png)\n\nNow when we make a POST request by changing the count, there's nothing specific that shows which region we are requesting.\n\nHowever when we look at our logs, notice that the primary region `sjc` handled the request.\n\n![](http://res.cloudinary.com/epic-web/image/upload/v1680815867/epicweb.dev/tutorials/deploy-web-applications/images-14-scaling-multi-region.mp4/14-scaling-multi-region_85_06-04720-because-thats-the-only-one-that-is-the-primary-region.png)\n\nWith this setup, you can send requests to your app and check the logs to confirm which region handled them.\n\nThat's how you get multiple regions running in Fly!",
						"title": "Scaling to Multiple Regions",
						"slug": "scaling-to-multiple-regions"
					},
					{
						"_id": "98c8ddac-aa0d-45c4-ad9b-80ba54c99220",
						"_type": "lesson",
						"videoResource": {
							"_updatedAt": "2024-03-19T20:41:06Z",
							"castingwords": {
								"transcript": "Presenter: [0:01] I hope you had a good time learning with me how to deploy to the Edge with Fly.io deploying long-running servers and your data as close to your users as possible. It blows my mind what we have available to us and getting the best of all worlds.  \n  \n[0:16] We have availability. We have data consistency with the transactional consistency stuff that we did. It's really cool what we've been able to do. I look forward to hearing about the cool things that you're running on the Edge. I'm so happy that you're with me."
							},
							"muxAsset": {
								"muxPlaybackId": "f3dvzrszGIjLO6a01DDId00WrvSxUjlSZ701MEfUFbUF2Y",
								"muxAssetId": "yQt00z7J4EuAIeWVAXuWI4x6i5g801hZcllD7TOFYQO6g"
							},
							"_id": "a09dff47-96ed-42cd-9540-0cd645c925b1",
							"originalMediaUrl": "https://egghead-video-uploads.s3.amazonaws.com/epic-web/15-outro.mp4",
							"duration": 30.133333,
							"_rev": "9CeTKuUcQZRsVUft8VmsoA",
							"_type": "videoResource",
							"transcript": {
								"text": "Presenter: [0:01] I hope you had a good time learning with me how to deploy to the Edge with Fly.io deploying long-running servers and your data as close to your users as possible. It blows my mind what we have available to us and getting the best of all worlds.  \n  \n[0:16] We have availability. We have data consistency with the transactional consistency stuff that we did. It's really cool what we've been able to do. I look forward to hearing about the cool things that you're running on the Edge. I'm so happy that you're with me.",
								"srt": null
							},
							"_createdAt": "2023-04-07T16:48:36Z",
							"title": "15-outro"
						},
						"solution": null,
						"_updatedAt": "2023-06-07T08:25:56Z",
						"title": "Concluding the Deployment Tutorial",
						"description": null,
						"body": "You've made it to the end!\n\nI hope you had a good time learning with me how to deploy to the edge with Fly.io.\n\nDeploying long-running servers and having your data as close to your users as possible... It just blows my mind what we have available to us!\n\nWe have availability and data consistency with transactional consistency, giving us the best of all worlds.\n\nThanks for learning with me, and I look forward to hearing about the cool things you running on the edge!",
						"slug": "concluding-the-deployment-tutorial"
					}
				],
				"resources": [],
				"_id": "a9b73e4f-09d4-4b6c-87d2-e8c120a6be99",
				"_type": "section",
				"_updatedAt": "2023-04-10T21:24:51Z",
				"title": "Multi-Region Data and Deployment",
				"description": null
			}
		]
	}
]
